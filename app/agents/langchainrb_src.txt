Project Path: langchainrb-0.19.4

Source Tree:

```
langchainrb-0.19.4
‚îú‚îÄ‚îÄ CHANGELOG.md
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ lib
‚îÇ   ‚îú‚îÄ‚îÄ langchain.rb
‚îÇ   ‚îú‚îÄ‚îÄ langchain
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assistant
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ messages
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_gemini_message.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_message.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_message.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_message.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mistral_ai_message.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ llm
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ adapters
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ ollama.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ google_gemini.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ mistral_ai.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ aws_bedrock_anthropic.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ openai.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ adapter.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependency_helper.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evals
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ragas
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ answer_relevance.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ main.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ context_relevance.rb
‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ answer_relevance.yml
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ context_relevance.yml
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ faithfulness_statements_verification.yml
‚îÇ   ‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ faithfulness_statements_extraction.yml
‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ faithfulness.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ response
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_titan_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llama_cpp_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hugging_face_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ollama_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_gemini_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_bedrock_meta_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai21_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cohere_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replicate_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ mistral_ai_response.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ai21.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cohere.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_gemini.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anthropic.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mistral_ai.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aws_bedrock.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ google_vertex_ai.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llama_cpp.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parameters
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ chat.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ unified_parameters.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ summarize_template.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ summarize_template.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ azure.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ replicate.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hugging_face.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loader.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vectorsearch
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hnswlib.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ weaviate.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pgvector.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ epsilla.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hyde.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qdrant.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pinecone.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ milvus.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ elasticsearch.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ to_boolean.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ image_wrapper.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cosine_similarity.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hash_transformer.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ processors
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pptx.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ eml.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ json.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ csv.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xlsx.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jsonl.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ xls.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pdf.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ html.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ docx.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool_response.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ output_parsers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ structured_output_parser.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ output_parser_exception.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ naive_fix_prompt.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ output_fixing_parser.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunk.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompt_template.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ few_shot_prompt_template.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loading.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assistant.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ version.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tool_definition.rb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chunker
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ text.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recursive_text.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ markdown.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sentence.rb
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prompts
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic_prompt_template.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ semantic.rb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tool
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ vectorsearch.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ news_retriever.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ wikipedia.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ file_system.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ calculator.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ weather.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ google_search.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ruby_code_interpreter.rb
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ database.rb
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ tavily.rb
‚îÇ   ‚îî‚îÄ‚îÄ langchainrb.rb
‚îî‚îÄ‚îÄ LICENSE.txt

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/CHANGELOG.md`:

```md
# CHANGELOG

## Key
- [BREAKING]: A breaking change. After an upgrade, your app may need modifications to keep working correctly.
- [FEATURE]: A non-breaking improvement to the app. Either introduces new functionality, or improves on an existing feature.
- [BUGFIX]: Fixes a bug with a non-breaking change.
- [COMPAT]: Compatibility improvements - changes to make Langchain.rb more compatible with different dependency versions.
- [OPTIM]: Optimization or performance increase.
- [DOCS]: Documentation changes. No changes to the library's behavior.
- [SECURITY]: A change which fixes a security vulnerability.

## [Unreleased]

## [0.19.4] - 2025-02-17
- [BREAKING] [https://github.com/patterns-ai-core/langchainrb/pull/894] Tools can now output image_urls, and all tool output must be wrapped by a tool_response() method
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/921] Fix for Assistant when OpenAI o1/o3 models are used

## [0.19.3] - 2025-01-13
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/900] Empty text content should not be set when content is nil when using AnthropicMessage

## [0.19.2] - 2024-11-26
- [FEATURE] [https://github.com/patterns-ai-core/langchainrb/pull/884] Add `tool_execution_callback` to `Langchain::Assistant`, a callback function (proc, lambda) that is called right before a tool is executed

## [0.19.1] - 2024-11-21
- [FEATURE] [https://github.com/patterns-ai-core/langchainrb/pull/858] Assistant, when using Anthropic, now also accepts image_url in the message.
- [FEATURE] [https://github.com/patterns-ai-core/langchainrb/pull/861] Clean up passing `max_tokens` to Anthropic constructor and chat method
- [FEATURE] [https://github.com/patterns-ai-core/langchainrb/pull/849] Langchain::Assistant now works with AWS Bedrock-hosted Anthropic models
- [OPTIM] [https://github.com/patterns-ai-core/langchainrb/pull/867] Refactor `GoogleGeminiMessage#to_hash` and `OpenAIMessage#to_hash` methods.
- [OPTIM] [https://github.com/patterns-ai-core/langchainrb/pull/849] Simplify Langchain::LLM::AwsBedrock class
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/869] AnthropicMessage now correctly handles tool calls with content.
- [OPTIM] [https://github.com/patterns-ai-core/langchainrb/pull/870] Assistant, when using Ollama (e.g.: llava model), now also accepts image_url in the message.

## [0.19.0] - 2024-10-23
- [BREAKING] [https://github.com/patterns-ai-core/langchainrb/pull/840] Rename `chat_completion_model_name` parameter to `chat_model` in Langchain::LLM parameters.
- [BREAKING] [https://github.com/patterns-ai-core/langchainrb/pull/840] Rename `completion_model_name` parameter to `completion_model` in Langchain::LLM parameters.
- [BREAKING] [https://github.com/patterns-ai-core/langchainrb/pull/840] Rename `embeddings_model_name` parameter to `embedding_model` in Langchain::LLM parameters.
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/850/] Fix MistralAIMessage to handle "Tool" Output
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/837] Fix bug when tool functions with no input variables are used with Langchain::LLM::Anthropic
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/836] Fix bug when assistant.instructions = nil did not remove the system message
- [FEATURE] [https://github.com/patterns-ai-core/langchainrb/pull/838] Allow setting safety_settings: [] in default_options for Langchain::LLM::GoogleGemini and Langchain::LLM::GoogleVertexAI constructors
- [BUGFIX] [https://github.com/patterns-ai-core/langchainrb/pull/871] Allow passing in options hash to Ollama

## [0.18.0] - 2024-10-12
- [BREAKING] Remove `Langchain::Assistant#clear_thread!` method
- [BREAKING] `Langchain::Messages::*` namespace had migrated to `Langchain::Assistant::Messages::*`
- [BREAKING] Modify `Langchain::LLM::AwsBedrock` constructor to pass model options via default_options: {...}
- Introduce `Langchain::Assistant#parallel_tool_calls` options whether to allow the LLM to make multiple parallel tool calls. Default: true
- Minor improvements to the Langchain::Assistant class
- Added support for streaming with Anthropic
- Bump anthropic gem
- Default Langchain::LLM::Anthropic chat model is "claude-3-5-sonnet-20240620" now

## [0.17.1] - 2024-10-07
- Move Langchain::Assistant::LLM::Adapter-related classes to separate files
- Fix Langchain::Tool::Database#describe_table method

## [0.17.0] - 2024-10-02
- [BREAKING] Langchain::Vectorsearch::Milvus was rewritten to work with newer milvus 0.10.0 gem
- [BREAKING] Removing Langchain::LLM::GooglePalm
- Assistant can now process image_urls in the messages (currently only for OpenAI and Mistral AI)
- Vectorsearch providers utilize the global Langchain.logger
- Update required milvus, qdrant and weaviate versions

## [0.16.1] - 2024-09-30
- Deprecate Langchain::LLM::GooglePalm
- Allow setting response_object: {} parameter when initializing supported Langchain::LLM::* classes
- Simplify and consolidate logging for some of the LLM providers (namely OpenAI and Google). Now most of the HTTP requests are being logged when on DEBUG level
- Improve doc on how to set up a custom logger with a custom destination

## [0.16.0] - 2024-09-19
- Remove `Langchain::Thread` class as it was not needed.
- Support `cohere` provider for `Langchain::LLM::AwsBedrock#embed`

## [0.15.6] - 2024-09-16
- Throw an error when `Langchain::Assistant#add_message_callback` is not a callable proc.
- Resetting instructions on Langchain::Assistant with Google Gemini no longer throws an error.
- Add Meta models support for AWS Bedrock LLM

## [0.15.5] - 2024-09-10 üáßüá¶
- Fix for Langchain::Prompt::PromptTemplate supporting nested JSON data
- Require common libs at top-level
- Add `add_message_callback` to `Langchain::Assistant` constructor to invoke an optional function when any message is added to the conversation
- Adding Assistant syntactic sugar with #run! and #add_message_and_run!

## [0.15.4] - 2024-08-30
- Improve the Langchain::Tool::Database tool
- Allow explictly setting tool_choice on the Assistant instance
- Add support for bulk embedding in Ollama
- `Langchain::Assistant` works with `Langchain::LLM::MistralAI` llm
- Fix Langchain::LLM::Azure not applying full default_options

## [0.15.3] - 2024-08-27
- Fix OpenAI#embed when text-embedding-ada-002 is used

## [0.15.2] - 2024-08-23
- Add Assistant#add_messages() method and fix Assistant#messages= method

## [0.15.1] - 2024-08-19
- Drop `to_bool` gem in favour of custom util class
- Drop `colorize` which is GPL-licensed in favour of `rainbow`
- Improve Langchain::Tool::Weather tool

## [0.15.0] - 2024-08-14
- Fix Langchain::Assistant when llm is Anthropic
- Fix GoogleGemini#chat method
- Langchain::LLM::Weaviate initializer does not require api_key anymore
- [BREAKING] Langchain::LLM::OpenAI#chat() uses `gpt-4o-mini` by default instead of `gpt-3.5-turbo` previously.
- [BREAKING] Assistant works with a number of open-source models via Ollama.
- [BREAKING] Introduce new `Langchain::ToolDefinition` module to define tools. This replaces the previous reliance on subclassing from `Langchain::Tool::Base`.

## [0.14.0] - 2024-07-12
- Removed TokenLength validators
- Assistant works with a Mistral LLM now
- Assistant keeps track of tokens used
- Misc fixes and improvements

## [0.13.5] - 2024-07-01
- Add Milvus#remove_texts() method
- Langchain::Assistant has a `state` now
- Misc fixes and improvements

## [0.13.4] - 2024-06-16
- Fix Chroma#remove_texts() method
- Fix NewsRetriever Tool returning non UTF-8 characters
- Misc fixes and improvements

## [0.13.3] - 2024-06-03
- New üõ†Ô∏è  `Langchain::Tool::Tavily` to execute search (better than the GoogleSearch tool)
- Remove `activesupport` dependency
- Misc fixes and improvements

## [0.13.2] - 2024-05-20
- New `Langchain::LLM::GoogleGemini#embed()` method
- `Langchain::Assistant` works with `Langchain::LLM::Anthropic` llm
- New XLS file processor
- Fixes and improvements

## [0.13.1] - 2024-05-14
- Better error handling for `Langchain::LLM::GoogleVertexAI`

## [0.13.0] - 2024-05-14
- New üõ†Ô∏è `Langchain::Tool::NewsRetriever` tool to fetch news via newsapi.org
- Langchain::Assistant works with `Langchain::LLM::GoogleVertexAI` and `Langchain::LLM::GoogleGemini` llms
- [BREAKING] Introduce new `Langchain::Messages::Base` abstraction

## [0.12.1] - 2024-05-13
- Langchain::LLM::Ollama now uses `llama3` by default
- Langchain::LLM::Anthropic#complete() now uses `claude-2.1` by default
- Updated with new OpenAI models, including `gpt-4o`
- New `Langchain::LLM::Cohere#chat()` method.
- Introducing `UnifiedParameters` to unify parameters across LLM classes

## [0.12.0] - 2024-04-22
- [BREAKING] Rename `dimension` parameter to `dimensions` everywhere

## [0.11.4] - 2024-04-19
- New `Langchain::LLM::AWSBedrock#chat()` to wrap Bedrock Claude requests
- New `Langchain::LLM::OllamaResponse#total_tokens()` method

## [0.11.3] - 2024-04-16
- New `Langchain::Processors::Pptx` to parse .pptx files
- New `Langchain::LLM::Anthropic#chat()` support
- Misc fixes

## [0.11.2]
- New `Langchain::Assistant#clear_thread!` and `Langchain::Assistant#instructions=` methods

## [0.11.1]
- Langchain::Tool::Vectorsearch that wraps Langchain::Vectorsearch::* classes. This allows the Assistant to call the tool and inject data from vector DBs.

## [0.11.0]
- Delete previously deprecated `Langchain::Agent::ReActAgent` and `Langchain::Agent::SQLQueryAgent` classes
- New `Langchain::Agent::FileSystem` tool that can read files, write to files, and list the contents of a directory

## [0.10.3]
- Bump dependencies
- Ollama#complete fix
- Misc fixes

## [0.10.2]
- New Langchain::LLM::Mistral
- Drop Ruby 3.0 support
- Fixes Zeitwerk::NameError

## [0.10.1] - GEM VERSION YANKED

## [0.10.0]
- Delete `Langchain::Conversation` class

## [0.9.5]
- Now using OpenAI's "text-embedding-3-small" model to generate embeddings
- Added `remove_texts(ids:)` method to Qdrant and Chroma
- Add Ruby 3.3 support

## [0.9.4]
- New `Ollama#summarize()` method
- Improved README
- Fixes + specs

## [0.9.3]
- Add EML processor
- Tools can support multiple-methods
- Bump gems and bug fixes

## [0.9.2]
- Fix vectorsearch#ask methods
- Bump cohere-ruby gem

## [0.9.1]
- Add support for new OpenAI models
- Add Ollama#chat method
- Fix and refactor of `Langchain::LLM::Ollama`, responses can now be streamed.

## [0.9.0]
- Introducing new `Langchain::Assistant` that will be replacing `Langchain::Conversation` and `Langchain::Agent`s.
- `Langchain::Conversation` is deprecated.

## [0.8.2]
- Introducing new `Langchain::Chunker::Markdown` chunker (thanks @spikex)
- Fixes

## [0.8.1]
- Support for Epsilla vector DB
- Fully functioning Google Vertex AI LLM
- Bug fixes

## [0.8.0]
- [BREAKING] Updated llama_cpp.rb to 0.9.4. The model file format used by the underlying llama.cpp library has changed to GGUF. llama.cpp ships with scripts to convert existing files and GGUF format models can be downloaded from HuggingFace.
- Introducing Langchain::LLM::GoogleVertexAi LLM provider

## [0.7.5] - 2023-11-13
- Fixes

## [0.7.4] - 2023-11-10
- AWS Bedrock is available as an LLM provider. Available models from AI21, Cohere, AWS, and Anthropic.

## [0.7.3] - 2023-11-08
- LLM response passes through the context in RAG cases
- Fix gpt-4 token length validation

## [0.7.2] - 2023-11-02
- Azure OpenAI LLM support

## [0.7.1] - 2023-10-26
- Ragas evals tool to evaluate Retrieval Augmented Generation (RAG) pipelines

## [0.7.0] - 2023-10-22
- BREAKING: Moving Rails-specific code to `langchainrb_rails` gem

## [0.6.19] - 2023-10-18
- Elasticsearch vector search support
- Fix `lib/langchain/railtie.rb` not being loaded with the gem

## [0.6.18] - 2023-10-16
- Introduce `Langchain::LLM::Response`` object
- Introduce `Langchain::Chunk` object
- Add the ask() method to the Langchain::ActiveRecord::Hooks

## [0.6.17] - 2023-10-10
- Bump weaviate and chroma-db deps
- `Langchain::Chunker::Semantic` chunker
- Re-structure Conversations class
- Bug fixes

## [0.6.16] - 2023-10-02
- HyDE-style similarity search
- `Langchain::Chunker::Sentence` chunker
- Bug fixes

## [0.6.15] - 2023-09-22
- Bump weaviate-ruby gem version
- Ollama support

## [0.6.14] - 2023-09-11
- Add `find` method to `Langchain::Vectorsearch::Qdrant`
- Enhance Google search output
- Raise ApiError when OpenAI returns an error
- Update OpenAI `complete` method to use chat completion api
  - Deprecate legacy completion models. See https://platform.openai.com/docs/deprecations/2023-07-06-gpt-and-embeddings

## [0.6.13] - 2023-08-23
- Add `k:` parameter to all `ask()` vector search methods
- Bump Faraday to 2.x

## [0.6.12] - 2023-08-13

## [0.6.11] - 2023-08-08

## [0.6.10] - 2023-08-01
- üó£Ô∏è LLMs
  - Introducing Anthropic support

## [0.6.9] - 2023-07-29

## [0.6.8] - 2023-07-21

## [0.6.7] - 2023-07-19
- Support for OpenAI functions
- Streaming vectorsearch ask() responses

## [0.6.6] - 2023-07-13
- Langchain::Chunker::RecursiveText
- Fixes

## [0.6.5] - 2023-07-06
- üó£Ô∏è LLMs
  - Introducing Llama.cpp support
- Langchain::OutputParsers::OutputFixingParser to wrap a Langchain::OutputParser and handle invalid response

## [0.6.4] - 2023-07-01
- Fix `Langchain::Vectorsearch::Qdrant#add_texts()`
- Introduce `ConversationMemory`
- Allow loading multiple files from a directory
- Add `get_default_schema()`, `create_default_schema()`, `destroy_default_schema()` missing methods to `Langchain::Vectorsearch::*` classes

## [0.6.3] - 2023-06-25
- Add #destroy_default_schema() to Langchain::Vectorsearch::* classes

## [0.6.2] - 2023-06-25
- Qdrant, Chroma, and Pinecone are supported by ActiveRecord hooks

## [0.6.1] - 2023-06-24
- Adding support to hook vectorsearch into ActiveRecord models

## [0.6.0] - 2023-06-22
- [BREAKING] Rename `ChainOfThoughtAgent` to `ReActAgent`
- Implement A21 token validator
- Add `Langchain::OutputParsers`

## [0.5.7] - 2023-06-19
- Developer can modify models used when initiliazing `Langchain::LLM::*` clients
- Improvements to the `SQLQueryAgent` and the database tool

## [0.5.6] - 2023-06-18
- If used with OpenAI, Langchain::Conversation responses can now be streamed.
- Improved logging
- Langchain::Tool::SerpApi has been renamed to Langchain::Tool::GoogleSearch
- JSON prompt templates have been converted to YAML
- Langchain::Chunker::Text is introduced to provide simple text chunking functionality
- Misc fixes and improvements

## [0.5.5] - 2023-06-12
- [BREAKING] Rename `Langchain::Chat` to `Langchain::Conversation`
- üõ†Ô∏è Tools
  - Introducing `Langchain::Tool::Weather`, a tool that calls Open Weather API to retrieve the current weather

## [0.5.4] - 2023-06-10
- üîç Vectorsearch
  - Introducing support for HNSWlib
- Improved and new `Langchain::Chat` interface that persists chat history in memory

## [0.5.3] - 2023-06-09
- üó£Ô∏è LLMs
  - Chat message history support for Langchain::LLM::GooglePalm and Langchain::LLM::OpenAI

## [0.5.2] - 2023-06-07
- üó£Ô∏è LLMs
  - Auto-calculate the max_tokens: setting to be passed on to OpenAI

## [0.5.1] - 2023-06-06
- üõ†Ô∏è Tools
  - Modified Tool usage. Agents now accept Tools instances instead of Tool strings.

## [0.5.0] - 2023-06-05
- [BREAKING] LLMs are now passed as objects to Vectorsearch classes instead of `llm: :name, llm_api_key:` previously
- üìã Prompts
  - YAML prompt templates are now supported
- üöö Loaders
  - Introduce `Langchain::Processors::Xlsx` to parse .xlsx files

## [0.4.2] - 2023-06-03
- üó£Ô∏è LLMs
  - Introducing support for AI21
- Better docs generation
- Refactors

## [0.4.1] - 2023-06-02
- Beautiful colored log messages
- üõ†Ô∏è Tools
  - Introducing `Langchain::Tool::RubyCodeInterpreter`, a tool executes sandboxed Ruby code

## [0.4.0] - 2023-06-01
- [BREAKING] Everything is namespaced under `Langchain::` now
- Pgvector similarity search uses the cosine distance by default now
- OpenAI token length validation using tiktoken_ruby

## [0.3.15] - 2023-05-30
- Drop Ruby 2.7 support. It had reached EOD.
- Bump pgvector-ruby to 0.2
- üöö Loaders
  - Support for options and block to be passed to CSV processor

## [0.3.14] - 2023-05-28
- üîç Vectorsearch
  - Not relying on Weaviate modules anymore
  - Adding missing specs for Qdrant and Milvus classes
- üöö Loaders
  - Add Langchain::Data result object for data loaders
- üó£Ô∏è LLMs
  - Add `summarize()` method to the LLMs

## [0.3.13] - 2023-05-26
- üîç Vectorsearch
  - Pgvector support
- üöö Loaders
  - CSV loader
  - JSON loader
  - JSONL loader

## [0.3.12] - 2023-05-25
- üîç Vectorsearch
  - Introduce namespace support for Pinecone
- üöö Loaders
  - Loaders overhaul

## [0.3.11] - 2023-05-23
- üó£Ô∏è LLMs
  - Introducing support for Google PaLM (Pathways Language Model)
- Bug fixes and improvements

## [0.3.10] - 2023-05-19
- üó£Ô∏è LLMs
  - Introducing support for Replicate.com

## [0.3.9] - 2023-05-19
- üöö Loaders
  - Introduce `Loaders::Docx` to parse .docx files

## [0.3.8] - 2023-05-19
- üîç Vectorsearch
  - Introduce support for Chroma DB

- üöö Loaders
  - Bug fix `Loaders::Text` to only parse .txt files

## [0.3.7] - 2023-05-19
- üöö Loaders
  - Introduce `Loaders::Text` to parse .txt files
  - Introduce `Loaders::PDF` to parse .pdf files

## [0.3.6] - 2023-05-17
- üó£Ô∏è LLMs
  - Bump `hugging-face` gem version

## [0.3.5] - 2023-05-16
- Bug fixes

## [0.3.4] - 2023-05-16
- üó£Ô∏è LLMs
  - Introducing support for HuggingFace

## [0.3.3] - 2023-05-16
- Dependencies are now optionally loaded and required at runtime
- Start using `standardrb` for linting
- Use the Ruby logger

## [0.3.2] - 2023-05-15
- ü§ñ Agents
  - Fix Chain of Thought prompt loader

## [0.3.1] - 2023-05-12
- üõ†Ô∏è Tools
  - Introducing `Tool::Wikipedia`, a tool that looks up Wikipedia entries

## [0.3.0] - 2023-05-12
- ü§ñ Agents
  - Introducing `Agent::ChainOfThoughtAgent`, a semi-autonomous bot that uses Tools to retrieve additional information in order to make best-effort informed replies to user's questions.
- üõ†Ô∏è Tools
  - Introducing `Tool::Calculator` tool that solves mathematical expressions.
  - Introducing `Tool::Search` tool that executes Google Searches.

## [0.2.0] - 2023-05-09
- üìã Prompt Templating
  - Ability to create prompt templates and save them to JSON files
  - Default `Prompt::FewShotPromptTemplate`
  - New examples added to `examples/`

## [0.1.4] - 2023-05-02
- Backfilling missing specs

## [0.1.3] - 2023-05-01
- Initial release

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/README.md`:

```md
üíéüîó Langchain.rb
---
‚ö° Building LLM-powered applications in Ruby ‚ö°

For deep Rails integration see: [langchainrb_rails](https://github.com/andreibondarev/langchainrb_rails) gem.

Available for paid consulting engagements! [Email me](mailto:andrei@sourcelabs.io).

![Tests status](https://github.com/andreibondarev/langchainrb/actions/workflows/ci.yml/badge.svg?branch=main)
[![Gem Version](https://badge.fury.io/rb/langchainrb.svg)](https://badge.fury.io/rb/langchainrb)
[![Docs](http://img.shields.io/badge/yard-docs-blue.svg)](http://rubydoc.info/gems/langchainrb)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](https://github.com/andreibondarev/langchainrb/blob/main/LICENSE.txt)
[![](https://dcbadge.vercel.app/api/server/WDARp7J2n8?compact=true&style=flat)](https://discord.gg/WDARp7J2n8)
[![X](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40rushing_andrei)](https://twitter.com/rushing_andrei)

## Use Cases
* Retrieval Augmented Generation (RAG) and vector search
* [Assistants](#assistants) (chat bots)

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Unified Interface for LLMs](#unified-interface-for-llms)
- [Prompt Management](#prompt-management)
- [Output Parsers](#output-parsers)
- [Building RAG](#building-retrieval-augment-generation-rag-system)
- [Assistants](#assistants)
- [Evaluations](#evaluations-evals)
- [Examples](#examples)
- [Logging](#logging)
- [Problems](#problems)
- [Development](#development)
- [Discord](#discord)

## Installation

Install the gem and add to the application's Gemfile by executing:

    bundle add langchainrb

If bundler is not being used to manage dependencies, install the gem by executing:

    gem install langchainrb

Additional gems may be required. They're not included by default so you can include only what you need.

## Usage

```ruby
require "langchain"
```

# Unified Interface for LLMs

The `Langchain::LLM` module provides a unified interface for interacting with various Large Language Model (LLM) providers. This abstraction allows you to easily switch between different LLM backends without changing your application code.

## Supported LLM Providers

- AI21
- Anthropic
- AWS Bedrock
- Azure OpenAI
- Cohere
- Google Gemini
- Google Vertex AI
- HuggingFace
- LlamaCpp
- Mistral AI
- Ollama
- OpenAI
- Replicate

## Usage

All LLM classes inherit from `Langchain::LLM::Base` and provide a consistent interface for common operations:

1. Generating embeddings
2. Generating prompt completions
3. Generating chat completions

### Initialization

Most LLM classes can be initialized with an API key and optional default options:

```ruby
llm = Langchain::LLM::OpenAI.new(
  api_key: ENV["OPENAI_API_KEY"],
  default_options: { temperature: 0.7, chat_model: "gpt-4o" }
)
```

### Generating Embeddings

Use the `embed` method to generate embeddings for given text:

```ruby
response = llm.embed(text: "Hello, world!")
embedding = response.embedding
```

#### Accepted parameters for `embed()`

- `text`: (Required) The input text to embed.
- `model`: (Optional) The model name to use or default embedding model will be used.

### Prompt completions

Use the `complete` method to generate completions for a given prompt:

```ruby
response = llm.complete(prompt: "Once upon a time")
completion = response.completion
```

#### Accepted parameters for `complete()`

- `prompt`: (Required) The input prompt for completion.
- `max_tokens`: (Optional) The maximum number of tokens to generate.
- `temperature`: (Optional) Controls randomness in generation. Higher values (e.g., 0.8) make output more random, while lower values (e.g., 0.2) make it more deterministic.
- `top_p`: (Optional) An alternative to temperature, controls diversity of generated tokens.
- `n`: (Optional) Number of completions to generate for each prompt.
- `stop`: (Optional) Sequences where the API will stop generating further tokens.
- `presence_penalty`: (Optional) Penalizes new tokens based on their presence in the text so far.
- `frequency_penalty`: (Optional) Penalizes new tokens based on their frequency in the text so far.

### Generating Chat Completions

Use the `chat` method to generate chat completions:

```ruby
messages = [
  { role: "system", content: "You are a helpful assistant." },
  { role: "user", content: "What's the weather like today?" }
  # Google Gemini and Google VertexAI expect messages in a different format:
  # { role: "user", parts: [{ text: "why is the sky blue?" }]}
]
response = llm.chat(messages: messages)
chat_completion = response.chat_completion
```

#### Accepted parameters for `chat()`

- `messages`: (Required) An array of message objects representing the conversation history.
- `model`: (Optional) The specific chat model to use.
- `temperature`: (Optional) Controls randomness in generation.
- `top_p`: (Optional) An alternative to temperature, controls diversity of generated tokens.
- `n`: (Optional) Number of chat completion choices to generate.
- `max_tokens`: (Optional) The maximum number of tokens to generate in the chat completion.
- `stop`: (Optional) Sequences where the API will stop generating further tokens.
- `presence_penalty`: (Optional) Penalizes new tokens based on their presence in the text so far.
- `frequency_penalty`: (Optional) Penalizes new tokens based on their frequency in the text so far.
- `logit_bias`: (Optional) Modifies the likelihood of specified tokens appearing in the completion.
- `user`: (Optional) A unique identifier representing your end-user.
- `tools`: (Optional) A list of tools the model may call.
- `tool_choice`: (Optional) Controls how the model calls functions.

## Switching LLM Providers

Thanks to the unified interface, you can easily switch between different LLM providers by changing the class you instantiate:

```ruby
# Using Anthropic
anthropic_llm = Langchain::LLM::Anthropic.new(api_key: ENV["ANTHROPIC_API_KEY"])

# Using Google Gemini
gemini_llm = Langchain::LLM::GoogleGemini.new(api_key: ENV["GOOGLE_GEMINI_API_KEY"])

# Using OpenAI
openai_llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
```

## Response Objects

Each LLM method returns a response object that provides a consistent interface for accessing the results:

- `embedding`: Returns the embedding vector
- `completion`: Returns the generated text completion
- `chat_completion`: Returns the generated chat completion
- `tool_calls`: Returns tool calls made by the LLM
- `prompt_tokens`: Returns the number of tokens in the prompt
- `completion_tokens`: Returns the number of tokens in the completion
- `total_tokens`: Returns the total number of tokens used

> [!NOTE]
> While the core interface is consistent across providers, some LLMs may offer additional features or parameters. Consult the documentation for each LLM class to learn about provider-specific capabilities and options.

### Prompt Management

#### Prompt Templates

Create a prompt with input variables:

```ruby
prompt = Langchain::Prompt::PromptTemplate.new(template: "Tell me a {adjective} joke about {content}.", input_variables: ["adjective", "content"])
prompt.format(adjective: "funny", content: "chickens") # "Tell me a funny joke about chickens."
```

Creating a PromptTemplate using just a prompt and no input_variables:

```ruby
prompt = Langchain::Prompt::PromptTemplate.from_template("Tell me a funny joke about chickens.")
prompt.input_variables # []
prompt.format # "Tell me a funny joke about chickens."
```

Save prompt template to JSON file:

```ruby
prompt.save(file_path: "spec/fixtures/prompt/prompt_template.json")
```

Loading a new prompt template using a JSON file:

```ruby
prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.json")
prompt.input_variables # ["adjective", "content"]
```

#### Few Shot Prompt Templates

Create a prompt with a few shot examples:

```ruby
prompt = Langchain::Prompt::FewShotPromptTemplate.new(
  prefix: "Write antonyms for the following words.",
  suffix: "Input: {adjective}\nOutput:",
  example_prompt: Langchain::Prompt::PromptTemplate.new(
    input_variables: ["input", "output"],
    template: "Input: {input}\nOutput: {output}"
  ),
  examples: [
    { "input": "happy", "output": "sad" },
    { "input": "tall", "output": "short" }
  ],
   input_variables: ["adjective"]
)

prompt.format(adjective: "good")

# Write antonyms for the following words.
#
# Input: happy
# Output: sad
#
# Input: tall
# Output: short
#
# Input: good
# Output:
```

Save prompt template to JSON file:

```ruby
prompt.save(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
```

Loading a new prompt template using a JSON file:

```ruby
prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
prompt.prefix # "Write antonyms for the following words."
```

Loading a new prompt template using a YAML file:

```ruby
prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.yaml")
prompt.input_variables #=> ["adjective", "content"]
```


### Output Parsers

Parse LLM text responses into structured output, such as JSON.

#### Structured Output Parser

You can use the `StructuredOutputParser` to generate a prompt that instructs the LLM to provide a JSON response adhering to a specific JSON schema:

```ruby
json_schema = {
  type: "object",
  properties: {
    name: {
      type: "string",
      description: "Persons name"
    },
    age: {
      type: "number",
      description: "Persons age"
    },
    interests: {
      type: "array",
      items: {
        type: "object",
        properties: {
          interest: {
            type: "string",
            description: "A topic of interest"
          },
          levelOfInterest: {
            type: "number",
            description: "A value between 0 and 100 of how interested the person is in this interest"
          }
        },
        required: ["interest", "levelOfInterest"],
        additionalProperties: false
      },
      minItems: 1,
      maxItems: 3,
      description: "A list of the person's interests"
    }
  },
  required: ["name", "age", "interests"],
  additionalProperties: false
}
parser = Langchain::OutputParsers::StructuredOutputParser.from_json_schema(json_schema)
prompt = Langchain::Prompt::PromptTemplate.new(template: "Generate details of a fictional character.\n{format_instructions}\nCharacter description: {description}", input_variables: ["description", "format_instructions"])
prompt_text = prompt.format(description: "Korean chemistry student", format_instructions: parser.get_format_instructions)
# Generate details of a fictional character.
# You must format your output as a JSON value that adheres to a given "JSON Schema" instance.
# ...
```

Then parse the llm response:

```ruby
llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
llm_response = llm.chat(messages: [{role: "user", content: prompt_text}]).completion
parser.parse(llm_response)
# {
#   "name" => "Kim Ji-hyun",
#   "age" => 22,
#   "interests" => [
#     {
#       "interest" => "Organic Chemistry",
#       "levelOfInterest" => 85
#     },
#     ...
#   ]
# }
```

If the parser fails to parse the LLM response, you can use the `OutputFixingParser`. It sends an error message, prior output, and the original prompt text to the LLM, asking for a "fixed" response:

```ruby
begin
  parser.parse(llm_response)
rescue Langchain::OutputParsers::OutputParserException => e
  fix_parser = Langchain::OutputParsers::OutputFixingParser.from_llm(
    llm: llm,
    parser: parser
  )
  fix_parser.parse(llm_response)
end
```

Alternatively, if you don't need to handle the `OutputParserException`, you can simplify the code:

```ruby
# we already have the `OutputFixingParser`:
# parser = Langchain::OutputParsers::StructuredOutputParser.from_json_schema(json_schema)
fix_parser = Langchain::OutputParsers::OutputFixingParser.from_llm(
  llm: llm,
  parser: parser
)
fix_parser.parse(llm_response)
```

See [here](https://github.com/andreibondarev/langchainrb/tree/main/examples/create_and_manage_prompt_templates_using_structured_output_parser.rb) for a concrete example

## Building Retrieval Augment Generation (RAG) system
RAG is a methodology that assists LLMs generate accurate and up-to-date information.
A typical RAG workflow follows the 3 steps below:
1. Relevant knowledge (or data) is retrieved from the knowledge base (typically a vector search DB)
2. A prompt, containing retrieved knowledge above, is constructed.
3. LLM receives the prompt above to generate a text completion.
Most common use-case for a RAG system is powering Q&A systems where users pose natural language questions and receive answers in natural language.

### Vector search databases
Langchain.rb provides a convenient unified interface on top of supported vectorsearch databases that make it easy to configure your index, add data, query and retrieve from it.

#### Supported vector search databases and features:

| Database                                                                                   | Open-source        | Cloud offering     |
| --------                                                                                   |:------------------:| :------------:     |
| [Chroma](https://trychroma.com/?utm_source=langchainrb&utm_medium=github)                  | ‚úÖ                 | ‚úÖ                 |
| [Epsilla](https://epsilla.com/?utm_source=langchainrb&utm_medium=github)                   | ‚úÖ                 | ‚úÖ                 |
| [Hnswlib](https://github.com/nmslib/hnswlib/?utm_source=langchainrb&utm_medium=github)     | ‚úÖ                 | ‚ùå                 |
| [Milvus](https://milvus.io/?utm_source=langchainrb&utm_medium=github)                      | ‚úÖ                 | ‚úÖ Zilliz Cloud    |
| [Pinecone](https://www.pinecone.io/?utm_source=langchainrb&utm_medium=github)              | ‚ùå                 | ‚úÖ                 |
| [Pgvector](https://github.com/pgvector/pgvector/?utm_source=langchainrb&utm_medium=github) | ‚úÖ                 | ‚úÖ                 |
| [Qdrant](https://qdrant.tech/?utm_source=langchainrb&utm_medium=github)                    | ‚úÖ                 | ‚úÖ                 |
| [Weaviate](https://weaviate.io/?utm_source=langchainrb&utm_medium=github)                  | ‚úÖ                 | ‚úÖ                 |
| [Elasticsearch](https://www.elastic.co/?utm_source=langchainrb&utm_medium=github)          | ‚úÖ                 | ‚úÖ                 |

### Using Vector Search Databases üîç

Pick the vector search database you'll be using, add the gem dependency and instantiate the client:
```ruby
gem "weaviate-ruby", "~> 0.8.9"
```

Choose and instantiate the LLM provider you'll be using to generate embeddings
```ruby
llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
```

```ruby
client = Langchain::Vectorsearch::Weaviate.new(
    url: ENV["WEAVIATE_URL"],
    api_key: ENV["WEAVIATE_API_KEY"],
    index_name: "Documents",
    llm: llm
)
```

You can instantiate any other supported vector search database:
```ruby
client = Langchain::Vectorsearch::Chroma.new(...)   # `gem "chroma-db", "~> 0.6.0"`
client = Langchain::Vectorsearch::Epsilla.new(...)  # `gem "epsilla-ruby", "~> 0.0.3"`
client = Langchain::Vectorsearch::Hnswlib.new(...)  # `gem "hnswlib", "~> 0.8.1"`
client = Langchain::Vectorsearch::Milvus.new(...)   # `gem "milvus", "~> 0.9.3"`
client = Langchain::Vectorsearch::Pinecone.new(...) # `gem "pinecone", "~> 0.1.6"`
client = Langchain::Vectorsearch::Pgvector.new(...) # `gem "pgvector", "~> 0.2"`
client = Langchain::Vectorsearch::Qdrant.new(...)   # `gem "qdrant-ruby", "~> 0.9.3"`
client = Langchain::Vectorsearch::Elasticsearch.new(...)   # `gem "elasticsearch", "~> 8.2.0"`
```

Create the default schema:
```ruby
client.create_default_schema
```

Add plain text data to your vector search database:
```ruby
client.add_texts(
  texts: [
    "Begin by preheating your oven to 375¬∞F (190¬∞C). Prepare four boneless, skinless chicken breasts by cutting a pocket into the side of each breast, being careful not to cut all the way through. Season the chicken with salt and pepper to taste. In a large skillet, melt 2 tablespoons of unsalted butter over medium heat. Add 1 small diced onion and 2 minced garlic cloves, and cook until softened, about 3-4 minutes. Add 8 ounces of fresh spinach and cook until wilted, about 3 minutes. Remove the skillet from heat and let the mixture cool slightly.",
      "In a bowl, combine the spinach mixture with 4 ounces of softened cream cheese, 1/4 cup of grated Parmesan cheese, 1/4 cup of shredded mozzarella cheese, and 1/4 teaspoon of red pepper flakes. Mix until well combined. Stuff each chicken breast pocket with an equal amount of the spinach mixture. Seal the pocket with a toothpick if necessary. In the same skillet, heat 1 tablespoon of olive oil over medium-high heat. Add the stuffed chicken breasts and sear on each side for 3-4 minutes, or until golden brown."
  ]
)
```

Or use the file parsers to load, parse and index data into your database:
```ruby
my_pdf = Langchain.root.join("path/to/my.pdf")
my_text = Langchain.root.join("path/to/my.txt")
my_docx = Langchain.root.join("path/to/my.docx")

client.add_data(paths: [my_pdf, my_text, my_docx])
```
Supported file formats: docx, html, pdf, text, json, jsonl, csv, xlsx, eml, pptx.

Retrieve similar documents based on the query string passed in:
```ruby
client.similarity_search(
  query:,
  k:       # number of results to be retrieved
)
```

Retrieve similar documents based on the query string passed in via the [HyDE technique](https://arxiv.org/abs/2212.10496):
```ruby
client.similarity_search_with_hyde()
```

Retrieve similar documents based on the embedding passed in:
```ruby
client.similarity_search_by_vector(
  embedding:,
  k:       # number of results to be retrieved
)
```

RAG-based querying
```ruby
client.ask(question: "...")
```

## Assistants
`Langchain::Assistant` is a powerful and flexible class that combines Large Language Models (LLMs), tools, and conversation management to create intelligent, interactive assistants. It's designed to handle complex conversations, execute tools, and provide coherent responses based on the context of the interaction.

### Features
* Supports multiple LLM providers (OpenAI, Google Gemini, Anthropic, Mistral AI and open-source models via Ollama)
* Integrates with various tools to extend functionality
* Manages conversation threads
* Handles automatic and manual tool execution
* Supports different message formats for various LLM providers

### Usage
```ruby
llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
assistant = Langchain::Assistant.new(
  llm: llm,
  instructions: "You're a helpful AI assistant",
  tools: [Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])]
)

# Add a user message and run the assistant
assistant.add_message_and_run!(content: "What's the latest news about AI?")

# Supply an image to the assistant
assistant.add_message_and_run!(
  content: "Show me a picture of a cat",
  image_url: "https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg"
)

# Access the conversation thread
messages = assistant.messages

# Run the assistant with automatic tool execution
assistant.run(auto_tool_execution: true)

# If you want to stream the response, you can add a response handler
assistant = Langchain::Assistant.new(
  llm: llm,
  instructions: "You're a helpful AI assistant",
  tools: [Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])]
) do |response_chunk|
  # ...handle the response stream
  # print(response_chunk.inspect)
end
assistant.add_message(content: "Hello")
assistant.run(auto_tool_execution: true)
```

Note that streaming is not currently supported for all LLMs.

### Configuration
* `llm`: The LLM instance to use (required)
* `tools`: An array of tool instances (optional)
* `instructions`: System instructions for the assistant (optional)
* `tool_choice`: Specifies how tools should be selected. Default: "auto". A specific tool function name can be passed. This will force the Assistant to **always** use this function.
* `parallel_tool_calls`: Whether to make multiple parallel tool calls. Default: true
* `add_message_callback`: A callback function (proc, lambda) that is called when any message is added to the conversation (optional)
```ruby
assistant.add_message_callback = -> (message) { puts "New message: #{message}" }
```
* `tool_execution_callback`: A callback function (proc, lambda) that is called right before a tool is executed (optional)
```ruby
assistant.tool_execution_callback = -> (tool_call_id, tool_name, method_name, tool_arguments) { puts "Executing tool_call_id: #{tool_call_id}, tool_name: #{tool_name}, method_name: #{method_name}, tool_arguments: #{tool_arguments}" }
```

### Key Methods
* `add_message`: Adds a user message to the messages array
* `run!`: Processes the conversation and generates responses
* `add_message_and_run!`: Combines adding a message and running the assistant
* `submit_tool_output`: Manually submit output to a tool call
* `messages`: Returns a list of ongoing messages

### Built-in Tools üõ†Ô∏è
* `Langchain::Tool::Calculator`: Useful for evaluating math expressions. Requires `gem "eqn"`.
* `Langchain::Tool::Database`: Connect your SQL database. Requires `gem "sequel"`.
* `Langchain::Tool::FileSystem`: Interact with the file system (read & write).
* `Langchain::Tool::RubyCodeInterpreter`: Useful for evaluating generated Ruby code. Requires `gem "safe_ruby"` (In need of a better solution).
* `Langchain::Tool::NewsRetriever`: A wrapper around [NewsApi.org](https://newsapi.org) to fetch news articles.
* `Langchain::Tool::Tavily`: A wrapper around [Tavily AI](https://tavily.com).
* `Langchain::Tool::Weather`: Calls [Open Weather API](https://home.openweathermap.org) to retrieve the current weather.
* `Langchain::Tool::Wikipedia`: Calls Wikipedia API.

### Creating custom Tools
The Langchain::Assistant can be easily extended with custom tools by creating classes that `extend Langchain::ToolDefinition` module and implement required methods.
```ruby
class MovieInfoTool
  extend Langchain::ToolDefinition

  define_function :search_movie, description: "MovieInfoTool: Search for a movie by title" do
    property :query, type: "string", description: "The movie title to search for", required: true
  end

  define_function :get_movie_details, description: "MovieInfoTool: Get detailed information about a specific movie" do
    property :movie_id, type: "integer", description: "The TMDb ID of the movie", required: true
  end

  def initialize(api_key:)
    @api_key = api_key
  end

  def search_movie(query:)
    tool_response(...)
  end

  def get_movie_details(movie_id:)
    tool_response(...)
  end
end
```

#### Example usage:
```ruby
movie_tool = MovieInfoTool.new(api_key: "...")

assistant = Langchain::Assistant.new(
  llm: llm,
  instructions: "You're a helpful AI assistant that can provide movie information",
  tools: [movie_tool]
)

assistant.add_message_and_run(content: "Can you tell me about the movie 'Inception'?")
# Check the response in the last message in the conversation
assistant.messages.last
```

### Error Handling
The assistant includes error handling for invalid inputs, unsupported LLM types, and tool execution failures. It uses a state machine to manage the conversation flow and handle different scenarios gracefully.

### Demos
1. [Building an AI Assistant that operates a simulated E-commerce Store](https://www.loom.com/share/83aa4fd8dccb492aad4ca95da40ed0b2)
2. [New Langchain.rb Assistants interface](https://www.loom.com/share/e883a4a49b8746c1b0acf9d58cf6da36)
3. [Langchain.rb Assistant demo with NewsRetriever and function calling on Gemini](https://youtu.be/-ieyahrpDpM&t=1477s) - [code](https://github.com/palladius/gemini-news-crawler)

## Evaluations (Evals)
The Evaluations module is a collection of tools that can be used to evaluate and track the performance of the output products by LLM and your RAG (Retrieval Augmented Generation) pipelines.

### RAGAS
Ragas helps you evaluate your Retrieval Augmented Generation (RAG) pipelines. The implementation is based on this [paper](https://arxiv.org/abs/2309.15217) and the original Python [repo](https://github.com/explodinggradients/ragas). Ragas tracks the following 3 metrics and assigns the 0.0 - 1.0 scores:
* Faithfulness - the answer is grounded in the given context.
* Context Relevance - the retrieved context is focused, containing little to no irrelevant information.
* Answer Relevance - the generated answer addresses the actual question that was provided.

```ruby
# We recommend using Langchain::LLM::OpenAI as your llm for Ragas
ragas = Langchain::Evals::Ragas::Main.new(llm: llm)

# The answer that the LLM generated
# The question (or the original prompt) that was asked
# The context that was retrieved (usually from a vectorsearch database)
ragas.score(answer: "", question: "", context: "")
# =>
# {
#   ragas_score: 0.6601257446503674,
#   answer_relevance_score: 0.9573145866787608,
#   context_relevance_score: 0.6666666666666666,
#   faithfulness_score: 0.5
# }
```

## Examples
Additional examples available: [/examples](https://github.com/andreibondarev/langchainrb/tree/main/examples)

## Logging

Langchain.rb uses the standard Ruby [Logger](https://ruby-doc.org/stdlib-2.4.0/libdoc/logger/rdoc/Logger.html) mechanism and defaults to same `level` value (currently `Logger::DEBUG`).

To show all log messages:

```ruby
Langchain.logger.level = Logger::DEBUG
```

The logger logs to `STDOUT` by default. In order to configure the log destination (ie. log to a file) do:

```ruby
Langchain.logger = Logger.new("path/to/file", **Langchain::LOGGER_OPTIONS)
```

## Problems
If you're having issues installing `unicode` gem required by `pragmatic_segmenter`, try running:
```bash
gem install unicode -- --with-cflags="-Wno-incompatible-function-pointer-types"
```

## Development

1. `git clone https://github.com/andreibondarev/langchainrb.git`
2. `cp .env.example .env`, then fill out the environment variables in `.env`
3. `bundle exec rake` to ensure that the tests pass and to run standardrb
4. `bin/console` to load the gem in a REPL session. Feel free to add your own instances of LLMs, Tools, Agents, etc. and experiment with them.
5. Optionally, install lefthook git hooks for pre-commit to auto lint: `gem install lefthook && lefthook install -f`

## Discord
Join us in the [Langchain.rb](https://discord.gg/WDARp7J2n8) Discord server.

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=andreibondarev/langchainrb&type=Date)](https://star-history.com/#andreibondarev/langchainrb&Date)

## Contributing

Bug reports and pull requests are welcome on GitHub at https://github.com/andreibondarev/langchainrb.

## License

The gem is available as open source under the terms of the [MIT License](https://opensource.org/licenses/MIT).

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain.rb`:

```rb
# frozen_string_literal: true

require "logger"
require "pathname"
require "zeitwerk"
require "uri"
require "json"

loader = Zeitwerk::Loader.for_gem
loader.ignore("#{__dir__}/langchainrb.rb")
loader.inflector.inflect(
  "ai21" => "AI21",
  "ai21_response" => "AI21Response",
  "ai21_validator" => "AI21Validator",
  "csv" => "CSV",
  "google_vertex_ai" => "GoogleVertexAI",
  "html" => "HTML",
  "json" => "JSON",
  "jsonl" => "JSONL",
  "llm" => "LLM",
  "mistral_ai" => "MistralAI",
  "mistral_ai_response" => "MistralAIResponse",
  "mistral_ai_message" => "MistralAIMessage",
  "openai" => "OpenAI",
  "openai_response" => "OpenAIResponse",
  "openai_message" => "OpenAIMessage",
  "pdf" => "PDF"
)

loader.collapse("#{__dir__}/langchain/llm/response")

# RubyCodeInterpreter does not work with Ruby 3.3;
# https://github.com/ukutaht/safe_ruby/issues/4
loader.ignore("#{__dir__}/langchain/tool/ruby_code_interpreter") if RUBY_VERSION >= "3.3.0"

loader.setup

# Langchain.rb a is library for building LLM-backed Ruby applications. It is an abstraction layer that sits on top of the emerging AI-related tools that makes it easy for developers to consume and string those services together.
#
# = Installation
# Install the gem and add to the application's Gemfile by executing:
#
#     $ bundle add langchainrb
#
# If bundler is not being used to manage dependencies, install the gem by executing:
#
#     $ gem install langchainrb
#
# Require the gem to start using it:
#
#     require "langchain"
#
# = Concepts
#
# == Processors
# Processors load and parse/process various data types such as CSVs, PDFs, Word documents, HTML pages, and others.
#
# == Chunkers
# Chunkers split data based on various available options such as delimeters, chunk sizes or custom-defined functions. Chunkers are used when data needs to be split up before being imported in vector databases.
#
# == Prompts
# Prompts are structured inputs to the LLMs. Prompts provide instructions, context and other user input that LLMs use to generate responses.
#
# == Large Language Models (LLMs)
# LLM is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.
#
# == Vectorsearch Databases
# Vector database is a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data.
#
# == Embedding
# Word embedding or word vector is an approach with which we represent documents and words. It is defined as a numeric vector input that allows words with similar meanings to have the same representation. It can approximate meaning and represent a word in a lower dimensional space.
#
#
# = Logging
#
# Langchain.rb uses standard logging mechanisms and defaults to :debug level. Most messages are at info level, but we will add debug or warn statements as needed. To show all log messages:
#
# Langchain.logger.level = :info
module Langchain
  class << self
    # @return [Logger]
    attr_accessor :logger
    # @return [Pathname]
    attr_reader :root
  end

  module Errors
    class BaseError < StandardError; end
  end

  module Colorizer
    class << self
      def red(str)
        "\e[31m#{str}\e[0m"
      end

      def green(str)
        "\e[32m#{str}\e[0m"
      end

      def yellow(str)
        "\e[33m#{str}\e[0m"
      end

      def blue(str)
        "\e[34m#{str}\e[0m"
      end

      def colorize_logger_msg(msg, severity)
        return msg unless msg.is_a?(String)

        return red(msg) if severity.to_sym == :ERROR
        return yellow(msg) if severity.to_sym == :WARN
        msg
      end
    end
  end

  LOGGER_OPTIONS = {
    progname: "Langchain.rb",

    formatter: ->(severity, time, progname, msg) do
      Logger::Formatter.new.call(
        severity,
        time,
        "[#{progname}]",
        Colorizer.colorize_logger_msg(msg, severity)
      )
    end
  }.freeze

  self.logger ||= ::Logger.new($stdout, **LOGGER_OPTIONS)

  @root = Pathname.new(__dir__)
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/base.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class Base
        attr_reader :role,
          :content,
          :image_url,
          :tool_calls,
          :tool_call_id

        # Check if the message came from a user
        #
        # @return [Boolean] true/false whether the message came from a user
        def user?
          role == "user"
        end

        # Check if the message came from an LLM
        #
        # @raise NotImplementedError if the subclass does not implement this method
        def llm?
          raise NotImplementedError, "Class #{self.class.name} must implement the method 'llm?'"
        end

        # Check if the message is a tool call
        #
        # @raise NotImplementedError if the subclass does not implement this method
        def tool?
          raise NotImplementedError, "Class #{self.class.name} must implement the method 'tool?'"
        end

        # Check if the message is a system prompt
        #
        # @raise NotImplementedError if the subclass does not implement this method
        def system?
          raise NotImplementedError, "Class #{self.class.name} must implement the method 'system?'"
        end

        # Returns the standardized role symbol based on the specific role methods
        #
        # @return [Symbol] the standardized role symbol (:system, :llm, :tool, :user, or :unknown)
        def standard_role
          return :user if user?
          return :llm if llm?
          return :tool if tool?
          return :system if system?

          # TODO: Should we return :unknown or raise an error?
          :unknown
        end

        def image
          image_url ? Utils::ImageWrapper.new(image_url) : nil
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/google_gemini_message.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class GoogleGeminiMessage < Base
        # Google Gemini uses the following roles:
        ROLES = [
          "user",
          "model",
          "function"
        ].freeze

        TOOL_ROLE = "function"

        # Initialize a new Google Gemini message
        #
        # @param role [String] The role of the message
        # @param content [String] The content of the message
        # @param tool_calls [Array<Hash>] The tool calls made in the message
        # @param tool_call_id [String] The ID of the tool call
        def initialize(role:, content: nil, tool_calls: [], tool_call_id: nil)
          raise ArgumentError, "Role must be one of #{ROLES.join(", ")}" unless ROLES.include?(role)
          raise ArgumentError, "Tool calls must be an array of hashes" unless tool_calls.is_a?(Array) && tool_calls.all? { |tool_call| tool_call.is_a?(Hash) }

          @role = role
          # Some Tools return content as a JSON hence `.to_s`
          @content = content.to_s
          @tool_calls = tool_calls
          @tool_call_id = tool_call_id
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def llm?
          model?
        end

        # Convert the message to a Google Gemini API-compatible hash
        #
        # @return [Hash] The message as a Google Gemini API-compatible hash
        def to_hash
          if tool?
            tool_hash
          elsif model?
            model_hash
          elsif user?
            user_hash
          end
        end

        # Google Gemini does not implement system prompts
        def system?
          false
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def tool?
          function?
        end

        # Check if the message is a user call
        #
        # @return [Boolean] true/false whether this message is a user call
        def user?
          role == "user"
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def function?
          role == "function"
        end

        # Convert the message to an GoogleGemini API-compatible hash
        # @return [Hash] The message as an GoogleGemini API-compatible hash, with the role as "model"
        def model_hash
          {
            role: role,
            parts: build_parts
          }
        end

        # Convert the message to an GoogleGemini API-compatible hash
        # @return [Hash] The message as an GoogleGemini API-compatible hash, with the role as "function"
        def tool_hash
          {
            role: role,
            parts: [{
              functionResponse: {
                name: tool_call_id,
                response: {
                  name: tool_call_id,
                  content: content
                }
              }
            }]
          }
        end

        # Convert the message to an GoogleGemini API-compatible hash
        # @return [Hash] The message as an GoogleGemini API-compatible hash, with the role as "user"
        def user_hash
          {
            role: role,
            parts: build_parts
          }
        end

        # Builds the part value for the message hash
        # @return [Array<Hash>] An array of content hashes of the text or of the tool calls if present
        def build_parts
          if tool_calls.any?
            tool_calls
          else
            [{text: content}]
          end
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def model?
          role == "model"
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/openai_message.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class OpenAIMessage < Base
        # OpenAI uses the following roles:
        ROLES = [
          "system",
          "assistant",
          "user",
          "tool"
        ].freeze

        TOOL_ROLE = "tool"

        # Initialize a new OpenAI message
        #
        # @param role [String] The role of the message
        # @param content [String] The content of the message
        # @param image_url [String] The URL of the image
        # @param tool_calls [Array<Hash>] The tool calls made in the message
        # @param tool_call_id [String] The ID of the tool call
        def initialize(
          role:,
          content: nil,
          image_url: nil,
          tool_calls: [],
          tool_call_id: nil
        )
          raise ArgumentError, "Role must be one of #{ROLES.join(", ")}" unless ROLES.include?(role)
          raise ArgumentError, "Tool calls must be an array of hashes" unless tool_calls.is_a?(Array) && tool_calls.all? { |tool_call| tool_call.is_a?(Hash) }

          @role = role
          # Some Tools return content as a JSON hence `.to_s`
          @content = content.to_s
          @image_url = image_url
          @tool_calls = tool_calls
          @tool_call_id = tool_call_id
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def llm?
          assistant?
        end

        # Convert the message to an OpenAI API-compatible hash
        #
        # @return [Hash] The message as an OpenAI API-compatible hash
        def to_hash
          if assistant?
            assistant_hash
          elsif system?
            system_hash
          elsif tool?
            tool_hash
          elsif user?
            user_hash
          end
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def assistant?
          role == "assistant"
        end

        # Check if the message are system instructions
        #
        # @return [Boolean] true/false whether this message are system instructions
        def system?
          role == "system"
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def tool?
          role == "tool"
        end

        def user?
          role == "user"
        end

        # Convert the message to an OpenAI API-compatible hash
        # @return [Hash] The message as an OpenAI API-compatible hash, with the role as "assistant"
        def assistant_hash
          if tool_calls.any?
            {
              role: "assistant",
              tool_calls: tool_calls
            }
          else
            {
              role: "assistant",
              content: build_content_array
            }
          end
        end

        # Convert the message to an OpenAI API-compatible hash
        # @return [Hash] The message as an OpenAI API-compatible hash, with the role as "system"
        def system_hash
          {
            role: "system",
            content: build_content_array
          }
        end

        # Convert the message to an OpenAI API-compatible hash
        # @return [Hash] The message as an OpenAI API-compatible hash, with the role as "tool"
        def tool_hash
          {
            role: "tool",
            tool_call_id: tool_call_id,
            content: build_content_array
          }
        end

        # Convert the message to an OpenAI API-compatible hash
        # @return [Hash] The message as an OpenAI API-compatible hash, with the role as "user"
        def user_hash
          {
            role: "user",
            content: build_content_array
          }
        end

        # Builds the content value for the message hash
        # @return [Array<Hash>] An array of content hashes, with keys :type and :text or :image_url.
        def build_content_array
          content_details = []
          if content && !content.empty?
            content_details << {
              type: "text",
              text: content
            }
          end

          if image_url
            content_details << {
              type: "image_url",
              image_url: {
                url: image_url
              }
            }
          end
          content_details
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/ollama_message.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class OllamaMessage < Base
        # OpenAI uses the following roles:
        ROLES = [
          "system",
          "assistant",
          "user",
          "tool"
        ].freeze

        TOOL_ROLE = "tool"

        # Initialize a new OpenAI message
        #
        # @param role [String] The role of the message
        # @param content [String] The content of the message
        # @param image_url [String] The URL of the image to include in the message
        # @param tool_calls [Array<Hash>] The tool calls made in the message
        # @param tool_call_id [String] The ID of the tool call
        def initialize(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
          raise ArgumentError, "Role must be one of #{ROLES.join(", ")}" unless ROLES.include?(role)
          raise ArgumentError, "Tool calls must be an array of hashes" unless tool_calls.is_a?(Array) && tool_calls.all? { |tool_call| tool_call.is_a?(Hash) }
          raise ArgumentError, "image_url must be a valid url" if image_url && !URI::DEFAULT_PARSER.make_regexp.match?(image_url)

          @role = role
          # Some Tools return content as a JSON hence `.to_s`
          @content = content.to_s
          @image_url = image_url
          @tool_calls = tool_calls
          @tool_call_id = tool_call_id
        end

        # Convert the message to an OpenAI API-compatible hash
        #
        # @return [Hash] The message as an OpenAI API-compatible hash
        def to_hash
          {}.tap do |h|
            h[:role] = role
            h[:content] = content if content # Content is nil for tool calls
            h[:images] = [image.base64] if image
            h[:tool_calls] = tool_calls if tool_calls.any?
            h[:tool_call_id] = tool_call_id if tool_call_id
          end
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def llm?
          assistant?
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def assistant?
          role == "assistant"
        end

        # Check if the message are system instructions
        #
        # @return [Boolean] true/false whether this message are system instructions
        def system?
          role == "system"
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def tool?
          role == "tool"
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/anthropic_message.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class AnthropicMessage < Base
        ROLES = [
          "assistant",
          "user",
          "tool_result"
        ].freeze

        TOOL_ROLE = "tool_result"

        # Initialize a new Anthropic message
        #
        # @param role [String] The role of the message
        # @param content [String] The content of the message
        # @param tool_calls [Array<Hash>] The tool calls made in the message
        # @param tool_call_id [String] The ID of the tool call
        def initialize(
          role:,
          content: nil,
          image_url: nil,
          tool_calls: [],
          tool_call_id: nil
        )
          raise ArgumentError, "Role must be one of #{ROLES.join(", ")}" unless ROLES.include?(role)
          raise ArgumentError, "Tool calls must be an array of hashes" unless tool_calls.is_a?(Array) && tool_calls.all? { |tool_call| tool_call.is_a?(Hash) }

          @role = role
          # Some Tools return content as a JSON hence `.to_s`
          @content = content.to_s
          @image_url = image_url
          @tool_calls = tool_calls
          @tool_call_id = tool_call_id
        end

        # Convert the message to an Anthropic API-compatible hash
        #
        # @return [Hash] The message as an Anthropic API-compatible hash
        def to_hash
          if assistant?
            assistant_hash
          elsif tool?
            tool_hash
          elsif user?
            user_hash
          end
        end

        # Convert the message to an Anthropic API-compatible hash
        #
        # @return [Hash] The message as an Anthropic API-compatible hash, with the role as "assistant"
        def assistant_hash
          content_array = []
          if content && !content.empty?
            content_array << {type: "text", text: content}
          end

          {
            role: "assistant",
            content: content_array.concat(tool_calls)
          }
        end

        # Convert the message to an Anthropic API-compatible hash
        #
        # @return [Hash] The message as an Anthropic API-compatible hash, with the role as "user"
        def tool_hash
          {
            role: "user",
            # TODO: Tool can also return images
            # https://docs.anthropic.com/en/docs/build-with-claude/tool-use#handling-tool-use-and-tool-result-content-blocks
            content: [
              {
                type: "tool_result",
                tool_use_id: tool_call_id,
                content: content
              }
            ]
          }
        end

        # Convert the message to an Anthropic API-compatible hash
        #
        # @return [Hash] The message as an Anthropic API-compatible hash, with the role as "user"
        def user_hash
          {
            role: "user",
            content: build_content_array
          }
        end

        # Builds the content value for the message hash
        # @return [Array<Hash>] An array of content hashes
        def build_content_array
          content_details = []

          if content && !content.empty?
            content_details << {
              type: "text",
              text: content
            }
          end

          if image
            content_details << {
              type: "image",
              source: {
                type: "base64",
                data: image.base64,
                media_type: image.mime_type
              }
            }
          end

          content_details
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def tool?
          role == "tool_result"
        end

        # Anthropic does not implement system prompts
        def system?
          false
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def assistant?
          role == "assistant"
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def llm?
          assistant?
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/messages/mistral_ai_message.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module Messages
      class MistralAIMessage < Base
        # MistralAI uses the following roles:
        ROLES = [
          "system",
          "assistant",
          "user",
          "tool"
        ].freeze

        TOOL_ROLE = "tool"

        # Initialize a new MistralAI message
        #
        # @param role [String] The role of the message
        # @param content [String] The content of the message
        # @param image_url [String] The URL of the image
        # @param tool_calls [Array<Hash>] The tool calls made in the message
        # @param tool_call_id [String] The ID of the tool call
        def initialize(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil) # TODO: Implement image_file: reference (https://platform.openai.com/docs/api-reference/messages/object#messages/object-content)
          raise ArgumentError, "Role must be one of #{ROLES.join(", ")}" unless ROLES.include?(role)
          raise ArgumentError, "Tool calls must be an array of hashes" unless tool_calls.is_a?(Array) && tool_calls.all? { |tool_call| tool_call.is_a?(Hash) }

          @role = role
          # Some Tools return content as a JSON hence `.to_s`
          @content = content.to_s
          # Make sure you're using the Pixtral model if you want to send image_url
          @image_url = image_url
          @tool_calls = tool_calls
          @tool_call_id = tool_call_id
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def llm?
          assistant?
        end

        # Convert the message to an MistralAI API-compatible hash
        #
        # @return [Hash] The message as an MistralAI API-compatible hash
        def to_hash
          if assistant?
            assistant_hash
          elsif system?
            system_hash
          elsif tool?
            tool_hash
          elsif user?
            user_hash
          end
        end

        # Check if the message came from an LLM
        #
        # @return [Boolean] true/false whether this message was produced by an LLM
        def assistant?
          role == "assistant"
        end

        # Check if the message are system instructions
        #
        # @return [Boolean] true/false whether this message are system instructions
        def system?
          role == "system"
        end

        # Check if the message is a tool call
        #
        # @return [Boolean] true/false whether this message is a tool call
        def tool?
          role == "tool"
        end

        # Convert the message to an MistralAI API-compatible hash
        # @return [Hash] The message as an MistralAI API-compatible hash, with the role as "assistant"
        def assistant_hash
          {
            role: "assistant",
            content: content,
            tool_calls: tool_calls,
            prefix: false
          }
        end

        # Convert the message to an MistralAI API-compatible hash
        # @return [Hash] The message as an MistralAI API-compatible hash, with the role as "system"
        def system_hash
          {
            role: "system",
            content: build_content_array
          }
        end

        # Convert the message to an MistralAI API-compatible hash
        # @return [Hash] The message as an MistralAI API-compatible hash, with the role as "tool"
        def tool_hash
          {
            role: "tool",
            content: content,
            tool_call_id: tool_call_id
          }
        end

        # Convert the message to an MistralAI API-compatible hash
        # @return [Hash] The message as an MistralAI API-compatible hash, with the role as "user"
        def user_hash
          {
            role: "user",
            content: build_content_array
          }
        end

        # Builds the content value for the message hash
        # @return [Array<Hash>] An array of content hashes, with keys :type and :text or :image_url.
        def build_content_array
          content_details = []

          if content && !content.empty?
            content_details << {
              type: "text",
              text: content
            }
          end

          if image_url
            content_details << {
              type: "image_url",
              image_url: image_url
            }
          end

          content_details
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/base.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class Base
          # Build the chat parameters for the LLM
          #
          # @param messages [Array] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            raise NotImplementedError, "Subclasses must implement build_chat_params"
          end

          # Extract the tool call information from the tool call hash
          #
          # @param tool_call [Hash] The tool call hash
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            raise NotImplementedError, "Subclasses must implement extract_tool_call_args"
          end

          # Build a message for the LLM
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::Base] The message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            raise NotImplementedError, "Subclasses must implement build_message"
          end

          # Does this adapter accept messages with role="system"?
          #
          # @return [Boolean] Whether the adapter supports system messages
          def support_system_message?
            raise NotImplementedError, "Subclasses must implement support_system_message?"
          end

          # Role name used to return the tool output
          #
          # @return [String] The tool role
          def tool_role
            raise NotImplementedError, "Subclasses must implement tool_role"
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/ollama.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class Ollama < Base
          # Build the chat parameters for the Ollama LLM
          #
          # @param messages [Array] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            Langchain.logger.warn "WARNING: `parallel_tool_calls:` is not supported by Ollama currently"
            Langchain.logger.warn "WARNING: `tool_choice:` is not supported by Ollama currently"

            params = {messages: messages}
            if tools.any?
              params[:tools] = build_tools(tools)
            end
            params
          end

          # Build an Ollama message
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::OllamaMessage] The Ollama message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            Messages::OllamaMessage.new(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)
          end

          # Extract the tool call information from the OpenAI tool call hash
          #
          # @param tool_call [Hash] The tool call hash
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            tool_call_id = tool_call.dig("id")

            function_name = tool_call.dig("function", "name")
            tool_name, method_name = function_name.split("__")

            tool_arguments = tool_call.dig("function", "arguments")
            tool_arguments = if tool_arguments.is_a?(Hash)
              Langchain::Utils::HashTransformer.symbolize_keys(tool_arguments)
            else
              JSON.parse(tool_arguments, symbolize_names: true)
            end

            [tool_call_id, tool_name, method_name, tool_arguments]
          end

          # Build the tools for the Ollama LLM
          def available_tool_names(tools)
            build_tools(tools).map { |tool| tool.dig(:function, :name) }
          end

          # Get the allowed assistant.tool_choice values for Ollama
          def allowed_tool_choices
            ["auto", "none"]
          end

          def tool_role
            Messages::OllamaMessage::TOOL_ROLE
          end

          def support_system_message?
            Messages::OllamaMessage::ROLES.include?("system")
          end

          private

          def build_tools(tools)
            tools.map { |tool| tool.class.function_schemas.to_openai_format }.flatten
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/google_gemini.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class GoogleGemini < Base
          # Build the chat parameters for the Google Gemini LLM
          #
          # @param messages [Array] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            Langchain.logger.warn "WARNING: `parallel_tool_calls:` is not supported by Google Gemini currently"

            params = {messages: messages}
            if tools.any?
              params[:tools] = build_tools(tools)
              params[:system] = instructions if instructions
              params[:tool_choice] = build_tool_config(tool_choice)
            end
            params
          end

          # Build a Google Gemini message
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::GoogleGeminiMessage] The Google Gemini message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            Langchain.logger.warn "Image URL is not supported by Google Gemini" if image_url

            Messages::GoogleGeminiMessage.new(role: role, content: content, tool_calls: tool_calls, tool_call_id: tool_call_id)
          end

          # Extract the tool call information from the Google Gemini tool call hash
          #
          # @param tool_call [Hash] The tool call hash, format: {"functionCall"=>{"name"=>"weather__execute", "args"=>{"input"=>"NYC"}}}
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            tool_call_id = tool_call.dig("functionCall", "name")
            function_name = tool_call.dig("functionCall", "name")
            tool_name, method_name = function_name.split("__")
            tool_arguments = tool_call.dig("functionCall", "args").transform_keys(&:to_sym)
            [tool_call_id, tool_name, method_name, tool_arguments]
          end

          # Build the tools for the Google Gemini LLM
          #
          # @param tools [Array<Langchain::Tool::Base>] The tools
          # @return [Array] The tools in Google Gemini format
          def build_tools(tools)
            tools.map { |tool| tool.class.function_schemas.to_google_gemini_format }.flatten
          end

          # Get the allowed assistant.tool_choice values for Google Gemini
          def allowed_tool_choices
            ["auto", "none"]
          end

          # Get the available tool names for Google Gemini
          def available_tool_names(tools)
            build_tools(tools).map { |tool| tool.dig(:name) }
          end

          def tool_role
            Messages::GoogleGeminiMessage::TOOL_ROLE
          end

          def support_system_message?
            Messages::GoogleGeminiMessage::ROLES.include?("system")
          end

          private

          def build_tool_config(choice)
            case choice
            when "auto"
              {function_calling_config: {mode: "auto"}}
            when "none"
              {function_calling_config: {mode: "none"}}
            else
              {function_calling_config: {mode: "any", allowed_function_names: [choice]}}
            end
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/anthropic.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class Anthropic < Base
          # Build the chat parameters for the Anthropic API
          #
          # @param messages [Array<Hash>] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array<Hash>] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            params = {messages: messages}
            if tools.any?
              params[:tools] = build_tools(tools)
              params[:tool_choice] = build_tool_choice(tool_choice, parallel_tool_calls)
            end
            params[:system] = instructions if instructions
            params
          end

          # Build an Anthropic message
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array<Hash>] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::AnthropicMessage] The Anthropic message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            Messages::AnthropicMessage.new(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)
          end

          # Extract the tool call information from the Anthropic tool call hash
          #
          # @param tool_call [Hash] The tool call hash, format: {"type"=>"tool_use", "id"=>"toolu_01TjusbFApEbwKPRWTRwzadR", "name"=>"news_retriever__get_top_headlines", "input"=>{"country"=>"us", "page_size"=>10}}], "stop_reason"=>"tool_use"}
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            tool_call_id = tool_call.dig("id")
            function_name = tool_call.dig("name")
            tool_name, method_name = function_name.split("__")
            tool_arguments = tool_call.dig("input").transform_keys(&:to_sym)
            [tool_call_id, tool_name, method_name, tool_arguments]
          end

          # Build the tools for the Anthropic API
          def build_tools(tools)
            tools.map { |tool| tool.class.function_schemas.to_anthropic_format }.flatten
          end

          # Get the allowed assistant.tool_choice values for Anthropic
          def allowed_tool_choices
            ["auto", "any"]
          end

          # Get the available tool function names for Anthropic
          #
          # @param tools [Array<Hash>] The tools
          # @return [Array<String>] The tool function names
          def available_tool_names(tools)
            build_tools(tools).map { |tool| tool.dig(:name) }
          end

          def tool_role
            Messages::AnthropicMessage::TOOL_ROLE
          end

          def support_system_message?
            Messages::AnthropicMessage::ROLES.include?("system")
          end

          private

          def build_tool_choice(choice, parallel_tool_calls)
            tool_choice_object = {disable_parallel_tool_use: !parallel_tool_calls}

            case choice
            when "auto"
              tool_choice_object[:type] = "auto"
            when "any"
              tool_choice_object[:type] = "any"
            else
              tool_choice_object[:type] = "tool"
              tool_choice_object[:name] = choice
            end

            tool_choice_object
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/mistral_ai.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class MistralAI < Base
          # Build the chat parameters for the Mistral AI LLM
          #
          # @param messages [Array] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            Langchain.logger.warn "WARNING: `parallel_tool_calls:` is not supported by Mistral AI currently"

            params = {messages: messages}
            if tools.any?
              params[:tools] = build_tools(tools)
              params[:tool_choice] = build_tool_choice(tool_choice)
            end
            params
          end

          # Build a Mistral AI message
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::MistralAIMessage] The Mistral AI message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            Messages::MistralAIMessage.new(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)
          end

          # Extract the tool call information from the OpenAI tool call hash
          #
          # @param tool_call [Hash] The tool call hash
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            tool_call_id = tool_call.dig("id")

            function_name = tool_call.dig("function", "name")
            tool_name, method_name = function_name.split("__")

            tool_arguments = tool_call.dig("function", "arguments")
            tool_arguments = if tool_arguments.is_a?(Hash)
              Langchain::Utils::HashTransformer.symbolize_keys(tool_arguments)
            else
              JSON.parse(tool_arguments, symbolize_names: true)
            end

            [tool_call_id, tool_name, method_name, tool_arguments]
          end

          # Build the tools for the Mistral AI LLM
          def build_tools(tools)
            tools.map { |tool| tool.class.function_schemas.to_openai_format }.flatten
          end

          # Get the allowed assistant.tool_choice values for Mistral AI
          def allowed_tool_choices
            ["auto", "none"]
          end

          # Get the available tool names for Mistral AI
          def available_tool_names(tools)
            build_tools(tools).map { |tool| tool.dig(:function, :name) }
          end

          def tool_role
            Messages::MistralAIMessage::TOOL_ROLE
          end

          def support_system_message?
            Messages::MistralAIMessage::ROLES.include?("system")
          end

          private

          def build_tool_choice(choice)
            case choice
            when "auto"
              choice
            else
              {"type" => "function", "function" => {"name" => choice}}
            end
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/aws_bedrock_anthropic.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class AwsBedrockAnthropic < Anthropic
          private

          # @param [String] choice
          # @param [Boolean] _parallel_tool_calls
          # @return [Hash]
          def build_tool_choice(choice, _parallel_tool_calls)
            # Aws Bedrock hosted Anthropic does not support parallel tool calls
            Langchain.logger.warn "WARNING: parallel_tool_calls is not supported by AWS Bedrock Anthropic currently"

            tool_choice_object = {}

            case choice
            when "auto"
              tool_choice_object[:type] = "auto"
            when "any"
              tool_choice_object[:type] = "any"
            else
              tool_choice_object[:type] = "tool"
              tool_choice_object[:name] = choice
            end

            tool_choice_object
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapters/openai.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      module Adapters
        class OpenAI < Base
          # Build the chat parameters for the OpenAI LLM
          #
          # @param messages [Array] The messages
          # @param instructions [String] The system instructions
          # @param tools [Array] The tools to use
          # @param tool_choice [String] The tool choice
          # @param parallel_tool_calls [Boolean] Whether to make parallel tool calls
          # @return [Hash] The chat parameters
          def build_chat_params(
            messages:,
            instructions:,
            tools:,
            tool_choice:,
            parallel_tool_calls:
          )
            params = {messages: messages}
            if tools.any?
              params[:tools] = build_tools(tools)
              params[:tool_choice] = build_tool_choice(tool_choice)
              # Temporary fix because OpenAI o1/o3/reasoning models don't support `parallel_tool_calls` parameter.
              # Set `Assistant.new(parallel_tool_calls: nil, ...)` to avoid the error.
              params[:parallel_tool_calls] = parallel_tool_calls unless parallel_tool_calls.nil?
            end
            params
          end

          # Build a OpenAI message
          #
          # @param role [String] The role of the message
          # @param content [String] The content of the message
          # @param image_url [String] The image URL
          # @param tool_calls [Array] The tool calls
          # @param tool_call_id [String] The tool call ID
          # @return [Messages::OpenAIMessage] The OpenAI message
          def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
            Messages::OpenAIMessage.new(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)
          end

          # Extract the tool call information from the OpenAI tool call hash
          #
          # @param tool_call [Hash] The tool call hash
          # @return [Array] The tool call information
          def extract_tool_call_args(tool_call:)
            tool_call_id = tool_call.dig("id")

            function_name = tool_call.dig("function", "name")
            tool_name, method_name = function_name.split("__")

            tool_arguments = tool_call.dig("function", "arguments")
            tool_arguments = if tool_arguments.is_a?(Hash)
              Langchain::Utils::HashTransformer.symbolize_keys(tool_arguments)
            else
              JSON.parse(tool_arguments, symbolize_names: true)
            end

            [tool_call_id, tool_name, method_name, tool_arguments]
          end

          # Build the tools for the OpenAI LLM
          def build_tools(tools)
            tools.map { |tool| tool.class.function_schemas.to_openai_format }.flatten
          end

          # Get the allowed assistant.tool_choice values for OpenAI
          def allowed_tool_choices
            ["auto", "none"]
          end

          # Get the available tool names for OpenAI
          def available_tool_names(tools)
            build_tools(tools).map { |tool| tool.dig(:function, :name) }
          end

          def tool_role
            Messages::OpenAIMessage::TOOL_ROLE
          end

          def support_system_message?
            Messages::OpenAIMessage::ROLES.include?("system")
          end

          private

          def build_tool_choice(choice)
            case choice
            when "auto"
              choice
            else
              {"type" => "function", "function" => {"name" => choice}}
            end
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant/llm/adapter.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Assistant
    module LLM
      # TODO: Fix the message truncation when context window is exceeded
      class Adapter
        def self.build(llm)
          if llm.is_a?(Langchain::LLM::Anthropic)
            LLM::Adapters::Anthropic.new
          elsif llm.is_a?(Langchain::LLM::AwsBedrock) && llm.defaults[:chat_model].include?("anthropic")
            LLM::Adapters::AwsBedrockAnthropic.new
          elsif llm.is_a?(Langchain::LLM::GoogleGemini) || llm.is_a?(Langchain::LLM::GoogleVertexAI)
            LLM::Adapters::GoogleGemini.new
          elsif llm.is_a?(Langchain::LLM::MistralAI)
            LLM::Adapters::MistralAI.new
          elsif llm.is_a?(Langchain::LLM::Ollama)
            LLM::Adapters::Ollama.new
          elsif llm.is_a?(Langchain::LLM::OpenAI)
            LLM::Adapters::OpenAI.new
          else
            raise ArgumentError, "Unsupported LLM type: #{llm.class}"
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/dependency_helper.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module DependencyHelper
    class LoadError < ::LoadError; end

    class VersionError < ScriptError; end

    # This method requires and loads the given gem, and then checks to see if the version of the gem meets the requirements listed in `langchain.gemspec`
    # This solution was built to avoid auto-loading every single gem in the Gemfile when the developer will mostly likely be only using a few of them.
    #
    # @param gem_name [String] The name of the gem to load
    # @return [Boolean] Whether or not the gem was loaded successfully
    # @raise [LoadError] If the gem is not installed
    # @raise [VersionError] If the gem is installed, but the version does not meet the requirements
    #
    def depends_on(gem_name, req: true)
      gem(gem_name) # require the gem

      return(true) unless defined?(Bundler) # If we're in a non-bundler environment, we're no longer able to determine if we'll meet requirements

      gem_version = Gem.loaded_specs[gem_name].version
      gem_requirement = Bundler.load.dependencies.find { |g| g.name == gem_name }&.requirement

      raise LoadError unless gem_requirement

      unless gem_requirement.satisfied_by?(gem_version)
        raise VersionError, "The #{gem_name} gem is installed, but version #{gem_requirement} is required. You have #{gem_version}."
      end

      lib_name = gem_name if req == true
      lib_name = req if req.is_a?(String)

      require(lib_name) if lib_name

      true
    rescue ::LoadError
      raise LoadError, "Could not load #{gem_name}. Please ensure that the #{gem_name} gem is installed."
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/data.rb`:

```rb
# frozen_string_literal: true

module Langchain
  # Abstraction for data loaded by a {Langchain::Loader}
  class Data
    # URL or Path of the data source
    # @return [String]
    attr_reader :source

    # @param data [String] data that was loaded
    # @option options [String] :source URL or Path of the data source
    def initialize(data, source: nil, chunker: Langchain::Chunker::Text)
      @source = source
      @data = data
      @chunker_klass = chunker
    end

    # @return [String]
    def value
      @data
    end

    # @param opts [Hash] options passed to the chunker
    # @return [Array<String>]
    def chunks(opts = {})
      @chunker_klass.new(@data, **opts).chunks
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/answer_relevance.rb`:

```rb
# freeze_string_literal: true

require "matrix"

module Langchain
  module Evals
    module Ragas
      # Answer Relevance refers to the idea that the generated answer should address the actual question that was provided.
      # This metric evaluates how closely the generated answer aligns with the initial question or instruction.
      class AnswerRelevance
        attr_reader :llm, :batch_size

        # @param llm [Langchain::LLM::*] Langchain::LLM::* object
        # @param batch_size [Integer] Batch size, i.e., number of generated questions to compare to the original question
        def initialize(llm:, batch_size: 3)
          @llm = llm
          @batch_size = batch_size
        end

        # @param question [String] Question
        # @param answer [String] Answer
        # @return [Float] Answer Relevance score
        def score(question:, answer:)
          generated_questions = []

          batch_size.times do |i|
            prompt = answer_relevance_prompt_template.format(
              question: question,
              answer: answer
            )
            generated_questions << llm.complete(prompt: prompt).completion
          end

          scores = generated_questions.map do |generated_question|
            calculate_similarity(original_question: question, generated_question: generated_question)
          end

          # Find the mean
          scores.sum(0.0) / scores.size
        end

        private

        # @param question_1 [String] Question 1
        # @param question_2 [String] Question 2
        # @return [Float] Dot product similarity between the two questions
        def calculate_similarity(original_question:, generated_question:)
          original_embedding = generate_embedding(original_question)
          generated_embedding = generate_embedding(generated_question)

          vector_1 = Vector.elements(original_embedding)
          vector_2 = Vector.elements(generated_embedding)
          vector_1.inner_product(vector_2)
        end

        # @param text [String] Text to generate an embedding for
        # @return [Array<Float>] Embedding
        def generate_embedding(text)
          llm.embed(text: text).embedding
        end

        # @return [PromptTemplate] PromptTemplate instance
        def answer_relevance_prompt_template
          @template ||= Langchain::Prompt.load_from_path(
            file_path: Langchain.root.join("langchain/evals/ragas/prompts/answer_relevance.yml")
          )
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/main.rb`:

```rb
# freeze_string_literal: true

module Langchain
  module Evals
    # The RAGAS (Retrieval Augmented Generative Assessment) is a framework for evaluating RAG (Retrieval Augmented Generation) pipelines.
    # Based on the following research: https://arxiv.org/pdf/2309.15217.pdf
    module Ragas
      class Main
        attr_reader :llm

        def initialize(llm:)
          @llm = llm
        end

        # Returns the RAGAS scores, e.g.:
        # {
        #   ragas_score: 0.6601257446503674,
        #   answer_relevance_score: 0.9573145866787608,
        #   context_relevance_score: 0.6666666666666666,
        #   faithfulness_score: 0.5
        # }
        #
        # @param question [String] Question
        # @param answer [String] Answer
        # @param context [String] Context
        # @return [Hash] RAGAS scores
        def score(question:, answer:, context:)
          answer_relevance_score = answer_relevance.score(question: question, answer: answer)
          context_relevance_score = context_relevance.score(question: question, context: context)
          faithfulness_score = faithfulness.score(question: question, answer: answer, context: context)

          {
            ragas_score: ragas_score(answer_relevance_score, context_relevance_score, faithfulness_score),
            answer_relevance_score: answer_relevance_score,
            context_relevance_score: context_relevance_score,
            faithfulness_score: faithfulness_score
          }
        end

        private

        # Overall RAGAS score (harmonic mean): https://github.com/explodinggradients/ragas/blob/1dd363e3e54744e67b0be85962a0258d8121500a/src/ragas/evaluation.py#L140-L143
        #
        # @param answer_relevance_score [Float] Answer Relevance score
        # @param context_relevance_score [Float] Context Relevance score
        # @param faithfulness_score [Float] Faithfulness score
        # @return [Float] RAGAS score
        def ragas_score(answer_relevance_score, context_relevance_score, faithfulness_score)
          reciprocal_sum = (1.0 / answer_relevance_score) + (1.0 / context_relevance_score) + (1.0 / faithfulness_score)
          (3 / reciprocal_sum)
        end

        # @return [Langchain::Evals::Ragas::AnswerRelevance] Class instance
        def answer_relevance
          @answer_relevance ||= Langchain::Evals::Ragas::AnswerRelevance.new(llm: llm)
        end

        # @return [Langchain::Evals::Ragas::ContextRelevance] Class instance
        def context_relevance
          @context_relevance ||= Langchain::Evals::Ragas::ContextRelevance.new(llm: llm)
        end

        # @return [Langchain::Evals::Ragas::Faithfulness] Class instance
        def faithfulness
          @faithfulness ||= Langchain::Evals::Ragas::Faithfulness.new(llm: llm)
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/context_relevance.rb`:

```rb
# freeze_string_literal: true

require "pragmatic_segmenter"

module Langchain
  module Evals
    module Ragas
      # Context Relevance refers to the idea that the retrieved context should be focused, containing as little irrelevant information as possible.
      class ContextRelevance
        attr_reader :llm

        # @param llm [Langchain::LLM::*] Langchain::LLM::* object
        def initialize(llm:)
          @llm = llm
        end

        # @param question [String] Question
        # @param context [String] Context
        # @return [Float] Context Relevance score
        def score(question:, context:)
          prompt = context_relevance_prompt_template.format(
            question: question,
            context: context
          )
          sentences = llm.complete(prompt: prompt).completion

          (sentence_count(sentences).to_f / sentence_count(context).to_f)
        end

        private

        def sentence_count(context)
          ps = PragmaticSegmenter::Segmenter.new(text: context)
          ps.segment.length
        end

        # @return [PromptTemplate] PromptTemplate instance
        def context_relevance_prompt_template
          @template ||= Langchain::Prompt.load_from_path(
            file_path: Langchain.root.join("langchain/evals/ragas/prompts/context_relevance.yml")
          )
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/prompts/answer_relevance.yml`:

```yml
_type: prompt
input_variables:
  - answer
template: |
  Generate question for the given answer.
  Answer: The PSLV-C56 mission is scheduled to be launched on Sunday, 30 July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space Centre, Sriharikota, Andhra Pradesh, India 
  Question: When is the scheduled launch date and time for the PSLV-C56 mission, and where will it be launched from?

  Answer: {answer}
  Question:

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/prompts/context_relevance.yml`:

```yml
_type: prompt
input_variables:
  - question
  - context
template: |
  Please extract relevant sentences from the provided context that is absolutely required answer the following question. If no relevant sentences are found, or if you believe the question cannot be answered from the given context, return the phrase "Insufficient Information".  While extracting candidate sentences you're not allowed to make any changes to sentences from given context.

  question:{question}
  context:\n{context}
  candidate sentences:\n

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/prompts/faithfulness_statements_verification.yml`:

```yml
_type: prompt
input_variables:
  - statements
  - context
template: |
  Consider the given context and following statements, then determine whether they are supported by the information present in the context.
  Provide a brief explanation for each statement before arriving at the verdict (Yes/No). Provide a final verdict for each statement in order at the end in the given format.
  Do not deviate from the specified format.

  Context:\nJohn is a student at XYZ University. He is pursuing a degree in Computer Science. He is enrolled in several courses this semester, including Data Structures, Algorithms, and Database Management. John is a diligent student and spends a significant amount of time studying and completing assignments. He often stays late in the library to work on his projects.
  statements:\n1. John is majoring in Biology.\n2. John is taking a course on Artificial Intelligence.\n3. John is a dedicated student.\n4. John has a part-time job.\n5. John is interested in computer programming.\n
  Answer:
  1. John is majoring in Biology.
  Explanation: John's major is explicitly mentioned as Computer Science. There is no information suggesting he is majoring in Biology.  Verdict: No.
  2. John is taking a course on Artificial Intelligence.
  Explanation: The context mentions the courses John is currently enrolled in, and Artificial Intelligence is not mentioned. Therefore, it cannot be deduced that John is taking a course on AI. Verdict: No.
  3. John is a dedicated student.
  Explanation: The prompt states that he spends a significant amount of time studying and completing assignments. Additionally, it mentions that he often stays late in the library to work on his projects, which implies dedication. Verdict: Yes.
  4. John has a part-time job.
  Explanation: There is no information given in the context about John having a part-time job. Therefore, it cannot be deduced that John has a part-time job.  Verdict: No.
  5. John is interested in computer programming.
  Explanation: The context states that John is pursuing a degree in Computer Science, which implies an interest in computer programming. Verdict: Yes.
  Final verdict for each statement in order: No. No. Yes. No. Yes.

  context:\n{context}
  statements:\n{statements}
  Answer:

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/prompts/faithfulness_statements_extraction.yml`:

```yml
_type: prompt
input_variables:
  - question
  - answer
template: |
  Given a question and answer, create one or more statements from each sentence in the given answer.
  question: {question}
  answer: {answer}
  statements:\n

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/evals/ragas/faithfulness.rb`:

```rb
# freeze_string_literal: true

module Langchain
  module Evals
    module Ragas
      # Faithfulness refers to the idea that the answer should be grounded in the given context,
      # ensuring that the retrieved context can act as a justification for the generated answer.
      # The answer is faithful to the context if the claims that are made in the answer can be inferred from the context.
      #
      # Score calculation:
      # F = |V| / |S|
      #
      # F = Faithfulness
      # |V| = Number of statements that were supported according to the LLM
      # |S| = Total number of statements extracted.
      #
      class Faithfulness
        attr_reader :llm

        # @param llm [Langchain::LLM::*] Langchain::LLM::* object
        def initialize(llm:)
          @llm = llm
        end

        # @param question [String] Question
        # @param answer [String] Answer
        # @param context [String] Context
        # @return [Float] Faithfulness score
        def score(question:, answer:, context:)
          statements = statements_extraction(question: question, answer: answer)
          statements_count = statements
            .split("\n")
            .count

          verifications = statements_verification(statements: statements, context: context)
          verifications_count = count_verified_statements(verifications)

          (verifications_count.to_f / statements_count.to_f)
        end

        private

        def count_verified_statements(verifications)
          match = verifications.match(/Final verdict for each statement in order:\s*(.*)/)
          return 0.0 unless match # no verified statements found

          verdicts = match.captures.first
          verdicts
            .split(".")
            .count { |value| to_boolean(value.strip) }
        end

        def statements_verification(statements:, context:)
          prompt = statements_verification_prompt_template.format(
            statements: statements,
            context: context
          )
          llm.complete(prompt: prompt).completion
        end

        def statements_extraction(question:, answer:)
          prompt = statements_extraction_prompt_template.format(
            question: question,
            answer: answer
          )
          llm.complete(prompt: prompt).completion
        end

        # @return [PromptTemplate] PromptTemplate instance
        def statements_verification_prompt_template
          @template_two ||= Langchain::Prompt.load_from_path(
            file_path: Langchain.root.join("langchain/evals/ragas/prompts/faithfulness_statements_verification.yml")
          )
        end

        # @return [PromptTemplate] PromptTemplate instance
        def statements_extraction_prompt_template
          @template_one ||= Langchain::Prompt.load_from_path(
            file_path: Langchain.root.join("langchain/evals/ragas/prompts/faithfulness_statements_extraction.yml")
          )
        end

        def to_boolean(value)
          Langchain::Utils::ToBoolean.new.to_bool(value)
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/base.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class ApiError < StandardError; end

  # A LLM is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabeled text using self-supervised learning or semi-supervised learning.
  #
  # Langchain.rb provides a common interface to interact with all supported LLMs:
  #
  # - {Langchain::LLM::AI21}
  # - {Langchain::LLM::Anthropic}
  # - {Langchain::LLM::Azure}
  # - {Langchain::LLM::Cohere}
  # - {Langchain::LLM::GoogleGemini}
  # - {Langchain::LLM::GoogleVertexAI}
  # - {Langchain::LLM::HuggingFace}
  # - {Langchain::LLM::LlamaCpp}
  # - {Langchain::LLM::OpenAI}
  # - {Langchain::LLM::Replicate}
  #
  # @abstract
  class Base
    include Langchain::DependencyHelper

    # A client for communicating with the LLM
    attr_accessor :client

    # Default LLM options. Can be overridden by passing `default_options: {}` to the Langchain::LLM::* constructors.
    attr_reader :defaults

    # Ensuring backward compatibility after https://github.com/patterns-ai-core/langchainrb/pull/586
    # TODO: Delete this method later
    def default_dimension
      default_dimensions
    end

    # Returns the number of vector dimensions used by DEFAULTS[:chat_model]
    #
    # @return [Integer] Vector dimensions
    def default_dimensions
      self.class.const_get(:DEFAULTS).dig(:dimensions)
    end

    #
    # Generate a chat completion for a given prompt. Parameters will depend on the LLM
    #
    # @raise NotImplementedError if not supported by the LLM
    def chat(...)
      raise NotImplementedError, "#{self.class.name} does not support chat"
    end

    #
    # Generate a completion for a given prompt. Parameters will depend on the LLM.
    #
    # @raise NotImplementedError if not supported by the LLM
    def complete(...)
      raise NotImplementedError, "#{self.class.name} does not support completion"
    end

    #
    # Generate an embedding for a given text. Parameters depends on the LLM.
    #
    # @raise NotImplementedError if not supported by the LLM
    #
    def embed(...)
      raise NotImplementedError, "#{self.class.name} does not support generating embeddings"
    end

    #
    # Generate a summary for a given text. Parameters depends on the LLM.
    #
    # @raise NotImplementedError if not supported by the LLM
    #
    def summarize(...)
      raise NotImplementedError, "#{self.class.name} does not support summarization"
    end

    #
    # Returns an instance of Langchain::LLM::Parameters::Chat
    #
    def chat_parameters(params = {})
      @chat_parameters ||= Langchain::LLM::Parameters::Chat.new(
        parameters: params
      )
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/ollama.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # Interface to Ollama API.
  # Available models: https://ollama.ai/library
  #
  # Usage:
  #    llm = Langchain::LLM::Ollama.new(url: ENV["OLLAMA_URL"], default_options: {})
  #
  class Ollama < Base
    attr_reader :url, :defaults

    DEFAULTS = {
      temperature: 0.0,
      completion_model: "llama3.2",
      embedding_model: "llama3.2",
      chat_model: "llama3.2",
      options: {}
    }.freeze

    EMBEDDING_SIZES = {
      codellama: 4_096,
      "dolphin-mixtral": 4_096,
      llama2: 4_096,
      llama3: 4_096,
      "llama3.1": 4_096,
      "llama3.2": 4_096,
      llava: 4_096,
      mistral: 4_096,
      "mistral-openorca": 4_096,
      mixtral: 4_096,
      tinydolphin: 2_048
    }.freeze

    # Initialize the Ollama client
    # @param url [String] The URL of the Ollama instance
    # @param api_key [String] The API key to use. This is optional and used when you expose Ollama API using Open WebUI
    # @param default_options [Hash] The default options to use
    #
    def initialize(url: "http://localhost:11434", api_key: nil, default_options: {})
      depends_on "faraday"
      @url = url
      @api_key = api_key
      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {default: @defaults[:temperature]},
        template: {},
        stream: {default: false},
        response_format: {default: @defaults[:response_format]},
        options: {default: @defaults[:options]}
      )
      chat_parameters.remap(response_format: :format)
    end

    # Returns the # of vector dimensions for the embeddings
    # @return [Integer] The # of vector dimensions
    def default_dimensions
      # since Ollama can run multiple models, look it up or generate an embedding and return the size
      @default_dimensions ||=
        EMBEDDING_SIZES.fetch(defaults[:embedding_model].to_sym) do
          embed(text: "test").embedding.size
        end
    end

    #
    # Generate the completion for a given prompt
    #
    # @param prompt [String] The prompt to complete
    # @param model [String] The model to use
    #   For a list of valid parameters and values, see:
    #   https://github.com/jmorganca/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values
    # @option block [Proc] Receive the intermediate responses as a stream of +OllamaResponse+ objects.
    # @return [Langchain::LLM::OllamaResponse] Response object
    #
    # Example:
    #
    #  final_resp = ollama.complete(prompt:) { |resp| print resp.completion }
    #  final_resp.total_tokens
    #
    def complete(
      prompt:,
      model: defaults[:completion_model],
      images: nil,
      format: nil,
      system: nil,
      template: nil,
      context: nil,
      raw: nil,
      mirostat: nil,
      mirostat_eta: nil,
      mirostat_tau: nil,
      num_ctx: nil,
      num_gqa: nil,
      num_gpu: nil,
      num_thread: nil,
      repeat_last_n: nil,
      repeat_penalty: nil,
      temperature: defaults[:temperature],
      seed: nil,
      stop: nil,
      tfs_z: nil,
      num_predict: nil,
      top_k: nil,
      top_p: nil,
      stop_sequences: nil,
      &block
    )
      if stop_sequences
        stop = stop_sequences
      end

      parameters = {
        prompt: prompt,
        model: model,
        images: images,
        format: format,
        system: system,
        template: template,
        context: context,
        stream: block_given?, # rubocop:disable Performance/BlockGivenWithExplicitBlock
        raw: raw
      }.compact

      llm_parameters = {
        mirostat: mirostat,
        mirostat_eta: mirostat_eta,
        mirostat_tau: mirostat_tau,
        num_ctx: num_ctx,
        num_gqa: num_gqa,
        num_gpu: num_gpu,
        num_thread: num_thread,
        repeat_last_n: repeat_last_n,
        repeat_penalty: repeat_penalty,
        temperature: temperature,
        seed: seed,
        stop: stop,
        tfs_z: tfs_z,
        num_predict: num_predict,
        top_k: top_k,
        top_p: top_p
      }

      parameters[:options] = llm_parameters.compact
      responses_stream = []

      client.post("api/generate", parameters) do |req|
        req.options.on_data = json_responses_chunk_handler do |parsed_chunk|
          responses_stream << parsed_chunk

          block&.call(OllamaResponse.new(parsed_chunk, model: parameters[:model]))
        end
      end

      generate_final_completion_response(responses_stream, parameters[:model])
    end

    # Generate a chat completion
    #
    # @param messages [Array] The chat messages
    # @param model [String] The model to use
    # @param params [Hash] Unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
    # @option params [Array<Hash>] :messages Array of messages
    # @option params [String] :model Model name
    # @option params [String] :format Format to return a response in. Currently the only accepted value is `json`
    # @option params [Float] :temperature The temperature to use
    # @option params [String] :template The prompt template to use (overrides what is defined in the `Modelfile`)
    # @option block [Proc] Receive the intermediate responses as a stream of +OllamaResponse+ objects.
    # @return [Langchain::LLM::OllamaResponse] Response object
    #
    # Example:
    #
    #  final_resp = ollama.chat(messages:) { |resp| print resp.chat_completion }
    #  final_resp.total_tokens
    #
    # The message object has the following fields:
    #   role: the role of the message, either system, user or assistant
    #   content: the content of the message
    #   images (optional): a list of images to include in the message (for multimodal models such as llava)
    def chat(messages:, model: nil, **params, &block)
      parameters = chat_parameters.to_params(params.merge(messages:, model:, stream: block_given?)) # rubocop:disable Performance/BlockGivenWithExplicitBlock
      responses_stream = []

      client.post("api/chat", parameters) do |req|
        req.options.on_data = json_responses_chunk_handler do |parsed_chunk|
          responses_stream << parsed_chunk

          block&.call(OllamaResponse.new(parsed_chunk, model: parameters[:model]))
        end
      end

      generate_final_chat_completion_response(responses_stream, parameters[:model])
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @param model [String] The model to use
    # @param options [Hash] The options to use
    # @return [Langchain::LLM::OllamaResponse] Response object
    #
    def embed(
      text:,
      model: defaults[:embedding_model],
      mirostat: nil,
      mirostat_eta: nil,
      mirostat_tau: nil,
      num_ctx: nil,
      num_gqa: nil,
      num_gpu: nil,
      num_thread: nil,
      repeat_last_n: nil,
      repeat_penalty: nil,
      temperature: defaults[:temperature],
      seed: nil,
      stop: nil,
      tfs_z: nil,
      num_predict: nil,
      top_k: nil,
      top_p: nil
    )
      parameters = {
        model: model,
        input: Array(text)
      }.compact

      llm_parameters = {
        mirostat: mirostat,
        mirostat_eta: mirostat_eta,
        mirostat_tau: mirostat_tau,
        num_ctx: num_ctx,
        num_gqa: num_gqa,
        num_gpu: num_gpu,
        num_thread: num_thread,
        repeat_last_n: repeat_last_n,
        repeat_penalty: repeat_penalty,
        temperature: temperature,
        seed: seed,
        stop: stop,
        tfs_z: tfs_z,
        num_predict: num_predict,
        top_k: top_k,
        top_p: top_p
      }

      parameters[:options] = llm_parameters.compact

      response = client.post("api/embed") do |req|
        req.body = parameters
      end

      OllamaResponse.new(response.body, model: parameters[:model])
    end

    # Generate a summary for a given text
    #
    # @param text [String] The text to generate a summary for
    # @return [String] The summary
    def summarize(text:)
      prompt_template = Langchain::Prompt.load_from_path(
        file_path: Langchain.root.join("langchain/llm/prompts/ollama/summarize_template.yaml")
      )
      prompt = prompt_template.format(text: text)

      complete(prompt: prompt)
    end

    private

    def client
      @client ||= Faraday.new(url: url, headers: auth_headers) do |conn|
        conn.request :json
        conn.response :json
        conn.response :raise_error
        conn.response :logger, Langchain.logger, {headers: true, bodies: true, errors: true}
      end
    end

    def auth_headers
      return unless @api_key

      {"Authorization" => "Bearer #{@api_key}"}
    end

    def json_responses_chunk_handler(&block)
      proc do |chunk, _size|
        chunk.split("\n").each do |chunk_line|
          parsed_chunk = JSON.parse(chunk_line)
          block.call(parsed_chunk)
        end
      end
    end

    def generate_final_completion_response(responses_stream, model)
      final_response = responses_stream.last.merge(
        "response" => responses_stream.map { |resp| resp["response"] }.join
      )

      OllamaResponse.new(final_response, model: model)
    end

    # BUG: If streamed, this method does not currently return the tool_calls response.
    def generate_final_chat_completion_response(responses_stream, model)
      final_response = responses_stream.last
      final_response["message"]["content"] = responses_stream.map { |resp| resp.dig("message", "content") }.join

      OllamaResponse.new(final_response, model: model)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/aws_titan_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class AwsTitanResponse < BaseResponse
    def embedding
      embeddings&.first
    end

    def embeddings
      [raw_response.dig("embedding")]
    end

    def prompt_tokens
      raw_response.dig("inputTextTokenCount")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/anthropic_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class AnthropicResponse < BaseResponse
    def model
      raw_response.dig("model")
    end

    def completion
      completions.first
    end

    def chat_completion
      chat_completion = chat_completions.find { |h| h["type"] == "text" }
      chat_completion&.dig("text")
    end

    def tool_calls
      tool_call = chat_completions.find { |h| h["type"] == "tool_use" }
      tool_call ? [tool_call] : []
    end

    def chat_completions
      raw_response.dig("content")
    end

    def completions
      [raw_response.dig("completion")]
    end

    def stop_reason
      raw_response.dig("stop_reason")
    end

    def stop
      raw_response.dig("stop")
    end

    def log_id
      raw_response.dig("log_id")
    end

    def prompt_tokens
      raw_response.dig("usage", "input_tokens").to_i
    end

    def completion_tokens
      raw_response.dig("usage", "output_tokens").to_i
    end

    def total_tokens
      prompt_tokens + completion_tokens
    end

    def role
      raw_response.dig("role")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/llama_cpp_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class LlamaCppResponse < BaseResponse
    def embedding
      embeddings
    end

    def embeddings
      raw_response.embeddings
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/hugging_face_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class HuggingFaceResponse < BaseResponse
    def embeddings
      [raw_response]
    end

    def embedding
      embeddings.first
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/ollama_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class OllamaResponse < BaseResponse
    def initialize(raw_response, model: nil, prompt_tokens: nil)
      @prompt_tokens = prompt_tokens
      super(raw_response, model: model)
    end

    def created_at
      Time.parse(raw_response.dig("created_at")) if raw_response.dig("created_at")
    end

    def chat_completion
      raw_response.dig("message", "content")
    end

    def completion
      raw_response.dig("response")
    end

    def completions
      [completion].compact
    end

    def embedding
      embeddings.first
    end

    def embeddings
      raw_response&.dig("embeddings") || []
    end

    def role
      "assistant"
    end

    def prompt_tokens
      raw_response.fetch("prompt_eval_count", 0) if done?
    end

    def completion_tokens
      raw_response.dig("eval_count") if done?
    end

    def total_tokens
      prompt_tokens + completion_tokens if done?
    end

    def tool_calls
      Array(raw_response.dig("message", "tool_calls"))
    end

    private

    def done?
      !!raw_response["done"]
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/openai_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class OpenAIResponse < BaseResponse
    def model
      raw_response["model"]
    end

    def created_at
      if raw_response.dig("created")
        Time.at(raw_response.dig("created"))
      end
    end

    def completion
      completions&.dig(0, "message", "content")
    end

    def role
      completions&.dig(0, "message", "role")
    end

    def chat_completion
      completion
    end

    def tool_calls
      if chat_completions.dig(0, "message").has_key?("tool_calls")
        chat_completions.dig(0, "message", "tool_calls")
      else
        []
      end
    end

    def embedding
      embeddings&.first
    end

    def completions
      raw_response.dig("choices")
    end

    def chat_completions
      raw_response.dig("choices")
    end

    def embeddings
      raw_response.dig("data")&.map { |datum| datum.dig("embedding") }
    end

    def prompt_tokens
      raw_response.dig("usage", "prompt_tokens")
    end

    def completion_tokens
      raw_response.dig("usage", "completion_tokens")
    end

    def total_tokens
      raw_response.dig("usage", "total_tokens")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/google_gemini_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class GoogleGeminiResponse < BaseResponse
    def initialize(raw_response, model: nil)
      super
    end

    def chat_completion
      raw_response.dig("candidates", 0, "content", "parts", 0, "text")
    end

    def role
      raw_response.dig("candidates", 0, "content", "role")
    end

    def tool_calls
      if raw_response.dig("candidates", 0, "content") && raw_response.dig("candidates", 0, "content", "parts", 0).has_key?("functionCall")
        raw_response.dig("candidates", 0, "content", "parts")
      else
        []
      end
    end

    def embedding
      embeddings.first
    end

    def embeddings
      if raw_response.key?("embedding")
        [raw_response.dig("embedding", "values")]
      else
        [raw_response.dig("predictions", 0, "embeddings", "values")]
      end
    end

    def prompt_tokens
      raw_response.dig("usageMetadata", "promptTokenCount")
    end

    def completion_tokens
      raw_response.dig("usageMetadata", "candidatesTokenCount")
    end

    def total_tokens
      raw_response.dig("usageMetadata", "totalTokenCount")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/aws_bedrock_meta_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class AwsBedrockMetaResponse < BaseResponse
    def completion
      completions.first
    end

    def completions
      [raw_response.dig("generation")]
    end

    def stop_reason
      raw_response.dig("stop_reason")
    end

    def prompt_tokens
      raw_response.dig("prompt_token_count").to_i
    end

    def completion_tokens
      raw_response.dig("generation_token_count").to_i
    end

    def total_tokens
      prompt_tokens + completion_tokens
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/base_response.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module LLM
    class BaseResponse
      attr_reader :raw_response, :model

      # Save context in the response when doing RAG workflow vectorsearch#ask()
      attr_accessor :context

      def initialize(raw_response, model: nil)
        @raw_response = raw_response
        @model = model
      end

      # Returns the timestamp when the response was created
      #
      # @return [Time]
      def created_at
        raise NotImplementedError
      end

      # Returns the completion text
      #
      # @return [String]
      #
      def completion
        raise NotImplementedError
      end

      # Returns the chat completion text
      #
      # @return [String]
      #
      def chat_completion
        raise NotImplementedError
      end

      # Return the first embedding
      #
      # @return [Array<Float>]
      def embedding
        raise NotImplementedError
      end

      # Return the completion candidates
      #
      # @return [Array<String>]
      def completions
        raise NotImplementedError
      end

      # Return the chat completion candidates
      #
      # @return [Array<String>]
      def chat_completions
        raise NotImplementedError
      end

      # Return the embeddings
      #
      # @return [Array<Array>]
      def embeddings
        raise NotImplementedError
      end

      # Number of tokens utilized in the prompt
      #
      # @return [Integer]
      def prompt_tokens
        raise NotImplementedError
      end

      # Number of tokens utilized to generate the completion
      #
      # @return [Integer]
      def completion_tokens
        raise NotImplementedError
      end

      # Total number of tokens utilized
      #
      # @return [Integer]
      def total_tokens
        raise NotImplementedError
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/ai21_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class AI21Response < BaseResponse
    def completions
      raw_response.dig(:completions)
    end

    def completion
      completions.dig(0, :data, :text)
    end

    def chat_completion
      raw_response.dig(:choices, 0, :message, :content)
    end

    def prompt_tokens
      raw_response.dig(:usage, :prompt_tokens).to_i
    end

    def completion_tokens
      raw_response.dig(:usage, :completion_tokens).to_i
    end

    def total_tokens
      raw_response.dig(:usage, :total_tokens).to_i
    end

    def role
      raw_response.dig(:choices, 0, :message, :role)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/cohere_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class CohereResponse < BaseResponse
    def embedding
      embeddings.first
    end

    def embeddings
      raw_response.dig("embeddings")
    end

    def completions
      raw_response.dig("generations")
    end

    def completion
      completions&.dig(0, "text")
    end

    def chat_completion
      raw_response.dig("text")
    end

    def role
      raw_response.dig("chat_history").last["role"]
    end

    def prompt_tokens
      raw_response.dig("meta", "billed_units", "input_tokens")
    end

    def completion_tokens
      raw_response.dig("meta", "billed_units", "output_tokens")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/replicate_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class ReplicateResponse < BaseResponse
    def completions
      # Response comes back as an array of strings, e.g.: ["Hi", "how ", "are ", "you?"]
      # The first array element is missing a space at the end, so we add it manually
      raw_response.output[0] += " "
      [raw_response.output.join]
    end

    def completion
      completions.first
    end

    def created_at
      Time.parse(raw_response.created_at)
    end

    def embedding
      embeddings.first
    end

    def embeddings
      [raw_response.output]
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/response/mistral_ai_response.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class MistralAIResponse < BaseResponse
    def model
      raw_response["model"]
    end

    def chat_completion
      chat_completions.dig(0, "message", "content")
    end

    def chat_completions
      raw_response.dig("choices")
    end

    def tool_calls
      chat_completions.dig(0, "message", "tool_calls") || []
    end

    def role
      raw_response.dig("choices", 0, "message", "role")
    end

    def embedding
      raw_response.dig("data", 0, "embedding")
    end

    def prompt_tokens
      raw_response.dig("usage", "prompt_tokens")
    end

    def total_tokens
      raw_response.dig("usage", "total_tokens")
    end

    def completion_tokens
      raw_response.dig("usage", "completion_tokens")
    end

    def created_at
      if raw_response.dig("created_at")
        Time.at(raw_response.dig("created_at"))
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/ai21.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around AI21 Studio APIs.
  #
  # Gem requirements:
  #   gem "ai21", "~> 0.2.1"
  #
  # Usage:
  #     llm = Langchain::LLM::AI21.new(api_key: ENV["AI21_API_KEY"])
  #
  class AI21 < Base
    DEFAULTS = {
      temperature: 0.0,
      model: "j2-ultra"
    }.freeze

    def initialize(api_key:, default_options: {})
      depends_on "ai21"

      @client = ::AI21::Client.new(api_key)
      @defaults = DEFAULTS.merge(default_options)
    end

    #
    # Generate a completion for a given prompt
    #
    # @param prompt [String] The prompt to generate a completion for
    # @param params [Hash] The parameters to pass to the API
    # @return [Langchain::LLM::AI21Response] The completion
    #
    def complete(prompt:, **params)
      parameters = complete_parameters params

      response = client.complete(prompt, parameters)
      Langchain::LLM::AI21Response.new response, model: parameters[:model]
    end

    #
    # Generate a summary for a given text
    #
    # @param text [String] The text to generate a summary for
    # @param params [Hash] The parameters to pass to the API
    # @return [String] The summary
    #
    def summarize(text:, **params)
      response = client.summarize(text, "TEXT", params)
      response.dig(:summary)
      # Should we update this to also return a Langchain::LLM::AI21Response?
    end

    private

    def complete_parameters(params)
      @defaults.dup.merge(params)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/cohere.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around the Cohere API.
  #
  # Gem requirements:
  #     gem "cohere-ruby", "~> 0.9.6"
  #
  # Usage:
  #     llm = Langchain::LLM::Cohere.new(api_key: ENV["COHERE_API_KEY"])
  #
  class Cohere < Base
    DEFAULTS = {
      temperature: 0.0,
      completion_model: "command",
      chat_model: "command-r-plus",
      embedding_model: "small",
      dimensions: 1024,
      truncate: "START"
    }.freeze

    def initialize(api_key:, default_options: {})
      depends_on "cohere-ruby", req: "cohere"

      @client = ::Cohere::Client.new(api_key: api_key)
      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {default: @defaults[:temperature]},
        response_format: {default: @defaults[:response_format]}
      )
      chat_parameters.remap(
        system: :preamble,
        messages: :chat_history,
        stop: :stop_sequences,
        top_k: :k,
        top_p: :p
      )
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @return [Langchain::LLM::CohereResponse] Response object
    #
    def embed(text:)
      response = client.embed(
        texts: [text],
        model: @defaults[:embedding_model]
      )

      Langchain::LLM::CohereResponse.new response, model: @defaults[:embedding_model]
    end

    #
    # Generate a completion for a given prompt
    #
    # @param prompt [String] The prompt to generate a completion for
    # @param params[:stop_sequences]
    # @return [Langchain::LLM::CohereResponse] Response object
    #
    def complete(prompt:, **params)
      default_params = {
        prompt: prompt,
        temperature: @defaults[:temperature],
        model: @defaults[:completion_model],
        truncate: @defaults[:truncate]
      }

      if params[:stop_sequences]
        default_params[:stop_sequences] = params.delete(:stop_sequences)
      end

      default_params.merge!(params)

      response = client.generate(**default_params)
      Langchain::LLM::CohereResponse.new response, model: @defaults[:completion_model]
    end

    # Generate a chat completion for given messages
    #
    # @param [Hash] params unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
    # @option params [Array<String>] :messages Input messages
    # @option params [String] :model The model that will complete your prompt
    # @option params [Integer] :max_tokens Maximum number of tokens to generate before stopping
    # @option params [Array<String>] :stop Custom text sequences that will cause the model to stop generating
    # @option params [Boolean] :stream Whether to incrementally stream the response using server-sent events
    # @option params [String] :system System prompt
    # @option params [Float] :temperature Amount of randomness injected into the response
    # @option params [Array<String>] :tools Definitions of tools that the model may use
    # @option params [Integer] :top_k Only sample from the top K options for each subsequent token
    # @option params [Float] :top_p Use nucleus sampling.
    # @return [Langchain::LLM::CohereResponse] The chat completion
    def chat(params = {})
      raise ArgumentError.new("messages argument is required") if Array(params[:messages]).empty?

      parameters = chat_parameters.to_params(params)

      # Cohere API requires `message:` parameter to be sent separately from `chat_history:`.
      # We extract the last message from the messages param.
      parameters[:message] = parameters[:chat_history].pop&.dig(:message)

      response = client.chat(**parameters)

      Langchain::LLM::CohereResponse.new(response)
    end

    # Generate a summary in English for a given text
    #
    # More parameters available to extend this method with: https://github.com/andreibondarev/cohere-ruby/blob/0.9.4/lib/cohere/client.rb#L107-L115
    #
    # @param text [String] The text to generate a summary for
    # @return [String] The summary
    def summarize(text:)
      response = client.summarize(text: text)
      response.dig("summary")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/google_gemini.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # Usage:
  #     llm = Langchain::LLM::GoogleGemini.new(api_key: ENV['GOOGLE_GEMINI_API_KEY'])
  class GoogleGemini < Base
    DEFAULTS = {
      chat_model: "gemini-1.5-pro-latest",
      embedding_model: "text-embedding-004",
      temperature: 0.0
    }

    attr_reader :defaults, :api_key

    def initialize(api_key:, default_options: {})
      @api_key = api_key
      @defaults = DEFAULTS.merge(default_options)

      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {default: @defaults[:temperature]},
        generation_config: {default: nil},
        safety_settings: {default: @defaults[:safety_settings]}
      )
      chat_parameters.remap(
        messages: :contents,
        system: :system_instruction,
        tool_choice: :tool_config
      )
    end

    # Generate a chat completion for a given prompt
    #
    # @param messages [Array<Hash>] List of messages comprising the conversation so far
    # @param model [String] The model to use
    # @param tools [Array<Hash>] A list of Tools the model may use to generate the next response
    # @param tool_choice [String] Specifies the mode in which function calling should execute. If unspecified, the default value will be set to AUTO. Possible values: AUTO, ANY, NONE
    # @param system [String] Developer set system instruction
    def chat(params = {})
      params[:system] = {parts: [{text: params[:system]}]} if params[:system]
      params[:tools] = {function_declarations: params[:tools]} if params[:tools]

      raise ArgumentError.new("messages argument is required") if Array(params[:messages]).empty?

      parameters = chat_parameters.to_params(params)
      parameters[:generation_config] ||= {}
      parameters[:generation_config][:temperature] ||= parameters[:temperature] if parameters[:temperature]
      parameters.delete(:temperature)
      parameters[:generation_config][:top_p] ||= parameters[:top_p] if parameters[:top_p]
      parameters.delete(:top_p)
      parameters[:generation_config][:top_k] ||= parameters[:top_k] if parameters[:top_k]
      parameters.delete(:top_k)
      parameters[:generation_config][:max_output_tokens] ||= parameters[:max_tokens] if parameters[:max_tokens]
      parameters.delete(:max_tokens)
      parameters[:generation_config][:response_mime_type] ||= parameters[:response_format] if parameters[:response_format]
      parameters.delete(:response_format)
      parameters[:generation_config][:stop_sequences] ||= parameters[:stop] if parameters[:stop]
      parameters.delete(:stop)

      uri = URI("https://generativelanguage.googleapis.com/v1beta/models/#{parameters[:model]}:generateContent?key=#{api_key}")

      parsed_response = http_post(uri, parameters)

      wrapped_response = Langchain::LLM::GoogleGeminiResponse.new(parsed_response, model: parameters[:model])

      if wrapped_response.chat_completion || Array(wrapped_response.tool_calls).any?
        wrapped_response
      else
        raise StandardError.new(parsed_response)
      end
    end

    def embed(
      text:,
      model: @defaults[:embedding_model]
    )
      params = {
        content: {
          parts: [
            {
              text: text
            }
          ]
        }
      }

      uri = URI("https://generativelanguage.googleapis.com/v1beta/models/#{model}:embedContent?key=#{api_key}")

      parsed_response = http_post(uri, params)

      Langchain::LLM::GoogleGeminiResponse.new(parsed_response, model: model)
    end

    private

    def http_post(url, params)
      http = Net::HTTP.new(url.hostname, url.port)
      http.use_ssl = url.scheme == "https"
      http.set_debug_output(Langchain.logger) if Langchain.logger.debug?

      request = Net::HTTP::Post.new(url)
      request.content_type = "application/json"
      request.body = params.to_json

      response = http.request(request)

      JSON.parse(response.body)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/anthropic.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around Anthropic APIs.
  #
  # Gem requirements:
  #   gem "anthropic", "~> 0.3.2"
  #
  # Usage:
  #     llm = Langchain::LLM::Anthropic.new(api_key: ENV["ANTHROPIC_API_KEY"])
  #
  class Anthropic < Base
    DEFAULTS = {
      temperature: 0.0,
      completion_model: "claude-2.1",
      chat_model: "claude-3-5-sonnet-20240620",
      max_tokens: 256
    }.freeze

    # Initialize an Anthropic LLM instance
    #
    # @param api_key [String] The API key to use
    # @param llm_options [Hash] Options to pass to the Anthropic client
    # @param default_options [Hash] Default options to use on every call to LLM, e.g.: { temperature:, completion_model:, chat_model:, max_tokens: }
    # @return [Langchain::LLM::Anthropic] Langchain::LLM::Anthropic instance
    def initialize(api_key:, llm_options: {}, default_options: {})
      depends_on "anthropic"

      @client = ::Anthropic::Client.new(access_token: api_key, **llm_options)
      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {default: @defaults[:temperature]},
        max_tokens: {default: @defaults[:max_tokens]},
        metadata: {},
        system: {}
      )
      chat_parameters.ignore(:n, :user)
      chat_parameters.remap(stop: :stop_sequences)
    end

    # Generate a completion for a given prompt
    #
    # @param prompt [String] Prompt to generate a completion for
    # @param model [String] The model to use
    # @param max_tokens_to_sample [Integer] The maximum number of tokens to sample
    # @param stop_sequences [Array<String>] The stop sequences to use
    # @param temperature [Float] The temperature to use
    # @param top_p [Float] The top p value to use
    # @param top_k [Integer] The top k value to use
    # @param metadata [Hash] The metadata to use
    # @param stream [Boolean] Whether to stream the response
    # @return [Langchain::LLM::AnthropicResponse] The completion
    def complete(
      prompt:,
      model: @defaults[:completion_model],
      max_tokens: @defaults[:max_tokens],
      stop_sequences: nil,
      temperature: @defaults[:temperature],
      top_p: nil,
      top_k: nil,
      metadata: nil,
      stream: nil
    )
      raise ArgumentError.new("model argument is required") if model.empty?
      raise ArgumentError.new("max_tokens argument is required") if max_tokens.nil?

      parameters = {
        model: model,
        prompt: prompt,
        max_tokens_to_sample: max_tokens,
        temperature: temperature
      }
      parameters[:stop_sequences] = stop_sequences if stop_sequences
      parameters[:top_p] = top_p if top_p
      parameters[:top_k] = top_k if top_k
      parameters[:metadata] = metadata if metadata
      parameters[:stream] = stream if stream

      response = with_api_error_handling do
        client.complete(parameters: parameters)
      end

      Langchain::LLM::AnthropicResponse.new(response)
    end

    # Generate a chat completion for given messages
    #
    # @param [Hash] params unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
    # @option params [Array<String>] :messages Input messages
    # @option params [String] :model The model that will complete your prompt
    # @option params [Integer] :max_tokens Maximum number of tokens to generate before stopping
    # @option params [Hash] :metadata Object describing metadata about the request
    # @option params [Array<String>] :stop_sequences Custom text sequences that will cause the model to stop generating
    # @option params [Boolean] :stream Whether to incrementally stream the response using server-sent events
    # @option params [String] :system System prompt
    # @option params [Float] :temperature Amount of randomness injected into the response
    # @option params [Array<String>] :tools Definitions of tools that the model may use
    # @option params [Integer] :top_k Only sample from the top K options for each subsequent token
    # @option params [Float] :top_p Use nucleus sampling.
    # @return [Langchain::LLM::AnthropicResponse] The chat completion
    def chat(params = {}, &block)
      set_extra_headers! if params[:tools]

      parameters = chat_parameters.to_params(params)

      raise ArgumentError.new("messages argument is required") if Array(parameters[:messages]).empty?
      raise ArgumentError.new("model argument is required") if parameters[:model].empty?
      raise ArgumentError.new("max_tokens argument is required") if parameters[:max_tokens].nil?

      if block
        @response_chunks = []
        parameters[:stream] = proc do |chunk|
          @response_chunks << chunk
          yield chunk
        end
      end

      response = client.messages(parameters: parameters)

      response = response_from_chunks if block
      reset_response_chunks

      Langchain::LLM::AnthropicResponse.new(response)
    end

    def with_api_error_handling
      response = yield
      return if response.empty?

      raise Langchain::LLM::ApiError.new "Anthropic API error: #{response.dig("error", "message")}" if response&.dig("error")

      response
    end

    def response_from_chunks
      grouped_chunks = @response_chunks.group_by { |chunk| chunk["index"] }.except(nil)

      usage = @response_chunks.find { |chunk| chunk["type"] == "message_delta" }&.dig("usage")
      stop_reason = @response_chunks.find { |chunk| chunk["type"] == "message_delta" }&.dig("delta", "stop_reason")

      content = grouped_chunks.map do |_index, chunks|
        text = chunks.map { |chunk| chunk.dig("delta", "text") }.join
        if !text.nil? && !text.empty?
          {"type" => "text", "text" => text}
        else
          tool_calls_from_choice_chunks(chunks)
        end
      end.flatten

      @response_chunks.first&.slice("id", "object", "created", "model")
        &.merge!(
          {
            "content" => content,
            "usage" => usage,
            "role" => "assistant",
            "stop_reason" => stop_reason
          }
        )
    end

    def tool_calls_from_choice_chunks(chunks)
      return unless (first_block = chunks.find { |chunk| chunk.dig("content_block", "type") == "tool_use" })

      chunks.group_by { |chunk| chunk["index"] }.map do |index, chunks|
        input = chunks.select { |chunk| chunk.dig("delta", "partial_json") }
          .map! { |chunk| chunk.dig("delta", "partial_json") }.join
        {
          "id" => first_block.dig("content_block", "id"),
          "type" => "tool_use",
          "name" => first_block.dig("content_block", "name"),
          "input" => JSON.parse(input).transform_keys(&:to_sym)
        }
      end.compact
    end

    private

    def reset_response_chunks
      @response_chunks = []
    end

    def set_extra_headers!
      ::Anthropic.configuration.extra_headers = {"anthropic-beta": "tools-2024-05-16"}
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/mistral_ai.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # Gem requirements:
  #    gem "mistral-ai"
  #
  # Usage:
  #    llm = Langchain::LLM::MistralAI.new(api_key: ENV["MISTRAL_AI_API_KEY"])
  class MistralAI < Base
    DEFAULTS = {
      chat_model: "mistral-large-latest",
      embedding_model: "mistral-embed"
    }.freeze

    attr_reader :defaults

    def initialize(api_key:, default_options: {})
      depends_on "mistral-ai"

      @client = Mistral.new(
        credentials: {api_key: api_key},
        options: {server_sent_events: true}
      )

      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        n: {default: @defaults[:n]},
        safe_prompt: {},
        temperature: {default: @defaults[:temperature]},
        response_format: {default: @defaults[:response_format]}
      )
      chat_parameters.remap(seed: :random_seed)
      chat_parameters.ignore(:n, :top_k)
    end

    def chat(params = {})
      parameters = chat_parameters.to_params(params)

      response = client.chat_completions(parameters)

      Langchain::LLM::MistralAIResponse.new(response.to_h)
    end

    def embed(
      text:,
      model: defaults[:embedding_model],
      encoding_format: nil
    )
      params = {
        input: text,
        model: model
      }
      params[:encoding_format] = encoding_format if encoding_format

      response = client.embeddings(params)

      Langchain::LLM::MistralAIResponse.new(response.to_h)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/aws_bedrock.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # LLM interface for Aws Bedrock APIs: https://docs.aws.amazon.com/bedrock/
  #
  # Gem requirements:
  #    gem 'aws-sdk-bedrockruntime', '~> 1.1'
  #
  # Usage:
  #    llm = Langchain::LLM::AwsBedrock.new(default_options: {})
  #
  class AwsBedrock < Base
    DEFAULTS = {
      chat_model: "anthropic.claude-3-5-sonnet-20240620-v1:0",
      completion_model: "anthropic.claude-v2:1",
      embedding_model: "amazon.titan-embed-text-v1",
      max_tokens_to_sample: 300,
      temperature: 1,
      top_k: 250,
      top_p: 0.999,
      stop_sequences: ["\n\nHuman:"],
      return_likelihoods: "NONE"
    }.freeze

    attr_reader :client, :defaults

    SUPPORTED_COMPLETION_PROVIDERS = %i[
      anthropic
      ai21
      cohere
      meta
    ].freeze

    SUPPORTED_CHAT_COMPLETION_PROVIDERS = %i[
      anthropic
      ai21
      mistral
    ].freeze

    SUPPORTED_EMBEDDING_PROVIDERS = %i[
      amazon
      cohere
    ].freeze

    def initialize(aws_client_options: {}, default_options: {})
      depends_on "aws-sdk-bedrockruntime", req: "aws-sdk-bedrockruntime"

      @client = ::Aws::BedrockRuntime::Client.new(**aws_client_options)
      @defaults = DEFAULTS.merge(default_options)

      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {},
        max_tokens: {default: @defaults[:max_tokens_to_sample]},
        metadata: {},
        system: {}
      )
      chat_parameters.ignore(:n, :user)
      chat_parameters.remap(stop: :stop_sequences)
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @param params extra parameters passed to Aws::BedrockRuntime::Client#invoke_model
    # @return [Langchain::LLM::AwsTitanResponse] Response object
    #
    def embed(text:, **params)
      raise "Completion provider #{embedding_provider} is not supported." unless SUPPORTED_EMBEDDING_PROVIDERS.include?(embedding_provider)

      parameters = compose_embedding_parameters params.merge(text:)

      response = client.invoke_model({
        model_id: @defaults[:embedding_model],
        body: parameters.to_json,
        content_type: "application/json",
        accept: "application/json"
      })

      parse_embedding_response response
    end

    #
    # Generate a completion for a given prompt
    #
    # @param prompt [String] The prompt to generate a completion for
    # @param params  extra parameters passed to Aws::BedrockRuntime::Client#invoke_model
    # @return [Langchain::LLM::AnthropicResponse], [Langchain::LLM::CohereResponse] or [Langchain::LLM::AI21Response] Response object
    #
    def complete(
      prompt:,
      model: @defaults[:completion_model],
      **params
    )
      raise "Completion provider #{model} is not supported." unless SUPPORTED_COMPLETION_PROVIDERS.include?(provider_name(model))

      parameters = compose_parameters(params, model)

      parameters[:prompt] = wrap_prompt prompt

      response = client.invoke_model({
        model_id: model,
        body: parameters.to_json,
        content_type: "application/json",
        accept: "application/json"
      })

      parse_response(response, model)
    end

    # Generate a chat completion for a given prompt
    # Currently only configured to work with the Anthropic provider and
    # the claude-3 model family
    #
    # @param [Hash] params unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
    # @option params [Array<String>] :messages The messages to generate a completion for
    # @option params [String] :system The system prompt to provide instructions
    # @option params [String] :model The model to use for completion defaults to @defaults[:chat_model]
    # @option params [Integer] :max_tokens The maximum number of tokens to generate defaults to @defaults[:max_tokens_to_sample]
    # @option params [Array<String>] :stop The stop sequences to use for completion
    # @option params [Array<String>] :stop_sequences The stop sequences to use for completion
    # @option params [Float] :temperature The temperature to use for completion
    # @option params [Float] :top_p Use nucleus sampling.
    # @option params [Integer] :top_k Only sample from the top K options for each subsequent token
    # @yield [Hash] Provides chunks of the response as they are received
    # @return [Langchain::LLM::AnthropicResponse] Response object
    def chat(params = {}, &block)
      parameters = chat_parameters.to_params(params)
      parameters = compose_parameters(parameters, parameters[:model])

      unless SUPPORTED_CHAT_COMPLETION_PROVIDERS.include?(provider_name(parameters[:model]))
        raise "Chat provider #{parameters[:model]} is not supported."
      end

      if block
        response_chunks = []

        client.invoke_model_with_response_stream(
          model_id: parameters[:model],
          body: parameters.except(:model).to_json,
          content_type: "application/json",
          accept: "application/json"
        ) do |stream|
          stream.on_event do |event|
            chunk = JSON.parse(event.bytes)
            response_chunks << chunk

            yield chunk
          end
        end

        response_from_chunks(response_chunks)
      else
        response = client.invoke_model({
          model_id: parameters[:model],
          body: parameters.except(:model).to_json,
          content_type: "application/json",
          accept: "application/json"
        })

        parse_response(response, parameters[:model])
      end
    end

    private

    def parse_model_id(model_id)
      model_id
        .gsub("us.", "") # Meta append "us." to their model ids
        .split(".")
    end

    def provider_name(model_id)
      parse_model_id(model_id).first.to_sym
    end

    def model_name(model_id)
      parse_model_id(model_id).last
    end

    def completion_provider
      @defaults[:completion_model].split(".").first.to_sym
    end

    def embedding_provider
      @defaults[:embedding_model].split(".").first.to_sym
    end

    def wrap_prompt(prompt)
      if completion_provider == :anthropic
        "\n\nHuman: #{prompt}\n\nAssistant:"
      else
        prompt
      end
    end

    def max_tokens_key
      if completion_provider == :anthropic
        :max_tokens_to_sample
      elsif completion_provider == :cohere
        :max_tokens
      elsif completion_provider == :ai21
        :maxTokens
      end
    end

    def compose_parameters(params, model_id)
      if provider_name(model_id) == :anthropic
        compose_parameters_anthropic(params)
      elsif provider_name(model_id) == :cohere
        compose_parameters_cohere(params)
      elsif provider_name(model_id) == :ai21
        params
      elsif provider_name(model_id) == :meta
        params
      elsif provider_name(model_id) == :mistral
        params
      end
    end

    def compose_embedding_parameters(params)
      if embedding_provider == :amazon
        compose_embedding_parameters_amazon params
      elsif embedding_provider == :cohere
        compose_embedding_parameters_cohere params
      end
    end

    def parse_response(response, model_id)
      if provider_name(model_id) == :anthropic
        Langchain::LLM::AnthropicResponse.new(JSON.parse(response.body.string))
      elsif provider_name(model_id) == :cohere
        Langchain::LLM::CohereResponse.new(JSON.parse(response.body.string))
      elsif provider_name(model_id) == :ai21
        Langchain::LLM::AI21Response.new(JSON.parse(response.body.string, symbolize_names: true))
      elsif provider_name(model_id) == :meta
        Langchain::LLM::AwsBedrockMetaResponse.new(JSON.parse(response.body.string))
      elsif provider_name(model_id) == :mistral
        Langchain::LLM::MistralAIResponse.new(JSON.parse(response.body.string))
      end
    end

    def parse_embedding_response(response)
      json_response = JSON.parse(response.body.string)

      if embedding_provider == :amazon
        Langchain::LLM::AwsTitanResponse.new(json_response)
      elsif embedding_provider == :cohere
        Langchain::LLM::CohereResponse.new(json_response)
      end
    end

    def compose_embedding_parameters_amazon(params)
      default_params = @defaults.merge(params)

      {
        inputText: default_params[:text],
        dimensions: default_params[:dimensions],
        normalize: default_params[:normalize]
      }.compact
    end

    def compose_embedding_parameters_cohere(params)
      default_params = @defaults.merge(params)

      {
        texts: [default_params[:text]],
        truncate: default_params[:truncate],
        input_type: default_params[:input_type],
        embedding_types: default_params[:embedding_types]
      }.compact
    end

    def compose_parameters_cohere(params)
      default_params = @defaults.merge(params)

      {
        max_tokens: default_params[:max_tokens_to_sample],
        temperature: default_params[:temperature],
        p: default_params[:top_p],
        k: default_params[:top_k],
        stop_sequences: default_params[:stop_sequences]
      }
    end

    def compose_parameters_anthropic(params)
      params.merge(anthropic_version: "bedrock-2023-05-31")
    end

    def response_from_chunks(chunks)
      raw_response = {}

      chunks.group_by { |chunk| chunk["type"] }.each do |type, chunks|
        case type
        when "message_start"
          raw_response = chunks.first["message"]
        when "content_block_start"
          raw_response["content"] = chunks.map { |chunk| chunk["content_block"] }
        when "content_block_delta"
          chunks.group_by { |chunk| chunk["index"] }.each do |index, deltas|
            deltas.group_by { |delta| delta.dig("delta", "type") }.each do |type, deltas|
              case type
              when "text_delta"
                raw_response["content"][index]["text"] = deltas.map { |delta| delta.dig("delta", "text") }.join
              when "input_json_delta"
                json_string = deltas.map { |delta| delta.dig("delta", "partial_json") }.join
                raw_response["content"][index]["input"] = json_string.empty? ? {} : JSON.parse(json_string)
              end
            end
          end
        when "message_delta"
          chunks.each do |chunk|
            raw_response = raw_response.merge(chunk["delta"])
            raw_response["usage"] = raw_response["usage"].merge(chunk["usage"]) if chunk["usage"]
          end
        end
      end

      Langchain::LLM::AnthropicResponse.new(raw_response)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/google_vertex_ai.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around the Google Vertex AI APIs: https://cloud.google.com/vertex-ai
  #
  # Gem requirements:
  #     gem "googleauth"
  #
  # Usage:
  #     llm = Langchain::LLM::GoogleVertexAI.new(project_id: ENV["GOOGLE_VERTEX_AI_PROJECT_ID"], region: "us-central1")
  #
  class GoogleVertexAI < Base
    DEFAULTS = {
      temperature: 0.1,
      max_output_tokens: 1000,
      top_p: 0.8,
      top_k: 40,
      dimensions: 768,
      embedding_model: "textembedding-gecko",
      chat_model: "gemini-1.0-pro"
    }.freeze

    # Google Cloud has a project id and a specific region of deployment.
    # For GenAI-related things, a safe choice is us-central1.
    attr_reader :defaults, :url, :authorizer

    def initialize(project_id:, region:, default_options: {})
      depends_on "googleauth"

      @authorizer = ::Google::Auth.get_application_default(scope: [
        "https://www.googleapis.com/auth/cloud-platform",
        "https://www.googleapis.com/auth/generative-language.retriever"
      ])
      proj_id = project_id || @authorizer.project_id || @authorizer.quota_project_id
      @url = "https://#{region}-aiplatform.googleapis.com/v1/projects/#{proj_id}/locations/#{region}/publishers/google/models/"

      @defaults = DEFAULTS.merge(default_options)

      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        temperature: {default: @defaults[:temperature]},
        safety_settings: {default: @defaults[:safety_settings]}
      )
      chat_parameters.remap(
        messages: :contents,
        system: :system_instruction,
        tool_choice: :tool_config
      )
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @param model [String] ID of the model to use
    # @return [Langchain::LLM::GoogleGeminiResponse] Response object
    #
    def embed(
      text:,
      model: @defaults[:embedding_model]
    )
      params = {instances: [{content: text}]}

      uri = URI("#{url}#{model}:predict")

      parsed_response = http_post(uri, params)

      Langchain::LLM::GoogleGeminiResponse.new(parsed_response, model: model)
    end

    # Generate a chat completion for given messages
    #
    # @param messages [Array<Hash>] Input messages
    # @param model [String] The model that will complete your prompt
    # @param tools [Array<Hash>] The tools to use
    # @param tool_choice [String] The tool choice to use
    # @param system [String] The system instruction to use
    # @return [Langchain::LLM::GoogleGeminiResponse] Response object
    def chat(params = {})
      params[:system] = {parts: [{text: params[:system]}]} if params[:system]
      params[:tools] = {function_declarations: params[:tools]} if params[:tools]

      raise ArgumentError.new("messages argument is required") if Array(params[:messages]).empty?

      parameters = chat_parameters.to_params(params)
      parameters[:generation_config] = {temperature: parameters.delete(:temperature)} if parameters[:temperature]

      uri = URI("#{url}#{parameters[:model]}:generateContent")

      parsed_response = http_post(uri, parameters)

      wrapped_response = Langchain::LLM::GoogleGeminiResponse.new(parsed_response, model: parameters[:model])

      if wrapped_response.chat_completion || Array(wrapped_response.tool_calls).any?
        wrapped_response
      else
        raise StandardError.new(parsed_response)
      end
    end

    private

    def http_post(url, params)
      http = Net::HTTP.new(url.hostname, url.port)
      http.use_ssl = url.scheme == "https"
      http.set_debug_output(Langchain.logger) if Langchain.logger.debug?

      request = Net::HTTP::Post.new(url)
      request.content_type = "application/json"
      request["Authorization"] = "Bearer #{@authorizer.fetch_access_token!["access_token"]}"
      request.body = params.to_json

      response = http.request(request)

      JSON.parse(response.body)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/openai.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # LLM interface for OpenAI APIs: https://platform.openai.com/overview
  #
  # Gem requirements:
  #    gem "ruby-openai", "~> 6.3.0"
  #
  # Usage:
  #    llm = Langchain::LLM::OpenAI.new(
  #      api_key: ENV["OPENAI_API_KEY"],
  #      llm_options: {}, # Available options: https://github.com/alexrudall/ruby-openai/blob/main/lib/openai/client.rb#L5-L13
  #      default_options: {}
  #    )
  class OpenAI < Base
    DEFAULTS = {
      n: 1,
      temperature: 0.0,
      chat_model: "gpt-4o-mini",
      embedding_model: "text-embedding-3-small"
    }.freeze

    EMBEDDING_SIZES = {
      "text-embedding-ada-002" => 1536,
      "text-embedding-3-large" => 3072,
      "text-embedding-3-small" => 1536
    }.freeze

    # Initialize an OpenAI LLM instance
    #
    # @param api_key [String] The API key to use
    # @param client_options [Hash] Options to pass to the OpenAI::Client constructor
    def initialize(api_key:, llm_options: {}, default_options: {})
      depends_on "ruby-openai", req: "openai"

      llm_options[:log_errors] = Langchain.logger.debug? unless llm_options.key?(:log_errors)

      @client = ::OpenAI::Client.new(access_token: api_key, **llm_options) do |f|
        f.response :logger, Langchain.logger, {headers: true, bodies: true, errors: true}
      end

      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        logprobs: {},
        top_logprobs: {},
        n: {default: @defaults[:n]},
        temperature: {default: @defaults[:temperature]},
        user: {},
        response_format: {default: @defaults[:response_format]}
      )
      chat_parameters.ignore(:top_k)
    end

    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @param model [String] ID of the model to use
    # @param encoding_format [String] The format to return the embeddings in. Can be either float or base64.
    # @param user [String] A unique identifier representing your end-user
    # @return [Langchain::LLM::OpenAIResponse] Response object
    def embed(
      text:,
      model: defaults[:embedding_model],
      encoding_format: nil,
      user: nil,
      dimensions: @defaults[:dimensions]
    )
      raise ArgumentError.new("text argument is required") if text.empty?
      raise ArgumentError.new("model argument is required") if model.empty?
      raise ArgumentError.new("encoding_format must be either float or base64") if encoding_format && %w[float base64].include?(encoding_format)

      parameters = {
        input: text,
        model: model
      }
      parameters[:encoding_format] = encoding_format if encoding_format
      parameters[:user] = user if user

      if dimensions
        parameters[:dimensions] = dimensions
      elsif EMBEDDING_SIZES.key?(model)
        parameters[:dimensions] = EMBEDDING_SIZES[model]
      end

      # dimensions parameter not supported by text-embedding-ada-002 model
      parameters.delete(:dimensions) if model == "text-embedding-ada-002"

      response = with_api_error_handling do
        client.embeddings(parameters: parameters)
      end

      Langchain::LLM::OpenAIResponse.new(response)
    end

    # rubocop:disable Style/ArgumentsForwarding
    # Generate a completion for a given prompt
    #
    # @param prompt [String] The prompt to generate a completion for
    # @param params [Hash] The parameters to pass to the `chat()` method
    # @return [Langchain::LLM::OpenAIResponse] Response object
    def complete(prompt:, **params)
      Langchain.logger.warn "DEPRECATED: `Langchain::LLM::OpenAI#complete` is deprecated, and will be removed in the next major version. Use `Langchain::LLM::OpenAI#chat` instead."

      if params[:stop_sequences]
        params[:stop] = params.delete(:stop_sequences)
      end
      # Should we still accept the `messages: []` parameter here?
      messages = [{role: "user", content: prompt}]
      chat(messages: messages, **params)
    end

    # rubocop:enable Style/ArgumentsForwarding

    # Generate a chat completion for given messages.
    #
    # @param [Hash] params unified chat parmeters from [Langchain::LLM::Parameters::Chat::SCHEMA]
    # @option params [Array<Hash>] :messages List of messages comprising the conversation so far
    # @option params [String] :model ID of the model to use
    def chat(params = {}, &block)
      parameters = chat_parameters.to_params(params)

      raise ArgumentError.new("messages argument is required") if Array(parameters[:messages]).empty?
      raise ArgumentError.new("model argument is required") if parameters[:model].to_s.empty?
      if parameters[:tool_choice] && Array(parameters[:tools]).empty?
        raise ArgumentError.new("'tool_choice' is only allowed when 'tools' are specified.")
      end

      if block
        @response_chunks = []
        parameters[:stream_options] = {include_usage: true}
        parameters[:stream] = proc do |chunk, _bytesize|
          chunk_content = chunk.dig("choices", 0) || {}
          @response_chunks << chunk
          yield chunk_content
        end
      end

      response = with_api_error_handling do
        client.chat(parameters: parameters)
      end

      response = response_from_chunks if block
      reset_response_chunks

      Langchain::LLM::OpenAIResponse.new(response)
    end

    # Generate a summary for a given text
    #
    # @param text [String] The text to generate a summary for
    # @return [String] The summary
    def summarize(text:)
      prompt_template = Langchain::Prompt.load_from_path(
        file_path: Langchain.root.join("langchain/llm/prompts/summarize_template.yaml")
      )
      prompt = prompt_template.format(text: text)

      complete(prompt: prompt)
    end

    def default_dimensions
      @defaults[:dimensions] || EMBEDDING_SIZES.fetch(defaults[:embedding_model])
    end

    private

    attr_reader :response_chunks

    def reset_response_chunks
      @response_chunks = []
    end

    def with_api_error_handling
      response = yield
      return if response.empty?

      raise Langchain::LLM::ApiError.new "OpenAI API error: #{response.dig("error", "message")}" if response&.dig("error")

      response
    end

    def response_from_chunks
      grouped_chunks = @response_chunks
        .group_by { |chunk| chunk.dig("choices", 0, "index") }
        .except(nil) # the last chunk (that contains the token usage) has no index
      final_choices = grouped_chunks.map do |index, chunks|
        {
          "index" => index,
          "message" => {
            "role" => "assistant",
            "content" => chunks.map { |chunk| chunk.dig("choices", 0, "delta", "content") }.join,
            "tool_calls" => tool_calls_from_choice_chunks(chunks)
          }.compact,
          "finish_reason" => chunks.last.dig("choices", 0, "finish_reason")
        }
      end
      @response_chunks.first&.slice("id", "object", "created", "model")&.merge({"choices" => final_choices, "usage" => @response_chunks.last["usage"]})
    end

    def tool_calls_from_choice_chunks(choice_chunks)
      tool_call_chunks = choice_chunks.select { |chunk| chunk.dig("choices", 0, "delta", "tool_calls") }
      return nil if tool_call_chunks.empty?

      tool_call_chunks.group_by { |chunk| chunk.dig("choices", 0, "delta", "tool_calls", 0, "index") }.map do |index, chunks|
        first_chunk = chunks.first

        {
          "id" => first_chunk.dig("choices", 0, "delta", "tool_calls", 0, "id"),
          "type" => first_chunk.dig("choices", 0, "delta", "tool_calls", 0, "type"),
          "function" => {
            "name" => first_chunk.dig("choices", 0, "delta", "tool_calls", 0, "function", "name"),
            "arguments" => chunks.map { |chunk| chunk.dig("choices", 0, "delta", "tool_calls", 0, "function", "arguments") }.join
          }
        }
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/llama_cpp.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # A wrapper around the LlamaCpp.rb library
  #
  # Gem requirements:
  #     gem "llama_cpp"
  #
  # Usage:
  #     llama = Langchain::LLM::LlamaCpp.new(
  #       model_path: ENV["LLAMACPP_MODEL_PATH"],
  #       n_gpu_layers: Integer(ENV["LLAMACPP_N_GPU_LAYERS"]),
  #       n_threads: Integer(ENV["LLAMACPP_N_THREADS"])
  #     )
  #
  class LlamaCpp < Base
    attr_accessor :model_path, :n_gpu_layers, :n_ctx, :seed
    attr_writer :n_threads

    # @param model_path [String] The path to the model to use
    # @param n_gpu_layers [Integer] The number of GPU layers to use
    # @param n_ctx [Integer] The number of context tokens to use
    # @param n_threads [Integer] The CPU number of threads to use
    # @param seed [Integer] The seed to use
    def initialize(model_path:, n_gpu_layers: 1, n_ctx: 2048, n_threads: 1, seed: 0)
      depends_on "llama_cpp"

      @model_path = model_path
      @n_gpu_layers = n_gpu_layers
      @n_ctx = n_ctx
      @n_threads = n_threads
      @seed = seed
    end

    # @param text [String] The text to embed
    # @return [Array<Float>] The embedding
    def embed(text:)
      # contexts are kinda stateful when it comes to embeddings, so allocate one each time
      context = embedding_context

      embedding_input = @model.tokenize(text: text, add_bos: true)
      return unless embedding_input.size.positive?

      context.eval(tokens: embedding_input, n_past: 0)
      Langchain::LLM::LlamaCppResponse.new(context, model: context.model.desc)
    end

    # @param prompt [String] The prompt to complete
    # @param n_predict [Integer] The number of tokens to predict
    # @return [String] The completed prompt
    def complete(prompt:, n_predict: 128)
      # contexts do not appear to be stateful when it comes to completion, so re-use the same one
      context = completion_context
      ::LLaMACpp.generate(context, prompt, n_predict: n_predict)
    end

    private

    def n_threads
      # Use the maximum number of CPU threads available, if not configured
      @n_threads ||= `sysctl -n hw.ncpu`.strip.to_i
    end

    def build_context_params(embeddings: false)
      context_params = ::LLaMACpp::ContextParams.new

      context_params.seed = seed
      context_params.n_ctx = n_ctx
      context_params.n_threads = n_threads
      context_params.embedding = embeddings

      context_params
    end

    def build_model_params
      model_params = ::LLaMACpp::ModelParams.new
      model_params.n_gpu_layers = n_gpu_layers

      model_params
    end

    def build_model(embeddings: false)
      return @model if defined?(@model)
      @model = ::LLaMACpp::Model.new(model_path: model_path, params: build_model_params)
    end

    def build_completion_context
      ::LLaMACpp::Context.new(model: build_model, params: build_context_params(embeddings: false))
    end

    def build_embedding_context
      ::LLaMACpp::Context.new(model: build_model, params: build_context_params(embeddings: true))
    end

    def completion_context
      @completion_context ||= build_completion_context
    end

    def embedding_context
      @embedding_context ||= build_embedding_context
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/parameters/chat.rb`:

```rb
# frozen_string_literal: true

require "delegate"

module Langchain::LLM::Parameters
  class Chat < SimpleDelegator
    # TODO: At the moment, the UnifiedParamters only considers keys.  In the
    # future, we may consider ActiveModel-style validations and further typed
    # options here.
    SCHEMA = {
      # Either "messages" or "prompt" is required
      messages: {},
      model: {},
      prompt: {},

      # System instructions. Used by Cohere, Anthropic and Google Gemini.
      system: {},

      # Allows to force the model to produce specific output format.
      response_format: {},

      stop: {}, # multiple types (e.g. OpenAI also allows Array, null)
      stream: {}, # Enable streaming

      max_tokens: {}, # Range: [1, context_length)
      temperature: {}, # Range: [0, 2]
      top_p: {}, # Range: (0, 1]
      top_k: {}, # Range: [1, Infinity) Not available for OpenAI models
      frequency_penalty: {}, # Range: [-2, 2]
      presence_penalty: {}, # Range: [-2, 2]
      repetition_penalty: {}, # Range: (0, 2]
      seed: {}, # OpenAI only

      # Function-calling
      tools: {default: []},
      tool_choice: {},
      parallel_tool_calls: {},

      # Additional optional parameters
      logit_bias: {},

      # Additional llm options. Ollama only.
      options: {}
    }

    def initialize(parameters: {})
      super(
        ::Langchain::LLM::UnifiedParameters.new(
          schema: SCHEMA.dup,
          parameters: parameters
        )
      )
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/unified_parameters.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  class UnifiedParameters
    include Enumerable

    attr_reader :schema, :aliases, :parameters, :ignored, :remapped

    class Null < self
      def initialize(parameters: {})
        super(schema: {}, parameters: parameters)
      end
    end

    def initialize(schema:, parameters: {})
      @schema = schema || {}
      @aliases = {}
      @remapped = {}
      @ignored = Set.new
      @schema.each do |name, param|
        @aliases[name] = Set.new(Array(param[:aliases])) if param[:aliases]
      end
      @parameters = to_params(parameters.to_h) if !parameters.to_h.empty?
    end

    def to_params(params = {})
      # if params are provided, reset any previously initialized
      @parameters = params if !params.empty?
      @parameters = (@parameters || {}).merge!(params).slice(*schema.keys)
      @aliases.each do |field, aliased_keys|
        # favor existing keys in case of conflicts,
        # and check for multiples
        aliased_keys.each do |alias_key|
          @parameters[field] ||= params[alias_key] if value_present?(params[alias_key])
        end
      end
      @schema.each do |field, param_options|
        param_options ||= {}
        default = param_options[:default]
        @parameters[field] ||= default if value_present?(default)
      end
      @remapped.each do |field, renamed_field|
        @parameters[renamed_field] = @parameters[field] if value_present?(@parameters[field])
      end
      @parameters = @parameters.except(*@ignored + @remapped.keys)
    end

    def remap(field_map)
      @remapped ||= {}
      @remapped.merge!(field_map)
      field_map.each do |field, renamed_field|
        @schema[renamed_field] = @schema[field]
      end
    end

    def update(schema = {})
      @schema.merge!(schema)
      schema.each do |name, param|
        if param[:aliases]
          @aliases[name] ||= Set.new
          @aliases[name] << param[:aliases]
        end
      end
      self
    end

    def ignore(*field_names)
      @ignored.merge(field_names)
    end

    def alias_field(field_name, as:)
      @aliases[field_name] ||= Set.new
      @aliases[field_name] << as
    end

    def to_h
      @parameters.to_h
    end

    def each(&block)
      to_params.each(&block)
    end

    def <=>(other)
      to_params.<=>(other.to_params)
    end

    def [](key)
      to_params[key]
    end

    private

    def value_present?(value)
      !value.nil? && (!value.is_a?(Enumerable) || !value.empty?)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/prompts/summarize_template.yaml`:

```yaml
_type: prompt
input_variables:
  - text
template: |
  Write a concise summary of the following:

  {text}

  CONCISE SUMMARY:

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/prompts/ollama/summarize_template.yaml`:

```yaml
_type: prompt
input_variables:
  - text
template: |
  Write a concise summary of the following TEXT. Do not include the word summary, just provide the summary.

  TEXT: {text}

  CONCISE SUMMARY:

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/azure.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  # LLM interface for Azure OpenAI Service APIs: https://learn.microsoft.com/en-us/azure/ai-services/openai/
  #
  # Gem requirements:
  #    gem "ruby-openai", "~> 6.3.0"
  #
  # Usage:
  #    llm = Langchain::LLM::Azure.new(api_key:, llm_options: {}, embedding_deployment_url: chat_deployment_url:)
  #
  class Azure < OpenAI
    attr_reader :embed_client
    attr_reader :chat_client

    def initialize(
      api_key:,
      llm_options: {},
      default_options: {},
      embedding_deployment_url: nil,
      chat_deployment_url: nil
    )
      depends_on "ruby-openai", req: "openai"
      @embed_client = ::OpenAI::Client.new(
        access_token: api_key,
        uri_base: embedding_deployment_url,
        **llm_options
      )
      @chat_client = ::OpenAI::Client.new(
        access_token: api_key,
        uri_base: chat_deployment_url,
        **llm_options
      )
      @defaults = DEFAULTS.merge(default_options)
      chat_parameters.update(
        model: {default: @defaults[:chat_model]},
        logprobs: {},
        top_logprobs: {},
        n: {default: @defaults[:n]},
        temperature: {default: @defaults[:temperature]},
        user: {},
        response_format: {default: @defaults[:response_format]}
      )
      chat_parameters.ignore(:top_k)
    end

    def embed(...)
      @client = @embed_client
      super
    end

    def complete(...)
      @client = @chat_client
      super
    end

    def chat(...)
      @client = @chat_client
      super
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/replicate.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around Replicate.com LLM provider
  #
  # Gem requirements:
  #     gem "replicate-ruby", "~> 0.2.2"
  #
  # Usage:
  #     llm = Langchain::LLM::Replicate.new(api_key: ENV["REPLICATE_API_KEY"])
  class Replicate < Base
    DEFAULTS = {
      # TODO: Figure out how to send the temperature to the API
      temperature: 0.01, # Minimum accepted value
      # TODO: Design the interface to pass and use different models
      completion_model: "replicate/vicuna-13b",
      embedding_model: "creatorrr/all-mpnet-base-v2",
      dimensions: 384
    }.freeze

    #
    # Intialize the Replicate LLM
    #
    # @param api_key [String] The API key to use
    #
    def initialize(api_key:, default_options: {})
      depends_on "replicate-ruby", req: "replicate"

      ::Replicate.configure do |config|
        config.api_token = api_key
      end

      @client = ::Replicate.client
      @defaults = DEFAULTS.merge(default_options)
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to generate an embedding for
    # @return [Langchain::LLM::ReplicateResponse] Response object
    #
    def embed(text:)
      response = embeddings_model.predict(input: text)

      until response.finished?
        response.refetch
        sleep(0.1)
      end

      Langchain::LLM::ReplicateResponse.new(response, model: @defaults[:embedding_model])
    end

    #
    # Generate a completion for a given prompt
    #
    # @param prompt [String] The prompt to generate a completion for
    # @return [Langchain::LLM::ReplicateResponse] Response object
    #
    def complete(prompt:, **params)
      response = completion_model.predict(prompt: prompt)

      until response.finished?
        response.refetch
        sleep(0.1)
      end

      Langchain::LLM::ReplicateResponse.new(response, model: @defaults[:completion_model])
    end

    #
    # Generate a summary for a given text
    #
    # @param text [String] The text to generate a summary for
    # @return [String] The summary
    #
    def summarize(text:)
      prompt_template = Langchain::Prompt.load_from_path(
        file_path: Langchain.root.join("langchain/llm/prompts/summarize_template.yaml")
      )
      prompt = prompt_template.format(text: text)

      complete(
        prompt: prompt,
        temperature: @defaults[:temperature],
        # Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
        max_tokens: 2048
      )
    end

    alias_method :generate_embedding, :embed

    private

    def completion_model
      @completion_model ||= client.retrieve_model(@defaults[:completion_model]).latest_version
    end

    def embeddings_model
      @embeddings_model ||= client.retrieve_model(@defaults[:embedding_model]).latest_version
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/llm/hugging_face.rb`:

```rb
# frozen_string_literal: true

module Langchain::LLM
  #
  # Wrapper around the HuggingFace Inference API: https://huggingface.co/inference-api
  #
  # Gem requirements:
  #     gem "hugging-face", "~> 0.3.4"
  #
  # Usage:
  #     llm = Langchain::LLM::HuggingFace.new(api_key: ENV["HUGGING_FACE_API_KEY"])
  #
  class HuggingFace < Base
    DEFAULTS = {
      embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    }.freeze

    EMBEDDING_SIZES = {
      "sentence-transformers/all-MiniLM-L6-v2": 384
    }.freeze

    #
    # Intialize the HuggingFace LLM
    #
    # @param api_key [String] The API key to use
    #
    def initialize(api_key:, default_options: {})
      depends_on "hugging-face", req: "hugging_face"

      @client = ::HuggingFace::InferenceApi.new(api_token: api_key)
      @defaults = DEFAULTS.merge(default_options)
    end

    # Returns the # of vector dimensions for the embeddings
    # @return [Integer] The # of vector dimensions
    def default_dimensions
      # since Huggin Face can run multiple models, look it up or generate an embedding and return the size
      @default_dimensions ||= @defaults[:dimensions] ||
        EMBEDDING_SIZES.fetch(@defaults[:embedding_model].to_sym) do
          embed(text: "test").embedding.size
        end
    end

    #
    # Generate an embedding for a given text
    #
    # @param text [String] The text to embed
    # @return [Langchain::LLM::HuggingFaceResponse] Response object
    #
    def embed(text:)
      response = client.embedding(
        input: text,
        model: @defaults[:embedding_model]
      )
      Langchain::LLM::HuggingFaceResponse.new(response, model: @defaults[:embedding_model])
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/prompt.rb`:

```rb
module Langchain
  module Prompt
    include Loading
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/loader.rb`:

```rb
# frozen_string_literal: true

require "open-uri"

module Langchain
  class Loader
    class FileNotFound < StandardError; end

    class UnknownFormatError < StandardError; end

    URI_REGEX = %r{\A[A-Za-z][A-Za-z0-9+\-.]*://}

    # Load data from a file or URL. Shorthand for  `Langchain::Loader.new(path).load`
    #
    # == Examples
    #
    #     # load a URL
    #     data = Langchain::Loader.load("https://example.com/docs/README.md")
    #
    #     # load a file
    #     data = Langchain::Loader.load("README.md")
    #
    #    # Load data using a custom processor
    #    data = Langchain::Loader.load("README.md") do |raw_data, options|
    #      # your processing code goes here
    #      # return data at the end here
    #    end
    #
    # @param path [String | Pathname] path to file or URL
    # @param options [Hash] options passed to the processor class used to process the data
    # @return [Data] data loaded from path
    # rubocop:disable Style/ArgumentsForwarding
    def self.load(path, options = {}, &block)
      new(path, options).load(&block)
    end
    # rubocop:enable Style/ArgumentsForwarding

    # Initialize Langchain::Loader
    # @param path [String | Pathname] path to file or URL
    # @param options [Hash] options passed to the processor class used to process the data
    # @return [Langchain::Loader] loader instance
    def initialize(path, options = {}, chunker: Langchain::Chunker::Text)
      @options = options
      @path = path
      @chunker = chunker
    end

    # Is the path a URL?
    #
    # @return [Boolean] true if path is URL
    def url?
      return false if @path.is_a?(Pathname)

      !!(@path =~ URI_REGEX)
    end

    # Is the path a directory
    #
    # @return [Boolean] true if path is a directory
    def directory?
      File.directory?(@path)
    end

    # Load data from a file or URL
    #
    #    loader = Langchain::Loader.new("README.md")
    #    # Load data using default processor for the file
    #    loader.load
    #
    #    # Load data using a custom processor
    #    loader.load do |raw_data, options|
    #      # your processing code goes here
    #      # return data at the end here
    #    end
    #
    # @yield [String, Hash] handle parsing raw output into string directly
    # @yieldparam [String] raw_data from the loaded URL or file
    # @yieldreturn [String] parsed data, as a String
    #
    # @return [Data] data that was loaded
    # rubocop:disable Style/ArgumentsForwarding
    def load(&block)
      return process_data(load_from_url, &block) if url?
      return load_from_directory(&block) if directory?

      process_data(load_from_path, &block)
    end
    # rubocop:enable Style/ArgumentsForwarding

    private

    def load_from_url
      unescaped_url = URI.decode_www_form_component(@path)
      escaped_url = URI::DEFAULT_PARSER.escape(unescaped_url)
      URI.parse(escaped_url).open
    end

    def load_from_path
      return File.open(@path) if File.exist?(@path)

      raise FileNotFound, "File #{@path} does not exist"
    end

    # rubocop:disable Style/ArgumentsForwarding
    def load_from_directory(&block)
      Dir.glob(File.join(@path, "**/*")).map do |file|
        # Only load and add to result files with supported extensions
        Langchain::Loader.new(file, @options).load(&block)
      rescue
        UnknownFormatError.new("Unknown format: #{source_type}")
      end.flatten.compact
    end
    # rubocop:enable Style/ArgumentsForwarding

    def process_data(data, &block)
      @raw_data = data

      result = if block
        yield @raw_data.read, @options
      else
        processor_klass.new(@options).parse(@raw_data)
      end

      Langchain::Data.new(result, source: @options[:source], chunker: @chunker)
    end

    def processor_klass
      raise UnknownFormatError.new("Unknown format: #{source_type}") unless (kind = find_processor)

      Langchain::Processors.const_get(kind)
    end

    def find_processor
      processors.find { |klass| processor_matches? "#{klass}::#{lookup_constant}", source_type }
    end

    def processor_matches?(constant, value)
      Langchain::Processors.const_get(constant).include?(value)
    end

    def processors
      Langchain::Processors.constants
    end

    def source_type
      url? ? @raw_data.content_type : File.extname(@path)
    end

    def lookup_constant
      url? ? :CONTENT_TYPES : :EXTENSIONS
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/base.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  # = Vector Databases
  # A vector database a type of database that stores data as high-dimensional vectors, which are mathematical representations of features or attributes. Each vector has a certain number of dimensions, which can range from tens to thousands, depending on the complexity and granularity of the data.
  #
  # == Available vector databases
  #
  # - {Langchain::Vectorsearch::Chroma}
  # - {Langchain::Vectorsearch::Epsilla}
  # - {Langchain::Vectorsearch::Elasticsearch}
  # - {Langchain::Vectorsearch::Hnswlib}
  # - {Langchain::Vectorsearch::Milvus}
  # - {Langchain::Vectorsearch::Pgvector}
  # - {Langchain::Vectorsearch::Pinecone}
  # - {Langchain::Vectorsearch::Qdrant}
  # - {Langchain::Vectorsearch::Weaviate}
  #
  # == Usage
  #
  # 1. Pick a vector database from list.
  # 2. Review its documentation to install the required gems, and create an account, get an API key, etc
  # 3. Instantiate the vector database class:
  #
  #     weaviate = Langchain::Vectorsearch::Weaviate.new(
  #       url:         ENV["WEAVIATE_URL"],
  #       api_key:     ENV["WEAVIATE_API_KEY"],
  #       index_name:  "Documents",
  #       llm:         Langchain::LLM::OpenAI.new(api_key:)
  #     )
  #
  #     # You can instantiate other supported vector databases the same way:
  #     epsilla  = Langchain::Vectorsearch::Epsilla.new(...)
  #     milvus   = Langchain::Vectorsearch::Milvus.new(...)
  #     qdrant   = Langchain::Vectorsearch::Qdrant.new(...)
  #     pinecone = Langchain::Vectorsearch::Pinecone.new(...)
  #     chroma   = Langchain::Vectorsearch::Chroma.new(...)
  #     pgvector = Langchain::Vectorsearch::Pgvector.new(...)
  #
  # == Schema Creation
  #
  # `create_default_schema()` creates default schema in your vector database.
  #
  #     search.create_default_schema
  #
  # (We plan on offering customizable schema creation shortly)
  #
  # == Adding Data
  #
  # You can add data with:
  # 1. `add_data(path:, paths:)` to add any kind of data type
  #
  #     my_pdf = Langchain.root.join("path/to/my.pdf")
  #     my_text = Langchain.root.join("path/to/my.txt")
  #     my_docx = Langchain.root.join("path/to/my.docx")
  #     my_csv = Langchain.root.join("path/to/my.csv")
  #
  #     search.add_data(paths: [my_pdf, my_text, my_docx, my_csv])
  #
  # 2. `add_texts(texts:)` to only add textual data
  #
  #     search.add_texts(
  #       texts: [
  #         "Lorem Ipsum is simply dummy text of the printing and typesetting industry.",
  #         "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s"
  #       ]
  #     )
  #
  # == Retrieving Data
  #
  # `similarity_search_by_vector(embedding:, k:)` searches the vector database for the closest `k` number of embeddings.
  #
  #    search.similarity_search_by_vector(
  #      embedding: ...,
  #      k: # number of results to be retrieved
  #    )
  #
  # `vector_store.similarity_search(query:, k:)` generates an embedding for the query and searches the vector database for the closest `k` number of embeddings.
  #
  # search.similarity_search_by_vector(
  #   embedding: ...,
  #   k: # number of results to be retrieved
  # )
  #
  # `ask(question:)` generates an embedding for the passed-in question, searches the vector database for closest embeddings and then passes these as context to the LLM to generate an answer to the question.
  #
  #     search.ask(question: "What is lorem ipsum?")
  #
  class Base
    include Langchain::DependencyHelper
    extend Forwardable

    attr_reader :client, :index_name, :llm

    DEFAULT_METRIC = "cosine"

    # @param llm [Object] The LLM client to use
    def initialize(llm:)
      @llm = llm
    end

    # Method supported by Vectorsearch DB to retrieve a default schema
    def get_default_schema
      raise NotImplementedError, "#{self.class.name} does not support retrieving a default schema"
    end

    # Method supported by Vectorsearch DB to create a default schema
    def create_default_schema
      raise NotImplementedError, "#{self.class.name} does not support creating a default schema"
    end

    # Method supported by Vectorsearch DB to delete the default schema
    def destroy_default_schema
      raise NotImplementedError, "#{self.class.name} does not support deleting a default schema"
    end

    # Method supported by Vectorsearch DB to add a list of texts to the index
    def add_texts(...)
      raise NotImplementedError, "#{self.class.name} does not support adding texts"
    end

    # Method supported by Vectorsearch DB to update a list of texts to the index
    def update_texts(...)
      raise NotImplementedError, "#{self.class.name} does not support updating texts"
    end

    # Method supported by Vectorsearch DB to delete a list of texts from the index
    def remove_texts(...)
      raise NotImplementedError, "#{self.class.name} does not support deleting texts"
    end

    # Method supported by Vectorsearch DB to search for similar texts in the index
    def similarity_search(...)
      raise NotImplementedError, "#{self.class.name} does not support similarity search"
    end

    # Paper: https://arxiv.org/abs/2212.10496
    # Hypothetical Document Embeddings (HyDE)-augmented similarity search
    #
    # @param query [String] The query to search for
    # @param k [Integer] The number of results to return
    # @return [String] Response
    def similarity_search_with_hyde(query:, k: 4)
      hyde_completion = llm.complete(prompt: generate_hyde_prompt(question: query)).completion
      similarity_search(query: hyde_completion, k: k)
    end

    # Method supported by Vectorsearch DB to search for similar texts in the index by the passed in vector.
    # You must generate your own vector using the same LLM that generated the embeddings stored in the Vectorsearch DB.
    def similarity_search_by_vector(...)
      raise NotImplementedError, "#{self.class.name} does not support similarity search by vector"
    end

    # Method supported by Vectorsearch DB to answer a question given a context (data) pulled from your Vectorsearch DB.
    def ask(...)
      raise NotImplementedError, "#{self.class.name} does not support asking questions"
    end

    # HyDE-style prompt
    #
    # @param [String] User's question
    # @return [String] Prompt
    def generate_hyde_prompt(question:)
      prompt_template = Langchain::Prompt.load_from_path(
        # Zero-shot prompt to generate a hypothetical document based on a given question
        file_path: Langchain.root.join("langchain/vectorsearch/prompts/hyde.yaml")
      )
      prompt_template.format(question: question)
    end

    # Retrieval Augmented Generation (RAG)
    #
    # @param question [String] User's question
    # @param context [String] The context to synthesize the answer from
    # @return [String] Prompt
    def generate_rag_prompt(question:, context:)
      prompt_template = Langchain::Prompt.load_from_path(
        file_path: Langchain.root.join("langchain/vectorsearch/prompts/rag.yaml")
      )
      prompt_template.format(question: question, context: context)
    end

    def add_data(paths:, options: {}, chunker: Langchain::Chunker::Text)
      raise ArgumentError, "Paths must be provided" if Array(paths).empty?

      texts = Array(paths)
        .flatten
        .map do |path|
          data = Langchain::Loader.new(path, options, chunker: chunker)&.load&.chunks
          data.map { |chunk| chunk.text }
        end

      texts.flatten!

      add_texts(texts: texts)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/hnswlib.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Hnswlib < Base
    #
    # Wrapper around HNSW (Hierarchical Navigable Small World) library.
    # HNSWLib is an in-memory vectorstore that can be saved to a file on disk.
    #
    # Gem requirements:
    #     gem "hnswlib", "~> 0.8.1"
    #
    # Usage:
    #     hnsw = Langchain::Vectorsearch::Hnswlib.new(llm:, path_to_index:)

    attr_reader :client, :path_to_index

    #
    # Initialize the HNSW vector search
    #
    # @param llm [Object] The LLM client to use
    # @param path_to_index [String] The local path to the index file, e.g.: "/storage/index.ann"
    # @return [Langchain::Vectorsearch::Hnswlib] Class instance
    #
    def initialize(llm:, path_to_index:)
      depends_on "hnswlib"

      super(llm: llm)

      @client = ::Hnswlib::HierarchicalNSW.new(space: DEFAULT_METRIC, dim: llm.default_dimensions)
      @path_to_index = path_to_index

      initialize_index
    end

    #
    # Add a list of texts and corresponding IDs to the index
    #
    # @param texts [Array<String>] The list of texts to add
    # @param ids [Array<Integer>] The list of corresponding IDs (integers) to the texts
    # @return [Boolean] The response from the HNSW library
    #
    def add_texts(texts:, ids:)
      resize_index(texts.size)

      Array(texts).each_with_index do |text, i|
        embedding = llm.embed(text: text).embedding

        client.add_point(embedding, ids[i])
      end

      client.save_index(path_to_index)
    end

    # TODO: Add update_texts method

    #
    # Search for similar texts
    #
    # @param query [String] The text to search for
    # @param k [Integer] The number of results to return
    # @return [Array] Results in the format `[[id1, id2], [distance1, distance2]]`
    #
    def similarity_search(
      query:,
      k: 4
    )
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    #
    # Search for the K nearest neighbors of a given vector
    #
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @return [Array] Results in the format `[[id1, id2], [distance1, distance2]]`
    #
    def similarity_search_by_vector(
      embedding:,
      k: 4
    )
      client.search_knn(embedding, k)
    end

    # TODO: Add the ask() method
    # def ask
    # end

    private

    #
    # Optionally resizes the index if there's no space for new data
    #
    # @param num_of_elements_to_add [Integer] The number of elements to add to the index
    #
    def resize_index(num_of_elements_to_add)
      current_count = client.current_count

      if (current_count + num_of_elements_to_add) > client.max_elements
        new_size = current_count + num_of_elements_to_add

        client.resize_index(new_size)
      end
    end

    #
    # Loads or initializes the new index
    #
    def initialize_index
      if File.exist?(path_to_index)
        client.load_index(path_to_index)

        Langchain.logger.debug("#{self.class} - Successfully loaded the index at \"#{path_to_index}\"")
      else
        # Default max_elements: 100, but we constantly resize the index as new data is written to it
        client.init_index(max_elements: 100)

        Langchain.logger.debug("#{self.class} - Creating a new index at \"#{path_to_index}\"")
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/chroma.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Chroma < Base
    #
    # Wrapper around Chroma DB
    #
    # Gem requirements:
    #     gem "chroma-db", "~> 0.6.0"
    #
    # Usage:
    # chroma = Langchain::Vectorsearch::Chroma.new(url:, index_name:, llm:, api_key: nil)
    #

    # Initialize the Chroma client
    # @param url [String] The URL of the Chroma server
    # @param index_name [String] The name of the index to use
    # @param llm [Object] The LLM client to use
    def initialize(url:, index_name:, llm:)
      depends_on "chroma-db"

      ::Chroma.connect_host = url
      ::Chroma.logger = Langchain.logger
      ::Chroma.log_level = Langchain.logger.level

      @index_name = index_name

      super(llm: llm)
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to add
    # @param ids [Array<String>] The list of ids to use for the texts (optional)
    # @param metadatas [Array<Hash>] The list of metadata to use for the texts (optional)
    # @return [Hash] The response from the server
    def add_texts(texts:, ids: [], metadatas: [])
      embeddings = Array(texts).map.with_index do |text, i|
        ::Chroma::Resources::Embedding.new(
          id: ids[i] ? ids[i].to_s : SecureRandom.uuid,
          embedding: llm.embed(text: text).embedding,
          metadata: metadatas[i] || {},
          document: text # Do we actually need to store the whole original document?
        )
      end

      collection = ::Chroma::Resources::Collection.get(index_name)
      collection.add(embeddings)
    end

    def update_texts(texts:, ids:, metadatas: [])
      embeddings = Array(texts).map.with_index do |text, i|
        ::Chroma::Resources::Embedding.new(
          id: ids[i].to_s,
          embedding: llm.embed(text: text).embedding,
          metadata: metadatas[i] || {},
          document: text # Do we actually need to store the whole original document?
        )
      end

      collection.update(embeddings)
    end

    # Remove a list of texts from the index
    # @param ids [Array<String>] The list of ids to remove
    # @return [Hash] The response from the server
    def remove_texts(ids:)
      collection.delete(
        ids: ids.map(&:to_s)
      )
    end

    # Create the collection with the default schema
    # @return [::Chroma::Resources::Collection] Created collection
    def create_default_schema
      ::Chroma::Resources::Collection.create(index_name)
    end

    # Get the default schema
    # @return [::Chroma::Resources::Collection] Default schema
    def get_default_schema
      ::Chroma::Resources::Collection.get(index_name)
    end

    # Delete the default schema
    # @return [bool] Success or failure
    def destroy_default_schema
      ::Chroma::Resources::Collection.delete(index_name)
    end

    # Search for similar texts
    # @param query [String] The text to search for
    # @param k [Integer] The number of results to return
    # @return [Chroma::Resources::Embedding] The response from the server
    def similarity_search(
      query:,
      k: 4
    )
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    # Search for similar texts by embedding
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @return [Chroma::Resources::Embedding] The response from the server
    def similarity_search_by_vector(
      embedding:,
      k: 4
    )
      # Requesting more results than the number of documents in the collection currently throws an error in Chroma DB
      # Temporary fix inspired by this comment: https://github.com/chroma-core/chroma/issues/301#issuecomment-1520494512
      count = collection.count
      n_results = [count, k].min

      # workaround mentioned here: https://github.com/mariochavez/chroma/issues/29
      collection.query(query_embeddings: [embedding], results: n_results, where: nil, where_document: nil)
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result.document
      end

      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end

    private

    # @return [Chroma::Resources::Collection] The collection
    def collection
      @collection ||= ::Chroma::Resources::Collection.get(index_name)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/weaviate.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Weaviate < Base
    #
    # Wrapper around Weaviate
    #
    # Gem requirements:
    #     gem "weaviate-ruby", "~> 0.9.2"
    #
    # Usage:
    #     weaviate = Langchain::Vectorsearch::Weaviate.new(url: ENV["WEAVIATE_URL"], api_key: ENV["WEAVIATE_API_KEY"], index_name: "Docs", llm: llm)
    #

    # Initialize the Weaviate adapter
    # @param url [String] The URL of the Weaviate instance
    # @param api_key [String] The API key to use
    # @param index_name [String] The capitalized name of the index to use
    # @param llm [Object] The LLM client to use
    def initialize(url:, index_name:, llm:, api_key: nil)
      depends_on "weaviate-ruby", req: "weaviate"

      @client = ::Weaviate::Client.new(
        url: url,
        api_key: api_key,
        logger: Langchain.logger
      )

      # Weaviate requires the class name to be Capitalized: https://weaviate.io/developers/weaviate/configuration/schema-configuration#create-a-class
      # TODO: Capitalize index_name
      @index_name = index_name

      super(llm: llm)
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to add
    # @return [Hash] The response from the server
    def add_texts(texts:, ids: [])
      client.objects.batch_create(
        objects: weaviate_objects(texts, ids)
      )
    end

    # Update a list of texts in the index
    # @param texts [Array<String>] The list of texts to update
    # @return [Hash] The response from the server
    def update_texts(texts:, ids:)
      uuids = []

      # Retrieve the UUIDs of the objects to update
      Array(texts).map.with_index do |text, i|
        record = client.query.get(
          class_name: index_name,
          fields: "_additional { id }",
          where: "{ path: [\"__id\"], operator: Equal, valueString: \"#{ids[i]}\" }"
        )
        uuids.push record[0].dig("_additional", "id")
      end

      # Update the objects
      texts.map.with_index do |text, i|
        client.objects.update(
          class_name: index_name,
          id: uuids[i],
          properties: {
            __id: ids[i].to_s,
            content: text
          },
          vector: llm.embed(text: text).embedding
        )
      end
    end

    # Deletes a list of texts in the index
    # @param ids [Array] The ids of texts to delete
    # @return [Hash] The response from the server
    def remove_texts(ids:)
      raise ArgumentError, "ids must be an array" unless ids.is_a?(Array)

      client.objects.batch_delete(
        class_name: index_name,
        where: {
          path: ["__id"],
          operator: "ContainsAny",
          valueTextArray: ids
        }
      )
    end

    # Create default schema
    # @return [Hash] The response from the server
    def create_default_schema
      client.schema.create(
        class_name: index_name,
        vectorizer: "none",
        properties: [
          # __id to be used a pointer to the original document
          {dataType: ["string"], name: "__id"}, # '_id' is a reserved property name (single underscore)
          {dataType: ["text"], name: "content"}
        ]
      )
    end

    # Get default schema
    # @return [Hash] The response from the server
    def get_default_schema
      client.schema.get(class_name: index_name)
    end

    # Delete the index
    # @return [Boolean] Whether the index was deleted
    def destroy_default_schema
      client.schema.delete(class_name: index_name)
    end

    # Return documents similar to the query
    # @param query [String] The query to search for
    # @param k [Integer|String] The number of results to return
    # @return [Hash] The search results
    def similarity_search(query:, k: 4)
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(embedding: embedding, k: k)
    end

    # Return documents similar to the vector
    # @param embedding [Array<Float>] The vector to search for
    # @param k [Integer|String] The number of results to return
    # @return [Hash] The search results
    def similarity_search_by_vector(embedding:, k: 4)
      near_vector = "{ vector: #{embedding} }"

      client.query.get(
        class_name: index_name,
        near_vector: near_vector,
        limit: k.to_s,
        fields: "__id content _additional { id }"
      )
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [Hash] The answer
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result.dig("content").to_s
      end
      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end

    private

    def weaviate_objects(texts, ids = [])
      Array(texts).map.with_index do |text, i|
        weaviate_object(text, ids[i])
      end
    end

    def weaviate_object(text, id = nil)
      {
        class: index_name,
        properties: {
          __id: id.to_s,
          content: text
        },
        vector: llm.embed(text: text).embedding
      }
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/pgvector.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Pgvector < Base
    #
    # The PostgreSQL vector search adapter
    #
    # Gem requirements:
    #     gem "sequel", "~> 5.87.0"
    #     gem "pgvector", "~> 0.2"
    #
    # Usage:
    #     pgvector = Langchain::Vectorsearch::Pgvector.new(url:, index_name:, llm:, namespace: nil)
    #

    # The operators supported by the PostgreSQL vector search adapter
    OPERATORS = {
      "cosine_distance" => "cosine",
      "euclidean_distance" => "euclidean",
      "inner_product_distance" => "inner_product"
    }
    DEFAULT_OPERATOR = "cosine_distance"

    attr_reader :db, :operator, :table_name, :namespace_column, :namespace, :documents_table

    # @param url [String] The URL of the PostgreSQL database
    # @param index_name [String] The name of the table to use for the index
    # @param llm [Object] The LLM client to use
    # @param namespace [String] The namespace to use for the index when inserting/querying
    def initialize(url:, index_name:, llm:, namespace: nil)
      depends_on "sequel"
      depends_on "pgvector"

      @db = Sequel.connect(url)

      @table_name = index_name

      @namespace_column = "namespace"
      @namespace = namespace
      @operator = OPERATORS[DEFAULT_OPERATOR]

      super(llm: llm)
    end

    def documents_model
      Class.new(Sequel::Model(table_name.to_sym)) do
        plugin :pgvector, :vectors
      end
    end

    # Upsert a list of texts to the index
    # @param texts [Array<String>] The texts to add to the index
    # @param ids [Array<Integer>] The ids of the objects to add to the index, in the same order as the texts
    # @return [PG::Result] The response from the database including the ids of
    # the added or updated texts.
    def upsert_texts(texts:, ids:)
      data = texts.zip(ids).flat_map do |(text, id)|
        {id: id, content: text, vectors: llm.embed(text: text).embedding.to_s, namespace: namespace}
      end
      # @db[table_name.to_sym].multi_insert(data, return: :primary_key)
      @db[table_name.to_sym]
        .insert_conflict(
          target: :id,
          update: {content: Sequel[:excluded][:content], vectors: Sequel[:excluded][:vectors]}
        )
        .multi_insert(data, return: :primary_key)
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The texts to add to the index
    # @param ids [Array<String>] The ids to add to the index, in the same order as the texts
    # @return [Array<Integer>] The the ids of the added texts.
    def add_texts(texts:, ids: nil)
      if ids.nil? || ids.empty?
        data = texts.map do |text|
          {content: text, vectors: llm.embed(text: text).embedding.to_s, namespace: namespace}
        end

        @db[table_name.to_sym].multi_insert(data, return: :primary_key)
      else
        upsert_texts(texts: texts, ids: ids)
      end
    end

    # Update a list of ids and corresponding texts to the index
    # @param texts [Array<String>] The texts to add to the index
    # @param ids [Array<String>] The ids to add to the index, in the same order as the texts
    # @return [Array<Integer>] The ids of the updated texts.
    def update_texts(texts:, ids:)
      upsert_texts(texts: texts, ids: ids)
    end

    # Remove a list of texts from the index
    # @param ids [Array<Integer>] The ids of the texts to remove from the index
    # @return [Integer] The number of texts removed from the index
    def remove_texts(ids:)
      @db[table_name.to_sym].where(id: ids).delete
    end

    # Create default schema
    def create_default_schema
      db.run "CREATE EXTENSION IF NOT EXISTS vector"
      namespace_column = @namespace_column
      vector_dimensions = llm.default_dimensions
      db.create_table? table_name.to_sym do
        primary_key :id
        text :content
        column :vectors, "vector(#{vector_dimensions})"
        text namespace_column.to_sym, default: nil
      end
    end

    # Destroy default schema
    def destroy_default_schema
      db.drop_table? table_name.to_sym
    end

    # Search for similar texts in the index
    # @param query [String] The text to search for
    # @param k [Integer] The number of top results to return
    # @return [Array<Hash>] The results of the search
    def similarity_search(query:, k: 4)
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    # Search for similar texts in the index by the passed in vector.
    # You must generate your own vector using the same LLM that generated the embeddings stored in the Vectorsearch DB.
    # @param embedding [Array<Float>] The vector to search for
    # @param k [Integer] The number of top results to return
    # @return [Array<Hash>] The results of the search
    def similarity_search_by_vector(embedding:, k: 4)
      db.transaction do # BEGIN
        documents_model
          .nearest_neighbors(:vectors, embedding, distance: operator).limit(k)
          .where(namespace_column.to_sym => namespace)
      end
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result.content.to_s
      end
      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/epsilla.rb`:

```rb
# frozen_string_literal: true

require "securerandom"
require "timeout"

module Langchain::Vectorsearch
  class Epsilla < Base
    #
    # Wrapper around Epsilla client library
    #
    # Gem requirements:
    #     gem "epsilla-ruby", "~> 0.0.3"
    #
    # Usage:
    #     epsilla = Langchain::Vectorsearch::Epsilla.new(url:, db_name:, db_path:, index_name:, llm:)
    #
    # Initialize Epsilla client
    # @param url [String] URL to connect to the Epsilla db instance, protocol://host:port
    # @param db_name [String] The name of the database to use
    # @param db_path [String] The path to the database to use
    # @param index_name [String] The name of the Epsilla table to use
    # @param llm [Object] The LLM client to use
    def initialize(url:, db_name:, db_path:, index_name:, llm:)
      depends_on "epsilla-ruby", req: "epsilla"

      uri = URI.parse(url)
      protocol = uri.scheme
      host = uri.host
      port = uri.port

      @client = ::Epsilla::Client.new(protocol, host, port)

      Timeout.timeout(5) do
        status_code, response = @client.database.load_db(db_name, db_path)

        if status_code != 200
          if status_code == 409 || (status_code == 500 && response["message"].include?("already loaded"))
            # When db is already loaded, Epsilla may return HTTP 409 Conflict.
            # This behavior is changed in https://github.com/epsilla-cloud/vectordb/pull/95
            # Old behavior (HTTP 500) is preserved for backwards compatibility.
            # It does not prevent us from using the db.
            Langchain.logger.debug("#{self.class} - Database already loaded")
          else
            raise "Failed to load database: #{response}"
          end
        end
      end

      @client.database.use_db(db_name)

      @db_name = db_name
      @db_path = db_path
      @table_name = index_name

      @vector_dimensions = llm.default_dimensions

      super(llm: llm)
    end

    # Create a table using the index_name passed in the constructor
    def create_default_schema
      status_code, response = @client.database.create_table(@table_name, [
        {"name" => "ID", "dataType" => "STRING", "primaryKey" => true},
        {"name" => "Doc", "dataType" => "STRING"},
        {"name" => "Embedding", "dataType" => "VECTOR_FLOAT", "dimensions" => @vector_dimensions}
      ])
      raise "Failed to create table: #{response}" if status_code != 200

      response
    end

    # Drop the table using the index_name passed in the constructor
    def destroy_default_schema
      status_code, response = @client.database.drop_table(@table_name)
      raise "Failed to drop table: #{response}" if status_code != 200

      response
    end

    # Add a list of texts to the database
    # @param texts [Array<String>] The list of texts to add
    # @param ids [Array<String>] The unique ids to add to the index, in the same order as the texts; if nil, it will be random uuids
    def add_texts(texts:, ids: nil)
      validated_ids = ids
      if ids.nil?
        validated_ids = texts.map { SecureRandom.uuid }
      elsif ids.length != texts.length
        raise "The number of ids must match the number of texts"
      end

      data = texts.map.with_index do |text, idx|
        {Doc: text, Embedding: llm.embed(text: text).embedding, ID: validated_ids[idx]}
      end

      status_code, response = @client.database.insert(@table_name, data)
      raise "Failed to insert texts: #{response}" if status_code != 200
      JSON.parse(response)
    end

    # Search for similar texts
    # @param query [String] The text to search for
    # @param k [Integer] The number of results to return
    # @return [String] The response from the server
    def similarity_search(query:, k: 4)
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    # Search for entries by embedding
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @return [String] The response from the server
    def similarity_search_by_vector(embedding:, k: 4)
      status_code, response = @client.database.query(@table_name, "Embedding", embedding, ["Doc"], k, false)
      raise "Failed to do similarity search: #{response}" if status_code != 200

      data = JSON.parse(response)["result"]
      data.map { |result| result["Doc"] }
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result.to_s
      end
      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/prompts/hyde.yaml`:

```yaml
# Inspiration: https://github.com/langchain-ai/langchain/blob/v0.0.254/libs/langchain/langchain/chains/hyde/prompts.py#L4-L6
_type: prompt
input_variables:
  - question
template: |
  Please write a passage to answer the question
  
  Question: {question}

  Passage:
```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/prompts/rag.yaml`:

```yaml
_type: prompt
input_variables:
  - question
  - context
template: |
  Context:
  {context}
  ---
  Question: {question}
  ---
  Answer:
```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/qdrant.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Qdrant < Base
    #
    # Wrapper around Qdrant
    #
    # Gem requirements:
    #     gem "qdrant-ruby", "~> 0.9.8"
    #
    # Usage:
    #     qdrant = Langchain::Vectorsearch::Qdrant.new(url:, api_key:, index_name:, llm:)
    #

    # Initialize the Qdrant client
    # @param url [String] The URL of the Qdrant server
    # @param api_key [String] The API key to use
    # @param index_name [String] The name of the index to use
    # @param llm [Object] The LLM client to use
    def initialize(url:, api_key:, index_name:, llm:)
      depends_on "qdrant-ruby", req: "qdrant"

      @client = ::Qdrant::Client.new(
        url: url,
        api_key: api_key,
        logger: Langchain.logger
      )
      @index_name = index_name

      super(llm: llm)
    end

    # Find records by ids
    # @param ids [Array<Integer>] The ids to find
    # @return [Hash] The response from the server
    def find(ids: [])
      client.points.get_all(
        collection_name: index_name,
        ids: ids,
        with_payload: true,
        with_vector: true
      )
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to add
    # @return [Hash] The response from the server
    def add_texts(texts:, ids: [], payload: {})
      batch = {ids: [], vectors: [], payloads: []}

      Array(texts).each_with_index do |text, i|
        id = ids[i] || SecureRandom.uuid
        batch[:ids].push(id)
        batch[:vectors].push(llm.embed(text: text).embedding)
        batch[:payloads].push({content: text}.merge(payload))
      end

      client.points.upsert(
        collection_name: index_name,
        batch: batch
      )
    end

    def update_texts(texts:, ids:)
      add_texts(texts: texts, ids: ids)
    end

    # Remove a list of texts from the index
    # @param ids [Array<Integer>] The ids to remove
    # @return [Hash] The response from the server
    def remove_texts(ids:)
      client.points.delete(
        collection_name: index_name,
        points: ids
      )
    end

    # Get the default schema
    # @return [Hash] The response from the server
    def get_default_schema
      client.collections.get(collection_name: index_name)
    end

    # Deletes the default schema
    # @return [Hash] The response from the server
    def destroy_default_schema
      client.collections.delete(collection_name: index_name)
    end

    # Create the index with the default schema
    # @return [Hash] The response from the server
    def create_default_schema
      client.collections.create(
        collection_name: index_name,
        vectors: {
          distance: DEFAULT_METRIC.capitalize,
          size: llm.default_dimensions
        }
      )
    end

    # Search for similar texts
    # @param query [String] The text to search for
    # @param k [Integer] The number of results to return
    # @return [Hash] The response from the server
    def similarity_search(
      query:,
      k: 4
    )
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    # Search for similar texts by embedding
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @return [Hash] The response from the server
    def similarity_search_by_vector(
      embedding:,
      k: 4
    )
      response = client.points.search(
        collection_name: index_name,
        limit: k,
        vector: embedding,
        with_payload: true,
        with_vector: true
      )
      response.dig("result")
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result.dig("payload").to_s
      end
      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/pinecone.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Pinecone < Base
    #
    # Wrapper around Pinecone API.
    #
    # Gem requirements:
    #     gem "pinecone", "~> 0.1.6"
    #
    # Usage:
    #     pinecone = Langchain::Vectorsearch::Pinecone.new(environment:, api_key:, index_name:, llm:)
    #

    # Initialize the Pinecone client
    # @param environment [String] The environment to use
    # @param api_key [String] The API key to use
    # @param index_name [String] The name of the index to use
    # @param llm [Object] The LLM client to use
    def initialize(environment:, api_key:, index_name:, llm:, base_uri: nil)
      depends_on "pinecone"

      ::Pinecone.configure do |config|
        config.api_key = api_key
        config.environment = environment
        config.base_uri = base_uri if base_uri
      end

      @client = ::Pinecone::Client.new
      @index_name = index_name

      super(llm: llm)
    end

    # Find records by ids
    # @param ids [Array<Integer>] The ids to find
    # @param namespace String The namespace to search through
    # @return [Hash] The response from the server
    def find(ids: [], namespace: "")
      raise ArgumentError, "Ids must be provided" if Array(ids).empty?

      client.index(index_name).fetch(
        ids: ids,
        namespace: namespace
      )
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to add
    # @param ids [Array<Integer>] The list of IDs to add
    # @param namespace [String] The namespace to add the texts to
    # @param metadata [Hash] The metadata to use for the texts
    # @return [Hash] The response from the server
    def add_texts(texts:, ids: [], namespace: "", metadata: nil)
      vectors = texts.map.with_index do |text, i|
        {
          id: ids[i] ? ids[i].to_s : SecureRandom.uuid,
          metadata: metadata || {content: text},
          values: llm.embed(text: text).embedding
        }
      end

      index = client.index(index_name)

      index.upsert(vectors: vectors, namespace: namespace)
    end

    def add_data(paths:, namespace: "", options: {}, chunker: Langchain::Chunker::Text)
      raise ArgumentError, "Paths must be provided" if Array(paths).empty?

      texts = Array(paths)
        .flatten
        .map do |path|
          data = Langchain::Loader.new(path, options, chunker: chunker)&.load&.chunks
          data.map { |chunk| chunk.text }
        end

      texts.flatten!

      add_texts(texts: texts, namespace: namespace)
    end

    # Update a list of texts in the index
    # @param texts [Array<String>] The list of texts to update
    # @param ids [Array<Integer>] The list of IDs to update
    # @param namespace [String] The namespace to update the texts in
    # @param metadata [Hash] The metadata to use for the texts
    # @return [Array] The response from the server
    def update_texts(texts:, ids:, namespace: "", metadata: nil)
      texts.map.with_index do |text, i|
        # Pinecone::Vector#update ignore args when it is empty
        index.update(
          namespace: namespace,
          id: ids[i].to_s,
          values: llm.embed(text: text).embedding,
          set_metadata: metadata
        )
      end
    end

    # Create the index with the default schema
    # @return [Hash] The response from the server
    def create_default_schema
      client.create_index(
        metric: DEFAULT_METRIC,
        name: index_name,
        dimension: llm.default_dimensions
      )
    end

    # Delete the index
    # @return [Hash] The response from the server
    def destroy_default_schema
      client.delete_index(index_name)
    end

    # Get the default schema
    # @return [Pinecone::Vector] The default schema
    def get_default_schema
      index
    end

    # Search for similar texts
    # @param query [String] The text to search for
    # @param k [Integer] The number of results to return
    # @param namespace [String] The namespace to search in
    # @param filter [String] The filter to use
    # @return [Array] The list of results
    def similarity_search(
      query:,
      k: 4,
      namespace: "",
      filter: nil
    )
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k,
        namespace: namespace,
        filter: filter
      )
    end

    # Search for similar texts by embedding
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @param namespace [String] The namespace to search in
    # @param filter [String] The filter to use
    # @return [Array] The list of results
    def similarity_search_by_vector(embedding:, k: 4, namespace: "", filter: nil)
      index = client.index(index_name)

      query_params = {
        vector: embedding,
        namespace: namespace,
        filter: filter,
        top_k: k,
        include_values: true,
        include_metadata: true
      }.compact

      response = index.query(query_params)
      response.dig("matches")
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param namespace [String] The namespace to search in
    # @param k [Integer] The number of results to have in context
    # @param filter [String] The filter to use
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, namespace: "", filter: nil, k: 4, &block)
      search_results = similarity_search(query: question, namespace: namespace, filter: filter, k: k)

      context = search_results.map do |result|
        result.dig("metadata").to_s
      end
      context = context.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end

    # Pinecone index
    # @return [Object] The Pinecone index
    private def index
      client.index(index_name)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/milvus.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Milvus < Base
    #
    # Wrapper around Milvus REST APIs.
    #
    # Gem requirements:
    #     gem "milvus", "~> 0.10.3"
    #
    # Usage:
    #     milvus = Langchain::Vectorsearch::Milvus.new(url:, index_name:, llm:, api_key:)
    #
    def initialize(url:, index_name:, llm:, api_key: nil)
      depends_on "milvus"

      @client = ::Milvus::Client.new(
        url: url,
        logger: Langchain.logger
      )
      @index_name = index_name

      super(llm: llm)
    end

    def add_texts(texts:)
      client.entities.insert(
        collection_name: index_name,
        data: texts.map do |text|
          {content: text, vector: llm.embed(text: text).embedding}
        end
      )
    end

    # TODO: Add update_texts method

    # Deletes a list of texts in the index
    #
    # @param ids [Array<Integer>] The ids of texts to delete
    # @return [Boolean] The response from the server
    def remove_texts(ids:)
      raise ArgumentError, "ids must be an array" unless ids.is_a?(Array)

      client.entities.delete(
        collection_name: index_name,
        filter: "id in #{ids}"
      )
    end

    # TODO: Add update_texts method

    # Create default schema
    # @return [Hash] The response from the server
    def create_default_schema
      client.collections.create(
        auto_id: true,
        collection_name: index_name,
        fields: [
          {
            fieldName: "id",
            isPrimary: true,
            dataType: "Int64"
          }, {
            fieldName: "content",
            isPrimary: false,
            dataType: "VarChar",
            elementTypeParams: {
              max_length: "32768" # Largest allowed value
            }
          }, {
            fieldName: "vector",
            isPrimary: false,
            dataType: "FloatVector",
            elementTypeParams: {
              dim: llm.default_dimensions.to_s
            }
          }
        ]
      )
    end

    # Create the default index
    # @return [Boolean] The response from the server
    def create_default_index
      client.indexes.create(
        collection_name: index_name,
        index_params: [
          {
            metricType: "L2",
            fieldName: "vector",
            indexName: "vector_idx",
            indexConfig: {
              index_type: "AUTOINDEX"
            }
          }
        ]
      )
    end

    # Get the default schema
    # @return [Hash] The response from the server
    def get_default_schema
      client.collections.describe(collection_name: index_name)
    end

    # Delete default schema
    # @return [Hash] The response from the server
    def destroy_default_schema
      client.collections.drop(collection_name: index_name)
    end

    # Load default schema into memory
    # @return [Boolean] The response from the server
    def load_default_schema
      client.collections.load(collection_name: index_name)
    end

    def similarity_search(query:, k: 4)
      embedding = llm.embed(text: query).embedding

      similarity_search_by_vector(
        embedding: embedding,
        k: k
      )
    end

    def similarity_search_by_vector(embedding:, k: 4)
      load_default_schema

      client.entities.search(
        collection_name: index_name,
        anns_field: "vector",
        data: [embedding],
        limit: k,
        output_fields: ["content", "id", "vector"]
      )
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      content_data = search_results.dig("data").map { |result| result.dig("content") }

      context = content_data.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/vectorsearch/elasticsearch.rb`:

```rb
# frozen_string_literal: true

module Langchain::Vectorsearch
  class Elasticsearch < Base
    #
    # Wrapper around Elasticsearch vector search capabilities.
    #
    # Setting up Elasticsearch:
    # 1. Get Elasticsearch up and running with Docker: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
    # 2. Copy the HTTP CA certificate SHA-256 fingerprint and set the ELASTICSEARCH_CA_FINGERPRINT environment variable
    # 3. Set the ELASTICSEARCH_URL environment variable
    #
    # Gem requirements:
    #     gem "elasticsearch", "~> 8.0.0"
    #
    # Usage:
    #     llm = Langchain::LLM::OpenAI.new(api_key: ENV["OPENAI_API_KEY"])
    #     es = Langchain::Vectorsearch::Elasticsearch.new(
    #       url: ENV["ELASTICSEARCH_URL"],
    #       index_name: "docs",
    #       llm: llm,
    #       es_options: {
    #         transport_options: {ssl: {verify: false}},
    #         ca_fingerprint: ENV["ELASTICSEARCH_CA_FINGERPRINT"]
    #       }
    #     )
    #
    #     es.create_default_schema
    #     es.add_texts(texts: ["..."])
    #     es.similarity_search(text: "...")
    #
    attr_accessor :es_client, :index_name, :options

    def initialize(url:, index_name:, llm:, api_key: nil, es_options: {})
      require "elasticsearch"

      @options = {
        url: url,
        request_timeout: 20,
        logger: Langchain.logger
      }.merge(es_options)

      @es_client = ::Elasticsearch::Client.new(**options)
      @index_name = index_name

      super(llm: llm)
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to add
    # @return [Elasticsearch::Response] from the Elasticsearch server
    def add_texts(texts: [])
      body = texts.map do |text|
        [
          {index: {_index: index_name}},
          {input: text, input_vector: llm.embed(text: text).embedding}
        ]
      end.flatten

      es_client.bulk(body: body)
    end

    # Add a list of texts to the index
    # @param texts [Array<String>] The list of texts to update
    # @param texts [Array<Integer>] The list of texts to update
    # @return [Elasticsearch::Response] from the Elasticsearch server
    def update_texts(texts: [], ids: [])
      body = texts.map.with_index do |text, i|
        [
          {index: {_index: index_name, _id: ids[i]}},
          {input: text, input_vector: llm.embed(text: text).embedding}
        ]
      end.flatten

      es_client.bulk(body: body)
    end

    # Remove a list of texts from the index
    # @param ids [Array<Integer>] The list of ids to delete
    # @return [Elasticsearch::Response] from the Elasticsearch server
    def remove_texts(ids: [])
      body = ids.map do |id|
        {delete: {_index: index_name, _id: id}}
      end

      es_client.bulk(body: body)
    end

    # Create the index with the default schema
    # @return [Elasticsearch::Response] Index creation
    def create_default_schema
      es_client.indices.create(
        index: index_name,
        body: default_schema
      )
    end

    # Deletes the default schema
    # @return [Elasticsearch::Response] Index deletion
    def delete_default_schema
      es_client.indices.delete(
        index: index_name
      )
    end

    def default_vector_settings
      {type: "dense_vector", dims: llm.default_dimensions}
    end

    def vector_settings
      options[:vector_settings] || default_vector_settings
    end

    def default_schema
      {
        mappings: {
          properties: {
            input: {
              type: "text"
            },
            input_vector: vector_settings
          }
        }
      }
    end

    def default_query(query_vector)
      {
        script_score: {
          query: {match_all: {}},
          script: {
            source: "cosineSimilarity(params.query_vector, 'input_vector') + 1.0",
            params: {
              query_vector: query_vector
            }
          }
        }
      }
    end

    # Ask a question and return the answer
    # @param question [String] The question to ask
    # @param k [Integer] The number of results to have in context
    # @yield [String] Stream responses back one String at a time
    # @return [String] The answer to the question
    def ask(question:, k: 4, &block)
      search_results = similarity_search(query: question, k: k)

      context = search_results.map do |result|
        result[:input]
      end.join("\n---\n")

      prompt = generate_rag_prompt(question: question, context: context)

      messages = [{role: "user", content: prompt}]
      response = llm.chat(messages: messages, &block)

      response.context = context
      response
    end

    # Search for similar texts
    # @param text [String] The text to search for
    # @param k [Integer] The number of results to return
    # @param query [Hash] Elasticsearch query that needs to be used while searching (Optional)
    # @return [Elasticsearch::Response] The response from the server
    def similarity_search(text: "", k: 10, query: {})
      if text.empty? && query.empty?
        raise "Either text or query should pass as an argument"
      end

      if query.empty?
        query_vector = llm.embed(text: text).embedding

        query = default_query(query_vector)
      end

      es_client.search(body: {query: query, size: k}).body
    end

    # Search for similar texts by embedding
    # @param embedding [Array<Float>] The embedding to search for
    # @param k [Integer] The number of results to return
    # @param query [Hash] Elasticsearch query that needs to be used while searching (Optional)
    # @return [Elasticsearch::Response] The response from the server
    def similarity_search_by_vector(embedding: [], k: 10, query: {})
      if embedding.empty? && query.empty?
        raise "Either embedding or query should pass as an argument"
      end

      query = default_query(embedding) if query.empty?

      es_client.search(body: {query: query, size: k}).body
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/utils/to_boolean.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Utils
    class ToBoolean
      TRUTHABLE_STRINGS = %w[1 true t yes y]
      private_constant :TRUTHABLE_STRINGS

      def to_bool(value)
        case value
        when String
          TRUTHABLE_STRINGS.include?(value.downcase)
        when Integer
          value == 1
        when TrueClass
          true
        when FalseClass
          false
        when Symbol
          to_bool(value.to_s)
        else
          false
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/utils/image_wrapper.rb`:

```rb
# frozen_string_literal: true

require "open-uri"
require "base64"

module Langchain
  module Utils
    class ImageWrapper
      attr_reader :image_url

      def initialize(image_url)
        @image_url = image_url
      end

      def base64
        @base64 ||= begin
          image_data = open_image.read
          Base64.strict_encode64(image_data)
        end
      end

      def mime_type
        # TODO: Make it work with local files
        open_image.meta["content-type"]
      end

      private

      def open_image
        # TODO: Make it work with local files
        uri = URI.parse(image_url)
        raise URI::InvalidURIError, "Invalid URL scheme" unless %w[http https].include?(uri.scheme)
        @open_image ||= URI.open(image_url) # rubocop:disable Security/Open
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/utils/cosine_similarity.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Utils
    class CosineSimilarity
      attr_reader :vector_a, :vector_b

      # @param vector_a [Array<Float>] First vector
      # @param vector_b [Array<Float>] Second vector
      def initialize(vector_a, vector_b)
        @vector_a = vector_a
        @vector_b = vector_b
      end

      # Calculate the cosine similarity between two vectors
      # @return [Float] The cosine similarity between the two vectors
      def calculate_similarity
        return nil unless vector_a.is_a? Array
        return nil unless vector_b.is_a? Array
        return nil if vector_a.size != vector_b.size

        dot_product = 0
        vector_a.zip(vector_b).each do |v1i, v2i|
          dot_product += v1i * v2i
        end

        a = vector_a.map { |n| n**2 }.reduce(:+)
        b = vector_b.map { |n| n**2 }.reduce(:+)

        dot_product / (Math.sqrt(a) * Math.sqrt(b))
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/utils/hash_transformer.rb`:

```rb
module Langchain
  module Utils
    class HashTransformer
      def self.symbolize_keys(hash)
        hash.map do |k, v|
          new_key = begin
            k.to_sym
          rescue
            k
          end
          new_value = v.is_a?(Hash) ? symbolize_keys(v) : v
          [new_key, new_value]
        end.to_h
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/base.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    # Processors load and parse/process various data types such as CSVs, PDFs, Word documents, HTML pages, and others.
    class Base
      include Langchain::DependencyHelper

      EXTENSIONS = []
      CONTENT_TYPES = []

      def initialize(options = {})
        @options = options
      end

      def parse(data)
        raise NotImplementedError
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/pptx.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Pptx < Base
      EXTENSIONS = [".pptx"]
      CONTENT_TYPES = ["application/vnd.openxmlformats-officedocument.presentationml.presentation"]

      def initialize(*)
        depends_on "power_point_pptx"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        presentation = PowerPointPptx::Document.open(data)

        slides = presentation.slides
        contents = slides.map(&:content)
        text = contents.map do |sections|
          sections.map(&:strip).join(" ")
        end

        text.join("\n\n")
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/eml.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Eml < Base
      EXTENSIONS = [".eml"]
      CONTENT_TYPES = ["message/rfc822"]

      def initialize(*)
        depends_on "mail"
      end

      # Parse the document and return the cleaned text
      # @param [File] data
      # @return [String]
      def parse(data)
        mail = Mail.read(data.path)
        text_content = extract_text_content(mail)
        clean_content(text_content)
      end

      private

      # Extract text content from the email, preferring plaintext over HTML
      def extract_text_content(mail)
        text_content = ""
        text_content += "From: #{mail.from}\n" \
                        "To: #{mail.to}\n" \
                        "Cc: #{mail.cc}\n" \
                        "Bcc: #{mail.bcc}\n" \
                        "Subject: #{mail.subject}\n" \
                        "Date: #{mail.date}\n\n"
        if mail.multipart?
          mail.parts.each do |part|
            if part.content_type.start_with?("text/plain")
              text_content += part.body.decoded.force_encoding("UTF-8").strip + "\n"
            elsif part.content_type.start_with?("multipart/alternative", "multipart/mixed")
              text_content += extract_text_content(part) + "\n" # Recursively extract from multipart
            elsif part.content_type.start_with?("message/rfc822")
              # Handle embedded .eml parts as separate emails
              embedded_mail = Mail.read_from_string(part.body.decoded)
              text_content += "--- Begin Embedded Email ---\n"
              text_content += extract_text_content(embedded_mail) + "\n"
              text_content += "--- End Embedded Email ---\n"
            end
          end
        elsif mail.content_type.start_with?("text/plain")
          text_content = mail.body.decoded.force_encoding("UTF-8").strip
        end
        text_content
      end

      # Clean and format the extracted content
      def clean_content(content)
        content
          .gsub(/\[cid:[^\]]+\]/, "") # Remove embedded image references
          .gsub(URI::DEFAULT_PARSER.make_regexp(%w[http https])) { |match| "<#{match}>" } # Format URLs
          .gsub(/\r\n?/, "\n") # Normalize line endings to Unix style
          .gsub(/[\u200B-\u200D\uFEFF]/, "") # Remove zero width spaces and similar characters
          .gsub(/<\/?[^>]+>/, "") # Remove any HTML tags that might have sneaked in
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/json.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class JSON < Base
      EXTENSIONS = [".json"]
      CONTENT_TYPES = ["application/json"]

      # Parse the document and return the text
      # @param [File] data
      # @return [Hash]
      def parse(data)
        ::JSON.parse(data.read)
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/text.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Text < Base
      EXTENSIONS = [".txt"]
      CONTENT_TYPES = ["text/plain"]

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        data.read
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/csv.rb`:

```rb
# frozen_string_literal: true

require "csv"

module Langchain
  module Processors
    class CSV < Base
      class InvalidChunkMode < StandardError; end

      EXTENSIONS = [".csv"]
      CONTENT_TYPES = ["text/csv"]
      CHUNK_MODE = {
        row: "row",
        file: "file"
      }

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        case chunk_mode
        when CHUNK_MODE[:row]
          chunk_row(data)
        when CHUNK_MODE[:file]
          chunk_file(data)
        else
          raise InvalidChunkMode
        end
      end

      private

      def separator
        @options[:col_sep] || ","
      end

      def chunk_mode
        if @options[:chunk_mode].to_s.empty?
          CHUNK_MODE[:row]
        else
          raise InvalidChunkMode unless CHUNK_MODE.value?(@options[:chunk_mode])

          @options[:chunk_mode]
        end
      end

      def chunk_row(data)
        ::CSV.new(data.read, col_sep: separator).map do |row|
          row
            .compact
            .map(&:strip)
            .join(separator)
        end.join("\n\n")
      end

      def chunk_file(data)
        data.read
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/markdown.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Markdown < Base
      EXTENSIONS = [".markdown", ".md"]
      CONTENT_TYPES = ["text/markdown"]

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        data.read
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/xlsx.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Xlsx < Base
      EXTENSIONS = [".xlsx", ".xlsm"].freeze
      CONTENT_TYPES = ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"].freeze

      def initialize(*)
        depends_on "roo"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [Array<Array<String>>] Array of rows, each row is an array of cells
      def parse(data)
        xlsx_file = Roo::Spreadsheet.open(data)
        xlsx_file.each_with_pagename.flat_map do |_, sheet|
          sheet.map do |row|
            row.map { |i| i.to_s.strip }
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/jsonl.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class JSONL < Base
      EXTENSIONS = [".jsonl"]
      CONTENT_TYPES = ["application/jsonl", "application/json-lines", "application/jsonlines"]

      # Parse the document and return the text
      # @param [File] data
      # @return [Array of Hash]
      def parse(data)
        data.read.lines.map do |line|
          ::JSON.parse(line)
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/xls.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Xls < Base
      EXTENSIONS = [".xls"].freeze
      CONTENT_TYPES = ["application/vnd.ms-excel"].freeze

      def initialize(*)
        depends_on "roo"
        depends_on "roo-xls"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [Array<Array<String>>] Array of rows, each row is an array of cells
      def parse(data)
        xls_file = Roo::Spreadsheet.open(data)
        xls_file.each_with_pagename.flat_map do |_, sheet|
          sheet.map do |row|
            row.map { |i| i.to_s.strip }
          end
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/pdf.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class PDF < Base
      EXTENSIONS = [".pdf"]
      CONTENT_TYPES = ["application/pdf"]

      def initialize(*)
        depends_on "pdf-reader"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        ::PDF::Reader
          .new(StringIO.new(data.read))
          .pages
          .map(&:text)
          .join("\n\n")
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/html.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class HTML < Base
      EXTENSIONS = [".html", ".htm"]
      CONTENT_TYPES = ["text/html"]

      # We only look for headings and paragraphs
      TEXT_CONTENT_TAGS = %w[h1 h2 h3 h4 h5 h6 p]

      def initialize(*)
        depends_on "nokogiri"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        Nokogiri::HTML(data.read)
          .css(TEXT_CONTENT_TAGS.join(","))
          .map(&:inner_text)
          .join("\n\n")
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/processors/docx.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Processors
    class Docx < Base
      EXTENSIONS = [".docx"]
      CONTENT_TYPES = ["application/vnd.openxmlformats-officedocument.wordprocessingml.document"]

      def initialize(*)
        depends_on "docx"
      end

      # Parse the document and return the text
      # @param [File] data
      # @return [String]
      def parse(data)
        ::Docx::Document
          .open(StringIO.new(data.read))
          .text
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool_response.rb`:

```rb
# frozen_string_literal: true

module Langchain
  # ToolResponse represents the standardized output of a tool.
  # It can contain either text content or an image URL.
  class ToolResponse
    attr_reader :content, :image_url

    # Initializes a new ToolResponse.
    #
    # @param content [String] The text content of the response.
    # @param image_url [String, nil] Optional URL to an image.
    def initialize(content: nil, image_url: nil)
      raise ArgumentError, "Either content or image_url must be provided" if content.nil? && image_url.nil?

      @content = content
      @image_url = image_url
    end

    def to_s
      content.to_s
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/output_parsers/base.rb`:

```rb
# frozen_string_literal: true

module Langchain::OutputParsers
  # Structured output parsers from the LLM.
  #
  # @abstract
  class Base
    # Parse the output of an LLM call.
    #
    # @param text - LLM output to parse.
    #
    # @return [Object] Parsed output.
    def parse(text:)
      raise NotImplementedError
    end

    # Return a string describing the format of the output.
    #
    # @return [String] Format instructions.
    #
    # @example returns the format instructions
    # ```json
    # {
    #  "foo": "bar"
    # }
    # ```
    def get_format_instructions
      raise NotImplementedError
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/output_parsers/structured_output_parser.rb`:

```rb
# frozen_string_literal: true

require "json-schema"

module Langchain::OutputParsers
  # = Structured Output Parser
  class StructuredOutputParser < Base
    attr_reader :schema

    # Initializes a new instance of the class.
    #
    # @param schema [JSON::Schema] The json schema
    def initialize(schema:)
      @schema = validate_schema!(schema)
    end

    def to_h
      {
        _type: "StructuredOutputParser",
        schema: schema.to_json
      }
    end

    # Creates a new instance of the class using the given JSON::Schema.
    #
    # @param schema [JSON::Schema] The JSON::Schema to use
    #
    # @return [Object] A new instance of the class
    def self.from_json_schema(schema)
      new(schema: schema)
    end

    # Returns a string containing instructions for how the output of a language model should be formatted
    # according to the @schema.
    #
    # @return [String] Instructions for how the output of a language model should be formatted
    # according to the @schema.
    def get_format_instructions
      <<~INSTRUCTIONS
        You must format your output as a JSON value that adheres to a given "JSON Schema" instance.

        "JSON Schema" is a declarative language that allows you to annotate and validate JSON documents.

        For example, the example "JSON Schema" instance {"properties": {"foo": {"description": "a list of test words", "type": "array", "items": {"type": "string"}}}, "required": ["foo"]}}
        would match an object with one required property, "foo". The "type" property specifies "foo" must be an "array", and the "description" property semantically describes it as "a list of test words". The items within "foo" must be strings.
        Thus, the object {"foo": ["bar", "baz"]} is a well-formatted instance of this example "JSON Schema". The object {"properties": {"foo": ["bar", "baz"]}}} is not well-formatted.

        Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match the schema exactly and there are no trailing commas!

        Here is the JSON Schema instance your output must adhere to. Include the enclosing markdown codeblock:
        ```json
        #{schema.to_json}
        ```
      INSTRUCTIONS
    end

    # Parse the output of an LLM call extracting an object that abides by the @schema
    #
    # @param text [String] Text output from the LLM call
    # @return [Object] object that abides by the @schema
    def parse(text)
      json = text.include?("```") ? text.strip.split(/```(?:json)?/)[1] : text.strip
      parsed = JSON.parse(json)
      JSON::Validator.validate!(schema, parsed)
      parsed
    rescue => e
      raise OutputParserException.new("Failed to parse. Text: \"#{text}\". Error: #{e}", text)
    end

    private

    def validate_schema!(schema)
      errors = JSON::Validator.fully_validate_schema(schema)
      unless errors.empty?
        raise ArgumentError, "Invalid schema: \n#{errors.join("\n")}"
      end
      schema
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/output_parsers/output_parser_exception.rb`:

```rb
class Langchain::OutputParsers::OutputParserException < StandardError
  def initialize(message, text)
    @message = message
    @text = text
  end

  def to_s
    "#{@message}\nText: #{@text}"
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/output_parsers/prompts/naive_fix_prompt.yaml`:

```yaml
_type: prompt
input_variables:
  - instructions
  - completion
  - error
template: | 
  Instructions:
  --------------
  {instructions}
  --------------
  Completion:
  --------------
  {completion}
  --------------
  
  Above, the Completion did not satisfy the constraints given in the Instructions.
  Error:
  --------------
  {error}
  --------------
  
  Please try again. Please only respond with an answer that satisfies the constraints laid out in the Instructions:
```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/output_parsers/output_fixing_parser.rb`:

```rb
# frozen_string_literal: true

module Langchain::OutputParsers
  # = Output Fixing Parser
  #
  class OutputFixingParser < Base
    attr_reader :llm, :parser, :prompt

    # Initializes a new instance of the class.
    #
    # @param llm [Langchain::LLM] The LLM used in the fixing process
    # @param parser [Langchain::OutputParsers] The parser originally used which resulted in parsing error
    # @param prompt [Langchain::Prompt::PromptTemplate]
    def initialize(llm:, parser:, prompt:)
      raise ArgumentError.new("llm must be an instance of Langchain::LLM got: #{llm.class}") unless llm.is_a?(Langchain::LLM::Base)
      raise ArgumentError.new("parser must be an instance of Langchain::OutputParsers got #{parser.class}") unless parser.is_a?(Langchain::OutputParsers::Base)
      raise ArgumentError.new("prompt must be an instance of Langchain::Prompt::PromptTemplate got #{prompt.class}") unless prompt.is_a?(Langchain::Prompt::PromptTemplate)
      @llm = llm
      @parser = parser
      @prompt = prompt
    end

    def to_h
      {
        _type: "OutputFixingParser",
        parser: parser.to_h,
        prompt: prompt.to_h
      }
    end

    # calls get_format_instructions on the @parser
    #
    # @return [String] Instructions for how the output of a language model should be formatted
    # according to the @schema.
    def get_format_instructions
      parser.get_format_instructions
    end

    # Parse the output of an LLM call, if fails with OutputParserException
    # then call the LLM with a fix prompt in an attempt to get the correctly
    # formatted response
    #
    # @param completion [String] Text output from the LLM call
    #
    # @return [Object] object that is succesfully parsed by @parser.parse
    def parse(completion)
      parser.parse(completion)
    rescue OutputParserException => e
      new_completion = llm.chat(
        messages: [{role: "user",
                    content: prompt.format(
                      instructions: parser.get_format_instructions,
                      completion: completion,
                      error: e
                    )}]
      ).completion
      parser.parse(new_completion)
    end

    # Creates a new instance of the class using the given JSON::Schema.
    #
    # @param llm [Langchain::LLM] The LLM used in the fixing process
    # @param parser [Langchain::OutputParsers] The parser originally used which resulted in parsing error
    # @param prompt [Langchain::Prompt::PromptTemplate]
    #
    # @return [Object] A new instance of the class
    def self.from_llm(llm:, parser:, prompt: nil)
      new(llm: llm, parser: parser, prompt: prompt || naive_fix_prompt)
    end

    private

    private_class_method def self.naive_fix_prompt
      Langchain::Prompt.load_from_path(
        file_path: Langchain.root.join("langchain/output_parsers/prompts/naive_fix_prompt.yaml")
      )
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunk.rb`:

```rb
# frozen_string_literal: true

module Langchain
  class Chunk
    # The chunking process is the process of splitting a document into smaller chunks and creating instances of Langchain::Chunk

    attr_reader :text

    # Initialize a new chunk
    # @param [String] text
    # @return [Langchain::Chunk]
    def initialize(text:)
      @text = text
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/prompt/base.rb`:

```rb
# frozen_string_literal: true

require "strscan"
require "yaml"

module Langchain::Prompt
  # Prompts are structured inputs to the LLMs. Prompts provide instructions, context and other user input that LLMs use to generate responses.
  #
  # @abstract
  class Base
    def format(**kwargs)
      raise NotImplementedError
    end

    # @return [String] the type of the prompt
    def prompt_type
      raise NotImplementedError
    end

    # @return [Hash] a hash representation of the prompt
    def to_h
      raise NotImplementedError
    end

    #
    # Validate the input variables against the template.
    #
    # @param template [String] The template to validate against.
    # @param input_variables [Array<String>] The input variables to validate.
    #
    # @raise [ArgumentError] If there are missing or extra variables.
    #
    # @return [void]
    #
    def validate(template:, input_variables:)
      input_variables_set = input_variables.uniq
      variables_from_template = Langchain::Prompt::Base.extract_variables_from_template(template)

      missing_variables = variables_from_template - input_variables_set
      extra_variables = input_variables_set - variables_from_template

      raise ArgumentError, "Missing variables: #{missing_variables}" if missing_variables.any?
      raise ArgumentError, "Extra variables: #{extra_variables}" if extra_variables.any?
    end

    #
    # Save the object to a file in JSON or YAML format.
    #
    # @param file_path [String, Pathname] The path to the file to save the object to
    #
    # @raise [ArgumentError] If file_path doesn't end with .json or .yaml or .yml
    #
    # @return [void]
    #
    def save(file_path:)
      save_path = file_path.is_a?(String) ? Pathname.new(file_path) : file_path
      directory_path = save_path.dirname
      FileUtils.mkdir_p(directory_path) unless directory_path.directory?

      case save_path.extname
      when ".json"
        File.write(file_path, to_h.to_json)
      when ".yaml", ".yml"
        File.write(file_path, to_h.to_yaml)
      else
        raise ArgumentError, "#{file_path} must be json or yaml file"
      end
    end

    #
    # Extracts variables from a template string.
    #
    # This method takes a template string and returns an array of input variable names
    # contained within the template. Input variables are defined as text enclosed in
    # curly braces (e.g. <code>\{variable_name\}</code>).
    #
    # Content within two consecutive curly braces (e.g. <code>\{\{ignore_me}}</code>) are ignored.
    #
    # @param template [String] The template string to extract variables from.
    #
    # @return [Array<String>] An array of input variable names.
    #
    def self.extract_variables_from_template(template)
      input_variables = []
      scanner = StringScanner.new(template)

      while scanner.scan_until(/\{([^}]*)\}/)
        variable = scanner[1].strip
        input_variables << variable unless variable.empty? || variable[0] == "{"
      end

      input_variables
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/prompt/prompt_template.rb`:

```rb
# frozen_string_literal: true

module Langchain::Prompt
  # = Prompt Templates
  #
  # Create a prompt with one input variable:
  #
  #     prompt = Langchain::Prompt::PromptTemplate.new(template: "Tell me a {adjective} joke.", input_variables: ["adjective"])
  #     prompt.format(adjective: "funny") # "Tell me a funny joke."
  #
  # Create a prompt with multiple input variables:
  #
  #     prompt = Langchain::Prompt::PromptTemplate.new(template: "Tell me a {adjective} joke about {content}.", input_variables: ["adjective", "content"])
  #     prompt.format(adjective: "funny", content: "chickens") # "Tell me a funny joke about chickens."
  #
  # Creating a PromptTemplate using just a prompt and no input_variables:
  #
  #     prompt = Langchain::Prompt::PromptTemplate.from_template("Tell me a {adjective} joke about {content}.")
  #     prompt.input_variables # ["adjective", "content"]
  #     prompt.format(adjective: "funny", content: "chickens") # "Tell me a funny joke about chickens."
  #
  # Save prompt template to JSON file:
  #
  #     prompt.save(file_path: "spec/fixtures/prompt/prompt_template.json")
  #
  # Loading a new prompt template using a JSON file:
  #
  #     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.json")
  #     prompt.input_variables # ["adjective", "content"]
  #
  # Loading a new prompt template using a YAML file:
  #     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.yaml")
  #     prompt.input_variables #=> ["adjective", "content"]
  #
  class PromptTemplate < Base
    attr_reader :template, :input_variables, :validate_template

    #
    # Initializes a new instance of the class.
    #
    # @param template [String] The prompt template.
    # @param input_variables [Array<String>] A list of the names of the variables the prompt template expects.
    # @param validate_template [Boolean] Whether or not to try validating the template.
    #
    def initialize(template:, input_variables:, validate_template: true)
      @template = template
      @input_variables = input_variables
      @validate_template = validate_template

      validate(template: @template, input_variables: @input_variables) if @validate_template
    end

    #
    # Format the prompt with the inputs. Double <code>{{}}</code> replaced with single <code>{}</code> to adhere to f-string spec.
    #
    # @param kwargs [Hash] Any arguments to be passed to the prompt template.
    # @return [String] A formatted string.
    #
    def format(**kwargs)
      result = @template
      result = result.gsub(/{{/, "{").gsub(/}}/, "}")
      kwargs.each { |key, value| result = result.gsub(/\{#{key}\}/, value.to_s) }
      result
    end

    #
    # Returns the key type of prompt as a string.
    #
    # @return [String] the prompt type key
    #
    def prompt_type
      "prompt"
    end

    def to_h
      {
        _type: prompt_type,
        input_variables: @input_variables,
        template: @template
      }
    end

    #
    # Creates a new instance of the class using the given template.
    #
    # @param template [String] The template to use
    #
    # @return [Object] A new instance of the class
    #
    def self.from_template(template)
      new(template: template, input_variables: extract_variables_from_template(template))
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/prompt/few_shot_prompt_template.rb`:

```rb
# frozen_string_literal: true

module Langchain::Prompt
  # = Few Shot Prompt Templates
  #
  # Create a prompt with a few shot examples:
  #
  #     prompt = Langchain::Prompt::FewShotPromptTemplate.new(
  #       prefix: "Write antonyms for the following words.",
  #       suffix: "Input: <code>{adjective}</code>\nOutput:",
  #       example_prompt: Langchain::Prompt::PromptTemplate.new(
  #         input_variables: ["input", "output"],
  #         template: "Input: {input}\nOutput: {output}"
  #       ),
  #       examples: [
  #         { "input": "happy", "output": "sad" },
  #         { "input": "tall", "output": "short" }
  #       ],
  #        input_variables: ["adjective"]
  #     )
  #
  #     prompt.format(adjective: "good")
  #
  #     # Write antonyms for the following words.
  #     #
  #     # Input: happy
  #     # Output: sad
  #     #
  #     # Input: tall
  #     # Output: short
  #     #
  #     # Input: good
  #     # Output:
  #
  # Save prompt template to JSON file:
  #
  #     prompt.save(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
  #
  # Loading a new prompt template using a JSON file:
  #
  #     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/few_shot_prompt_template.json")
  #     prompt.prefix # "Write antonyms for the following words."
  #
  # Loading a new prompt template using a YAML file:
  #
  #     prompt = Langchain::Prompt.load_from_path(file_path: "spec/fixtures/prompt/prompt_template.yaml")
  #     prompt.input_variables #=> ["adjective", "content"]
  #
  class FewShotPromptTemplate < Base
    attr_reader :examples, :example_prompt, :input_variables, :prefix, :suffix, :example_separator

    #
    # Initializes a new instance of the class.
    #
    # @param examples [Array<Hash>] Examples to format into the prompt.
    # @param example_prompt [PromptTemplate] PromptTemplate used to format an individual example.
    # @param suffix [String] A prompt template string to put after the examples.
    # @param input_variables [Array<String>] A list of the names of the variables the prompt template expects.
    # @param example_separator [String] String separator used to join the prefix, the examples, and suffix.
    # @param prefix [String] A prompt template string to put before the examples.
    # @param validate_template [Boolean] Whether or not to try validating the template.
    #
    def initialize(
      examples:,
      example_prompt:,
      input_variables:,
      suffix:,
      prefix: "",
      example_separator: "\n\n",
      validate_template: true
    )
      @examples = examples
      @example_prompt = example_prompt
      @input_variables = input_variables
      @prefix = prefix
      @suffix = suffix
      @example_separator = example_separator
      @validate_template = validate_template

      validate(template: @prefix + @suffix, input_variables: @input_variables) if @validate_template
    end

    #
    # Format the prompt with the inputs.
    #
    # @param kwargs [Hash] Any arguments to be passed to the prompt template.
    #
    # @return [String] A formatted string.
    #
    def format(**kwargs)
      example_string = @examples.map { |example| @example_prompt.format(**example) }

      suffix_string = @suffix
      kwargs.each { |key, value| suffix_string = suffix_string.gsub(/\{#{key}\}/, value.to_s) }

      [@prefix, *example_string, suffix_string].join(@example_separator)
    end

    #
    # Returns the key type of prompt as a string.
    #
    # @return [String] the prompt type key
    #
    def prompt_type
      "few_shot"
    end

    def to_h
      {
        _type: prompt_type,
        input_variables: @input_variables,
        prefix: @prefix,
        example_prompt: @example_prompt.to_h,
        examples: @examples,
        suffix: @suffix
      }
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/prompt/loading.rb`:

```rb
# frozen_string_literal: true

require "strscan"
require "pathname"
require "yaml"

module Langchain::Prompt
  TYPE_TO_LOADER = {
    "prompt" => ->(config) { load_prompt(config) },
    "few_shot" => ->(config) { load_few_shot_prompt(config) }
  }

  module Loading
    def self.included(base)
      base.extend ClassMethods
    end

    module ClassMethods
      #
      # Load prompt from file.
      #
      # @param file_path [String, Pathname] The path of the file to read the configuration data from.
      #
      # @return [Object] The loaded prompt loaded.
      #
      # @raise [ArgumentError] If the file type of the specified file path is not supported.
      #
      def load_from_path(file_path:)
        file_path = file_path.is_a?(String) ? Pathname.new(file_path) : file_path

        case file_path.extname
        when ".json"
          config = JSON.parse(File.read(file_path))
        when ".yaml", ".yml"
          config = YAML.safe_load_file(file_path)
        else
          raise ArgumentError, "Got unsupported file type #{file_path.extname}"
        end

        load_from_config(config)
      end

      #
      # Loads a prompt template with the given configuration.
      #
      # @param config [Hash] A hash containing the configuration for the prompt.
      #
      # @return [PromptTemplate] The loaded prompt loaded.
      #
      def load_prompt(config)
        template, input_variables = config.values_at("template", "input_variables")
        PromptTemplate.new(template: template, input_variables: input_variables)
      end

      #
      # Loads a prompt template with the given configuration.
      #
      # @param config [Hash] A hash containing the configuration for the prompt.
      #
      # @return [FewShotPromptTemplate] The loaded prompt loaded.
      #
      def load_few_shot_prompt(config)
        prefix, suffix, example_prompt, examples, input_variables = config.values_at("prefix", "suffix", "example_prompt", "examples", "input_variables")
        example_prompt = load_prompt(example_prompt)
        FewShotPromptTemplate.new(prefix: prefix, suffix: suffix, example_prompt: example_prompt, examples: examples, input_variables: input_variables)
      end

      private

      #
      # Loads the prompt from the given configuration hash
      #
      # @param config [Hash] the configuration hash to load from
      #
      # @return [Object] the loaded prompt
      #
      # @raise [ArgumentError] if the prompt type specified in the config is not supported
      #
      def load_from_config(config)
        # If `_type` key is not present in the configuration hash, add it with a default value of `prompt`
        unless config.key?("_type")
          Langchain.logger.warn("#{self.class} - No `_type` key found, defaulting to `prompt`")
          config["_type"] = "prompt"
        end

        # If the prompt type specified in the configuration hash is not supported, raise an exception
        unless TYPE_TO_LOADER.key?(config["_type"])
          raise ArgumentError, "Loading #{config["_type"]} prompt not supported"
        end

        # Load the prompt using the corresponding loader function from the `TYPE_TO_LOADER` hash
        prompt_loader = TYPE_TO_LOADER[config["_type"]]
        prompt_loader.call(config)
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/assistant.rb`:

```rb
# frozen_string_literal: true

module Langchain
  # Assistants are Agent-like objects that leverage helpful instructions, LLMs, tools and knowledge to respond to user queries.
  # Assistants can be configured with an LLM of your choice, any vector search database and easily extended with additional tools.
  #
  # Usage:
  #     llm = Langchain::LLM::GoogleGemini.new(api_key: ENV["GOOGLE_GEMINI_API_KEY"])
  #     assistant = Langchain::Assistant.new(
  #       llm: llm,
  #       instructions: "You're a News Reporter AI",
  #       tools: [Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])]
  #     )
  class Assistant
    attr_reader :llm,
      :instructions,
      :state,
      :llm_adapter,
      :messages,
      :tool_choice,
      :total_prompt_tokens,
      :total_completion_tokens,
      :total_tokens

    attr_accessor :tools,
      :add_message_callback,
      :tool_execution_callback,
      :parallel_tool_calls

    # Create a new assistant
    #
    # @param llm [Langchain::LLM::Base] LLM instance that the assistant will use
    # @param tools [Array<Langchain::Tool::Base>] Tools that the assistant has access to
    # @param instructions [String] The system instructions
    # @param tool_choice [String] Specify how tools should be selected. Options: "auto", "any", "none", or <specific function name>
    # @param parallel_tool_calls [Boolean] Whether or not to run tools in parallel
    # @param messages [Array<Langchain::Assistant::Messages::Base>] The messages
    # @param add_message_callback [Proc] A callback function (Proc or lambda) that is called when any message is added to the conversation
    # @param tool_execution_callback [Proc] A callback function (Proc or lambda) that is called right before a tool function is executed
    def initialize(
      llm:,
      tools: [],
      instructions: nil,
      tool_choice: "auto",
      parallel_tool_calls: true,
      messages: [],
      # Callbacks
      add_message_callback: nil,
      tool_execution_callback: nil,
      &block
    )
      unless tools.is_a?(Array) && tools.all? { |tool| tool.class.singleton_class.included_modules.include?(Langchain::ToolDefinition) }
        raise ArgumentError, "Tools must be an array of objects extending Langchain::ToolDefinition"
      end

      @llm = llm
      @llm_adapter = LLM::Adapter.build(llm)

      @add_message_callback = add_message_callback if validate_callback!("add_message_callback", add_message_callback)
      @tool_execution_callback = tool_execution_callback if validate_callback!("tool_execution_callback", tool_execution_callback)

      self.messages = messages
      @tools = tools
      @parallel_tool_calls = parallel_tool_calls
      self.tool_choice = tool_choice
      self.instructions = instructions
      @block = block
      @state = :ready

      @total_prompt_tokens = 0
      @total_completion_tokens = 0
      @total_tokens = 0
    end

    # Add a user message to the messages array
    #
    # @param role [String] The role attribute of the message. Default: "user"
    # @param content [String] The content of the message
    # @param image_url [String] The URL of the image to include in the message
    # @param tool_calls [Array<Hash>] The tool calls to include in the message
    # @param tool_call_id [String] The ID of the tool call to include in the message
    # @return [Array<Langchain::Message>] The messages
    def add_message(role: "user", content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
      message = build_message(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)

      # Call the callback with the message
      add_message_callback.call(message) if add_message_callback # rubocop:disable Style/SafeNavigation

      # Prepend the message to the messages array
      messages << message

      @state = :ready

      messages
    end

    # Convert messages to an LLM APIs-compatible array of hashes
    #
    # @return [Array<Hash>] Messages as an OpenAI API-compatible array of hashes
    def array_of_message_hashes
      messages
        .map(&:to_hash)
        .compact
    end

    # Only used by the Assistant when it calls the LLM#complete() method
    def prompt_of_concatenated_messages
      messages.map(&:to_s).join
    end

    # Set multiple messages
    #
    # @param messages [Array<Langchain::Message>] The messages to set
    # @return [Array<Langchain::Message>] The messages
    def messages=(messages)
      raise ArgumentError, "messages array must only contain Langchain::Message instance(s)" unless messages.is_a?(Array) && messages.all? { |m| m.is_a?(Messages::Base) }

      @messages = messages
    end

    # Add multiple messages
    #
    # @param messages [Array<Hash>] The messages to add
    # @return [Array<Langchain::Message>] The messages
    def add_messages(messages:)
      messages.each do |message_hash|
        add_message(**message_hash.slice(:content, :role, :tool_calls, :tool_call_id))
      end
    end

    # Run the assistant
    #
    # @param auto_tool_execution [Boolean] Whether or not to automatically run tools
    # @return [Array<Langchain::Message>] The messages
    def run(auto_tool_execution: false)
      if messages.empty?
        Langchain.logger.warn("#{self.class} - No messages to process")
        @state = :completed
        return
      end

      @state = :in_progress
      @state = handle_state until run_finished?(auto_tool_execution)

      messages
    end

    # Run the assistant with automatic tool execution
    #
    # @return [Array<Langchain::Message>] The messages
    def run!
      run(auto_tool_execution: true)
    end

    # Add a user message and run the assistant
    #
    # @param content [String] The content of the message
    # @param auto_tool_execution [Boolean] Whether or not to automatically run tools
    # @return [Array<Langchain::Message>] The messages
    def add_message_and_run(content: nil, image_url: nil, auto_tool_execution: false)
      add_message(content: content, image_url: image_url, role: "user")
      run(auto_tool_execution: auto_tool_execution)
    end

    # Add a user message and run the assistant with automatic tool execution
    #
    # @param content [String] The content of the message
    # @return [Array<Langchain::Message>] The messages
    def add_message_and_run!(content: nil, image_url: nil)
      add_message_and_run(content: content, image_url: image_url, auto_tool_execution: true)
    end

    # Submit tool output
    #
    # @param tool_call_id [String] The ID of the tool call to submit output for
    # @param output [String] The output of the tool
    # @return [Array<Langchain::Message>] The messages
    def submit_tool_output(tool_call_id:, output:)
      # TODO: Validate that `tool_call_id` is valid by scanning messages and checking if this tool call ID was invoked
      add_message(role: @llm_adapter.tool_role, content: output, tool_call_id: tool_call_id)
    end

    # Delete all messages
    #
    # @return [Array] Empty messages array
    def clear_messages!
      # TODO: If this a bug? Should we keep the "system" message?
      @messages = []
    end

    # Set new instructions
    #
    # @param new_instructions [String] New instructions that will be set as a system message
    # @return [Array<Langchain::Message>] The messages
    def instructions=(new_instructions)
      @instructions = new_instructions

      if @llm_adapter.support_system_message?
        # TODO: Should we still set a system message even if @instructions is "" or nil?
        replace_system_message!(content: new_instructions)
      end
    end

    # Set tool_choice, how tools should be selected
    #
    # @param new_tool_choice [String] Tool choice
    # @return [String] Selected tool choice
    def tool_choice=(new_tool_choice)
      validate_tool_choice!(new_tool_choice)
      @tool_choice = new_tool_choice
    end

    private

    # Replace old system message with new one
    #
    # @param content [String] New system message content
    # @return [Array<Langchain::Message>] The messages
    def replace_system_message!(content:)
      messages.delete_if(&:system?)
      return if content.nil?

      message = build_message(role: "system", content: content)
      messages.unshift(message)
    end

    # TODO: If tool_choice = "tool_function_name" and then tool is removed from the assistant, should we set tool_choice back to "auto"?
    def validate_tool_choice!(tool_choice)
      allowed_tool_choices = llm_adapter.allowed_tool_choices.concat(available_tool_names)
      unless allowed_tool_choices.include?(tool_choice)
        raise ArgumentError, "Tool choice must be one of: #{allowed_tool_choices.join(", ")}"
      end
    end

    # Check if the run is finished
    #
    # @param auto_tool_execution [Boolean] Whether or not to automatically run tools
    # @return [Boolean] Whether the run is finished
    def run_finished?(auto_tool_execution)
      finished_states = [:completed, :failed]

      requires_manual_action = (@state == :requires_action) && !auto_tool_execution
      finished_states.include?(@state) || requires_manual_action
    end

    # Handle the current state and transition to the next state
    #
    # @return [Symbol] The next state
    def handle_state
      case @state
      when :in_progress
        process_latest_message
      when :requires_action
        execute_tools
      end
    end

    # Process the latest message
    #
    # @return [Symbol] The next state
    def process_latest_message
      last_message = messages.last

      case last_message.standard_role
      when :system
        handle_system_message
      when :llm
        handle_llm_message
      when :user, :tool
        handle_user_or_tool_message
      else
        handle_unexpected_message
      end
    end

    # Handle system message scenario
    #
    # @return [Symbol] The completed state
    def handle_system_message
      Langchain.logger.warn("#{self.class} - At least one user message is required after a system message")
      :completed
    end

    # Handle LLM message scenario
    #
    # @return [Symbol] The next state
    def handle_llm_message
      messages.last.tool_calls.any? ? :requires_action : :completed
    end

    # Handle unexpected message scenario
    #
    # @return [Symbol] The failed state
    def handle_unexpected_message
      Langchain.logger.error("#{self.class} - Unexpected message role encountered: #{messages.last.standard_role}")
      :failed
    end

    # Handle user or tool message scenario by processing the LLM response
    #
    # @return [Symbol] The next state
    def handle_user_or_tool_message
      response = chat_with_llm

      add_message(role: response.role, content: response.chat_completion, tool_calls: response.tool_calls)
      record_used_tokens(response.prompt_tokens, response.completion_tokens, response.total_tokens)

      set_state_for(response: response)
    end

    def set_state_for(response:)
      if response.tool_calls.any?
        :in_progress
      elsif response.chat_completion
        :completed
      elsif response.completion # Currently only used by Ollama
        :completed
      else
        Langchain.logger.error("#{self.class} - LLM response does not contain tool calls, chat or completion response")
        :failed
      end
    end

    # Execute the tools based on the tool calls in the last message
    #
    # @return [Symbol] The next state
    def execute_tools
      run_tools(messages.last.tool_calls)
      :in_progress
    rescue => e
      Langchain.logger.error("#{self.class} - Error running tools: #{e.message}; #{e.backtrace.join('\n')}")
      :failed
    end

    # Call to the LLM#chat() method
    #
    # @return [Langchain::LLM::BaseResponse] The LLM response object
    def chat_with_llm
      Langchain.logger.debug("#{self.class} - Sending a call to #{llm.class}")

      params = @llm_adapter.build_chat_params(
        instructions: @instructions,
        messages: array_of_message_hashes,
        tools: @tools,
        tool_choice: tool_choice,
        parallel_tool_calls: parallel_tool_calls
      )
      @llm.chat(**params, &@block)
    end

    # Run the tools automatically
    #
    # @param tool_calls [Array<Hash>] The tool calls to run
    def run_tools(tool_calls)
      # Iterate over each function invocation and submit tool output
      tool_calls.each do |tool_call|
        run_tool(tool_call)
      end
    end

    # Run the tool call
    #
    # @param tool_call [Hash] The tool call to run
    # @return [Object] The result of the tool call
    def run_tool(tool_call)
      tool_call_id, tool_name, method_name, tool_arguments = @llm_adapter.extract_tool_call_args(tool_call: tool_call)

      tool_instance = tools.find do |t|
        t.class.tool_name == tool_name
      end or raise ArgumentError, "Tool: #{tool_name} not found in assistant.tools"

      # Call the callback if set
      tool_execution_callback.call(tool_call_id, tool_name, method_name, tool_arguments) if tool_execution_callback # rubocop:disable Style/SafeNavigation

      output = tool_instance.send(method_name, **tool_arguments)

      # Handle both ToolResponse and legacy return values
      if output.is_a?(ToolResponse)
        add_message(role: @llm_adapter.tool_role, content: output.content, image_url: output.image_url, tool_call_id: tool_call_id)
      else
        submit_tool_output(tool_call_id: tool_call_id, output: output)
      end
    end

    # Build a message
    #
    # @param role [String] The role of the message
    # @param content [String] The content of the message
    # @param image_url [String] The URL of the image to include in the message
    # @param tool_calls [Array<Hash>] The tool calls to include in the message
    # @param tool_call_id [String] The ID of the tool call to include in the message
    # @return [Langchain::Message] The Message object
    def build_message(role:, content: nil, image_url: nil, tool_calls: [], tool_call_id: nil)
      @llm_adapter.build_message(role: role, content: content, image_url: image_url, tool_calls: tool_calls, tool_call_id: tool_call_id)
    end

    # Increment the tokens count based on the last interaction with the LLM
    #
    # @param prompt_tokens [Integer] The number of used prmopt tokens
    # @param completion_tokens [Integer] The number of used completion tokens
    # @param total_tokens [Integer] The total number of used tokens
    # @return [Integer] The current total tokens count
    def record_used_tokens(prompt_tokens, completion_tokens, total_tokens_from_operation)
      @total_prompt_tokens += prompt_tokens if prompt_tokens
      @total_completion_tokens += completion_tokens if completion_tokens
      @total_tokens += total_tokens_from_operation if total_tokens_from_operation
    end

    def available_tool_names
      llm_adapter.available_tool_names(tools)
    end

    def validate_callback!(attr_name, callback)
      if !callback.nil? && !callback.respond_to?(:call)
        raise ArgumentError, "#{attr_name} must be a callable object, like Proc or lambda"
      end

      true
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/version.rb`:

```rb
# frozen_string_literal: true

module Langchain
  VERSION = "0.19.4"
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool_definition.rb`:

```rb
# frozen_string_literal: true

#
# Extends a class to be used as a tool in the assistant.
# A tool is a collection of functions (methods) used to perform specific tasks.
#
# == Usage
#
# 1. Extend your class with {Langchain::ToolDefinition}
# 2. Use {#define_function} to define each function of the tool
#
# == Key Concepts
#
# - {#define_function}: Defines a new function (method) for the tool
# - {ParameterBuilder#property}: Defines properties for the function parameters
# - {ParameterBuilder#item}: Alias for {ParameterBuilder#property}, used for array items
#
# These methods support various data types and nested structures, allowing for flexible and expressive tool definitions.
#
# @example Defining a tool with various property types and configurations
#   define_function :sample_function, description: "Demonstrates various property types and configurations" do
#     property :string_prop, type: "string", description: "A simple string property"
#     property :number_prop, type: "number", description: "A number property"
#     property :integer_prop, type: "integer", description: "An integer property"
#     property :boolean_prop, type: "boolean", description: "A boolean property"
#     property :enum_prop, type: "string", description: "An enum property", enum: ["option1", "option2", "option3"]
#     property :required_prop, type: "string", description: "A required property", required: true
#     property :array_prop, type: "array", description: "An array property" do
#       item type: "string", description: "Array item"
#     end
#     property :object_prop, type: "object", description: "An object property" do
#       property :nested_string, type: "string", description: "Nested string property"
#       property :nested_number, type: "number", description: "Nested number property"
#     end
#   end
#
module Langchain::ToolDefinition
  # Defines a function for the tool
  #
  # @param method_name [Symbol] Name of the method to define
  # @param description [String] Description of the function
  # @yield Block that defines the parameters for the function
  def define_function(method_name, description:, &block)
    function_schemas.add_function(method_name:, description:, &block)
  end

  # Returns the FunctionSchemas instance for this tool
  #
  # @return [FunctionSchemas] The FunctionSchemas instance
  def function_schemas
    @function_schemas ||= FunctionSchemas.new(tool_name)
  end

  # Returns the snake_case version of the class name as the tool's name
  #
  # @return [String] The snake_case version of the class name
  def tool_name
    @tool_name ||= name
      .gsub("::", "_")
      .gsub(/(?<=[A-Z])(?=[A-Z][a-z])|(?<=[a-z\d])(?=[A-Z])/, "_")
      .downcase
  end

  def self.extended(base)
    base.include(InstanceMethods)
  end

  module InstanceMethods
    # Create a tool response
    # @param content [String, nil] The content of the tool response
    # @param image_url [String, nil] The URL of an image
    # @return [Langchain::ToolResponse] The tool response
    def tool_response(content: nil, image_url: nil)
      Langchain::ToolResponse.new(content: content, image_url: image_url)
    end
  end

  # Manages schemas for functions
  class FunctionSchemas
    def initialize(tool_name)
      @schemas = {}
      @tool_name = tool_name
    end

    # Adds a function to the schemas
    #
    # @param method_name [Symbol] Name of the method to add
    # @param description [String] Description of the function
    # @yield Block that defines the parameters for the function
    # @raise [ArgumentError] If a block is defined and no parameters are specified for the function
    def add_function(method_name:, description:, &block)
      name = "#{@tool_name}__#{method_name}"

      if block_given? # rubocop:disable Performance/BlockGivenWithExplicitBlock
        parameters = ParameterBuilder.new(parent_type: "object").build(&block)

        if parameters[:properties].empty?
          raise ArgumentError, "Function parameters must have at least one property defined within it, if a block is provided"
        end
      end

      @schemas[method_name] = {
        type: "function",
        function: {name:, description:, parameters:}.compact
      }
    end

    # Converts schemas to OpenAI-compatible format
    #
    # @return [String] JSON string of schemas in OpenAI format
    def to_openai_format
      @schemas.values
    end

    # Converts schemas to Anthropic-compatible format
    #
    # @return [String] JSON string of schemas in Anthropic format
    def to_anthropic_format
      @schemas.values.map do |schema|
        # Adds a default input_schema if no parameters are present
        schema[:function][:parameters] ||= {
          type: "object",
          properties: {},
          required: []
        }

        schema[:function].transform_keys(parameters: :input_schema)
      end
    end

    # Converts schemas to Google Gemini-compatible format
    #
    # @return [String] JSON string of schemas in Google Gemini format
    def to_google_gemini_format
      @schemas.values.map { |schema| schema[:function] }
    end
  end

  # Builds parameter schemas for functions
  class ParameterBuilder
    VALID_TYPES = %w[object array string number integer boolean].freeze

    def initialize(parent_type:)
      @schema = (parent_type == "object") ? {type: "object", properties: {}, required: []} : {}
      @parent_type = parent_type
    end

    # Builds the parameter schema
    #
    # @yield Block that defines the properties of the schema
    # @return [Hash] The built schema
    def build(&block)
      instance_eval(&block)
      @schema
    end

    # Defines a property in the schema
    #
    # @param name [Symbol] Name of the property (required only for a parent of type object)
    # @param type [String] Type of the property
    # @param description [String] Description of the property
    # @param enum [Array] Array of allowed values
    # @param required [Boolean] Whether the property is required
    # @yield [Block] Block for nested properties (only for object and array types)
    # @raise [ArgumentError] If any parameter is invalid
    def property(name = nil, type:, description: nil, enum: nil, required: false, &block)
      validate_parameters(name:, type:, enum:, required:)

      prop = {type:, description:, enum:}.compact

      if block_given? # rubocop:disable Performance/BlockGivenWithExplicitBlock
        nested_schema = ParameterBuilder.new(parent_type: type).build(&block)

        case type
        when "object"
          if nested_schema[:properties].empty?
            raise ArgumentError, "Object properties must have at least one property defined within it"
          end
          prop = nested_schema
        when "array"
          if nested_schema.empty?
            raise ArgumentError, "Array properties must have at least one item defined within it"
          end
          prop[:items] = nested_schema
        end
      end

      if @parent_type == "object"
        @schema[:properties][name] = prop
        @schema[:required] << name.to_s if required
      else
        @schema = prop
      end
    end

    # Alias for property method, used for defining array items
    alias_method :item, :property

    private

    # Validates the parameters for a property
    #
    # @param name [Symbol] Name of the property
    # @param type [String] Type of the property
    # @param enum [Array] Array of allowed values
    # @param required [Boolean] Whether the property is required
    # @raise [ArgumentError] If any parameter is invalid
    def validate_parameters(name:, type:, enum:, required:)
      if @parent_type == "object"
        if name.nil?
          raise ArgumentError, "Name must be provided for properties of an object"
        end
        unless name.is_a?(Symbol)
          raise ArgumentError, "Invalid name '#{name}'. Name must be a symbol"
        end
      end

      unless VALID_TYPES.include?(type)
        raise ArgumentError, "Invalid type '#{type}'. Valid types are: #{VALID_TYPES.join(", ")}"
      end

      unless enum.nil? || enum.is_a?(Array)
        raise ArgumentError, "Invalid enum '#{enum}'. Enum must be nil or an array"
      end

      unless [true, false].include?(required)
        raise ArgumentError, "Invalid required '#{required}'. Required must be a boolean"
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/base.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Chunker
    # = Chunkers
    # Chunkers are used to split documents into smaller chunks before indexing into vector search databases.
    # Otherwise large documents, when retrieved and passed to LLMs, may hit the context window limits.
    #
    # == Available chunkers
    #
    # - {Langchain::Chunker::RecursiveText}
    # - {Langchain::Chunker::Text}
    # - {Langchain::Chunker::Semantic}
    # - {Langchain::Chunker::Sentence}
    class Base
      # @return [Array<Langchain::Chunk>]
      def chunks
        raise NotImplementedError
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/text.rb`:

```rb
# frozen_string_literal: true

require "baran"

module Langchain
  module Chunker
    # Simple text chunker
    #
    # Usage:
    #     Langchain::Chunker::Text.new(text).chunks
    class Text < Base
      attr_reader :text, :chunk_size, :chunk_overlap, :separator

      # @param [String] text
      # @param [Integer] chunk_size
      # @param [Integer] chunk_overlap
      # @param [String] separator
      def initialize(text, chunk_size: 1000, chunk_overlap: 200, separator: "\n\n")
        @text = text
        @chunk_size = chunk_size
        @chunk_overlap = chunk_overlap
        @separator = separator
      end

      # @return [Array<Langchain::Chunk>]
      def chunks
        splitter = Baran::CharacterTextSplitter.new(
          chunk_size: chunk_size,
          chunk_overlap: chunk_overlap,
          separator: separator
        )

        splitter.chunks(text).map do |chunk|
          Langchain::Chunk.new(text: chunk[:text])
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/recursive_text.rb`:

```rb
# frozen_string_literal: true

require "baran"

module Langchain
  module Chunker
    # Recursive text chunker. Preferentially splits on separators.
    #
    # Usage:
    #     Langchain::Chunker::RecursiveText.new(text).chunks
    class RecursiveText < Base
      attr_reader :text, :chunk_size, :chunk_overlap, :separators

      # @param [String] text
      # @param [Integer] chunk_size
      # @param [Integer] chunk_overlap
      # @param [Array<String>] separators
      def initialize(text, chunk_size: 1000, chunk_overlap: 200, separators: ["\n\n"])
        @text = text
        @chunk_size = chunk_size
        @chunk_overlap = chunk_overlap
        @separators = separators
      end

      # @return [Array<Langchain::Chunk>]
      def chunks
        splitter = Baran::RecursiveCharacterTextSplitter.new(
          chunk_size: chunk_size,
          chunk_overlap: chunk_overlap,
          separators: separators
        )

        splitter.chunks(text).map do |chunk|
          Langchain::Chunk.new(text: chunk[:text])
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/markdown.rb`:

```rb
# frozen_string_literal: true

require "baran"

module Langchain
  module Chunker
    # Simple text chunker
    #
    # Usage:
    #     Langchain::Chunker::Markdown.new(text).chunks
    class Markdown < Base
      attr_reader :text, :chunk_size, :chunk_overlap

      # @param [String] text
      # @param [Integer] chunk_size
      # @param [Integer] chunk_overlap
      # @param [String] separator
      def initialize(text, chunk_size: 1000, chunk_overlap: 200)
        @text = text
        @chunk_size = chunk_size
        @chunk_overlap = chunk_overlap
      end

      # @return [Array<Langchain::Chunk>]
      def chunks
        splitter = Baran::MarkdownSplitter.new(
          chunk_size: chunk_size,
          chunk_overlap: chunk_overlap
        )

        splitter.chunks(text).map do |chunk|
          Langchain::Chunk.new(text: chunk[:text])
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/sentence.rb`:

```rb
# frozen_string_literal: true

require "pragmatic_segmenter"

module Langchain
  module Chunker
    # This chunker splits text by sentences.
    #
    # Usage:
    #     Langchain::Chunker::Sentence.new(text).chunks
    class Sentence < Base
      attr_reader :text

      # @param text [String]
      # @return [Langchain::Chunker::Sentence]
      def initialize(text)
        @text = text
      end

      # @return [Array<Langchain::Chunk>]
      def chunks
        ps = PragmaticSegmenter::Segmenter.new(text: text)
        ps.segment.map do |chunk|
          Langchain::Chunk.new(text: chunk)
        end
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/prompts/semantic_prompt_template.yml`:

```yml
_type: prompt
input_variables:
  - text
template: | 
  Please split the following text by topics.
  Output only the paragraphs delimited by "---":

  {text}
```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/chunker/semantic.rb`:

```rb
# frozen_string_literal: true

module Langchain
  module Chunker
    # LLM-powered semantic chunker.
    # Semantic chunking is a technique of splitting texts by their semantic meaning, e.g.: themes, topics, and ideas.
    # We use an LLM to accomplish this. The Anthropic LLM is highly recommended for this task as it has the longest context window (100k tokens).
    #
    # Usage:
    #     Langchain::Chunker::Semantic.new(
    #       text,
    #       llm: Langchain::LLM::Anthropic.new(api_key: ENV["ANTHROPIC_API_KEY"])
    #     ).chunks
    class Semantic < Base
      attr_reader :text, :llm, :prompt_template
      # @param [Langchain::LLM::Base] Langchain::LLM::* instance
      # @param [Langchain::Prompt::PromptTemplate] Optional custom prompt template
      def initialize(text, llm:, prompt_template: nil)
        @text = text
        @llm = llm
        @prompt_template = prompt_template || default_prompt_template
      end

      # @return [Array<Langchain::Chunk>]
      def chunks
        prompt = prompt_template.format(text: text)

        # Replace static 50k limit with dynamic limit based on text length (max_tokens_to_sample)
        completion = llm.complete(prompt: prompt, max_tokens_to_sample: 50000).completion
        completion
          .gsub("Here are the paragraphs split by topic:\n\n", "")
          .split("---")
          .map(&:strip)
          .reject(&:empty?)
          .map do |chunk|
            Langchain::Chunk.new(text: chunk)
          end
      end

      private

      # @return [Langchain::Prompt::PromptTemplate] Default prompt template for semantic chunking
      def default_prompt_template
        Langchain::Prompt.load_from_path(
          file_path: Langchain.root.join("langchain/chunker/prompts/semantic_prompt_template.yml")
        )
      end
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/vectorsearch.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A tool wraps vectorsearch classes
  #
  # Usage:
  #    # Initialize the LLM that will be used to generate embeddings
  #    ollama = Langchain::LLM::Ollama.new(url: ENV["OLLAMA_URL"]
  #    chroma = Langchain::Vectorsearch::Chroma.new(url: ENV["CHROMA_URL"], index_name: "my_index", llm: ollama)
  #
  #    # This tool can now be used by the Assistant
  #    vectorsearch_tool = Langchain::Tool::Vectorsearch.new(vectorsearch: chroma)
  #
  class Vectorsearch
    extend Langchain::ToolDefinition

    define_function :similarity_search, description: "Vectorsearch: Retrieves relevant document for the query" do
      property :query, type: "string", description: "Query to find similar documents for", required: true
      property :k, type: "integer", description: "Number of similar documents to retrieve. Default value: 4"
    end

    attr_reader :vectorsearch

    # Initializes the Vectorsearch tool
    #
    # @param vectorsearch [Langchain::Vectorsearch::Base] Vectorsearch instance to use
    def initialize(vectorsearch:)
      @vectorsearch = vectorsearch
    end

    # Executes the vector search and returns the results
    #
    # @param query [String] The query to search for
    # @param k [Integer] The number of results to return
    # @return [Langchain::Tool::Response] The response from the server
    def similarity_search(query:, k: 4)
      result = vectorsearch.similarity_search(query:, k: 4)
      tool_response(content: result)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/news_retriever.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A tool that retrieves latest news from various sources via https://newsapi.org/.
  # An API key needs to be obtained from https://newsapi.org/ to use this tool.
  #
  # Usage:
  #    news_retriever = Langchain::Tool::NewsRetriever.new(api_key: ENV["NEWS_API_KEY"])
  #
  class NewsRetriever
    extend Langchain::ToolDefinition

    define_function :get_everything, description: "News Retriever: Search through millions of articles from over 150,000 large and small news sources and blogs" do
      property :q, type: "string", description: 'Keywords or phrases to search for in the article title and body. Surround phrases with quotes (") for exact match. Alternatively you can use the AND / OR / NOT keywords, and optionally group these with parenthesis. Must be URL-encoded'
      property :search_in, type: "string", description: "The fields to restrict your q search to", enum: ["title", "description", "content"]
      property :sources, type: "string", description: "A comma-separated string of identifiers (maximum 20) for the news sources or blogs you want headlines from. Use the /sources endpoint to locate these programmatically or look at the sources index"
      property :domains, type: "string", description: "A comma-separated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to restrict the search to"
      property :exclude_domains, type: "string", description: "A comma-separated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to remove from the results"
      property :from, type: "string", description: "A date and optional time for the oldest article allowed. This should be in ISO 8601 format"
      property :to, type: "string", description: "A date and optional time for the newest article allowed. This should be in ISO 8601 format"
      property :language, type: "string", description: "The 2-letter ISO-639-1 code of the language you want to get headlines for", enum: ["ar", "de", "en", "es", "fr", "he", "it", "nl", "no", "pt", "ru", "sv", "ud", "zh"]
      property :sort_by, type: "string", description: "The order to sort the articles in", enum: ["relevancy", "popularity", "publishedAt"]
      property :page_size, type: "integer", description: "The number of results to return per page (request). 5 is the default, 100 is the maximum"
      property :page, type: "integer", description: "Use this to page through the results if the total results found is greater than the page size"
    end

    define_function :get_top_headlines, description: "News Retriever: Provides live top and breaking headlines for a country, specific category in a country, single source, or multiple sources. You can also search with keywords. Articles are sorted by the earliest date published first" do
      property :country, type: "string", description: "The 2-letter ISO 3166-1 code of the country you want to get headlines for", enum: ["ae", "ar", "at", "au", "be", "bg", "br", "ca", "ch", "cn", "co", "cu", "cz", "de", "eg", "fr", "gb", "gr", "hk", "hu", "id", "ie", "il", "in", "it", "jp", "kr", "lt", "lv", "ma", "mx", "my", "ng", "nl", "no", "nz", "ph", "pl", "pt", "ro", "rs", "ru", "sa", "se", "sg", "si", "sk", "th", "tr", "tw", "ua", "us", "ve", "za"]
      property :category, type: "string", description: "The category you want to get headlines for", enum: ["business", "entertainment", "general", "health", "science", "sports", "technology"]
      property :q, type: "string", description: "Keywords or a phrase to search for"
      property :page_size, type: "integer", description: "The number of results to return per page (request). 5 is the default, 100 is the maximum"
      property :page, type: "integer", description: "Use this to page through the results if the total results found is greater than the page size"
    end

    define_function :get_sources, description: "News Retriever: This endpoint returns the subset of news publishers that top headlines (/v2/top-headlines) are available from. It's mainly a convenience endpoint that you can use to keep track of the publishers available on the API, and you can pipe it straight through to your users" do
      property :country, type: "string", description: "The 2-letter ISO 3166-1 code of the country you want to get headlines for. Default: all countries", enum: ["ae", "ar", "at", "au", "be", "bg", "br", "ca", "ch", "cn", "co", "cu", "cz", "de", "eg", "fr", "gb", "gr", "hk", "hu", "id", "ie", "il", "in", "it", "jp", "kr", "lt", "lv", "ma", "mx", "my", "ng", "nl", "no", "nz", "ph", "pl", "pt", "ro", "rs", "ru", "sa", "se", "sg", "si", "sk", "th", "tr", "tw", "ua", "us", "ve", "za"]
      property :category, type: "string", description: "The category you want to get headlines for. Default: all categories", enum: ["business", "entertainment", "general", "health", "science", "sports", "technology"]
      property :language, type: "string", description: "The 2-letter ISO-639-1 code of the language you want to get headlines for", enum: ["ar", "de", "en", "es", "fr", "he", "it", "nl", "no", "pt", "ru", "sv", "ud", "zh"]
    end

    def initialize(api_key: ENV["NEWS_API_KEY"])
      @api_key = api_key
    end

    # Retrieve all news
    #
    # @param q [String] Keywords or phrases to search for in the article title and body.
    # @param search_in [String] The fields to restrict your q search to. The possible options are: title, description, content.
    # @param sources [String] A comma-separated string of identifiers (maximum 20) for the news sources or blogs you want headlines from. Use the /sources endpoint to locate these programmatically or look at the sources index.
    # @param domains [String] A comma-separated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to restrict the search to.
    # @param exclude_domains [String] A comma-separated string of domains (eg bbc.co.uk, techcrunch.com, engadget.com) to remove from the results.
    # @param from [String] A date and optional time for the oldest article allowed. This should be in ISO 8601 format.
    # @param to [String] A date and optional time for the newest article allowed. This should be in ISO 8601 format.
    # @param language [String] The 2-letter ISO-639-1 code of the language you want to get headlines for. Possible options: ar, de, en, es, fr, he, it, nl, no, pt, ru, se, ud, zh.
    # @param sort_by [String] The order to sort the articles in. Possible options: relevancy, popularity, publishedAt.
    # @param page_size [Integer] The number of results to return per page. 20 is the API's default, 100 is the maximum. Our default is 5.
    # @param page [Integer] Use this to page through the results.
    #
    # @return [Langchain::Tool::Response] JSON response
    def get_everything(
      q: nil,
      search_in: nil,
      sources: nil,
      domains: nil,
      exclude_domains: nil,
      from: nil,
      to: nil,
      language: nil,
      sort_by: nil,
      page_size: 5, # The API default is 20 but that's too many.
      page: nil
    )
      Langchain.logger.debug("#{self.class} - Retrieving all news")

      params = {apiKey: @api_key}
      params[:q] = q if q
      params[:searchIn] = search_in if search_in
      params[:sources] = sources if sources
      params[:domains] = domains if domains
      params[:excludeDomains] = exclude_domains if exclude_domains
      params[:from] = from if from
      params[:to] = to if to
      params[:language] = language if language
      params[:sortBy] = sort_by if sort_by
      params[:pageSize] = page_size if page_size
      params[:page] = page if page

      response = send_request(path: "everything", params: params)
      tool_response(content: response)
    end

    # Retrieve top headlines
    #
    # @param country [String] The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae, ar, at, au, be, bg, br, ca, ch, cn, co, cu, cz, de, eg, fr, gb, gr, hk, hu, id, ie, il, in, it, jp, kr, lt, lv, ma, mx, my, ng, nl, no, nz, ph, pl, pt, ro, rs, ru, sa, se, sg, si, sk, th, tr, tw, ua, us, ve, za.
    # @param category [String] The category you want to get headlines for. Possible options: business, entertainment, general, health, science, sports, technology.
    # @param sources [String] A comma-separated string of identifiers for the news sources or blogs you want headlines from. Use the /sources endpoint to locate these programmatically.
    # @param q [String] Keywords or a phrase to search for.
    # @param page_size [Integer] The number of results to return per page. 20 is the API's default, 100 is the maximum. Our default is 5.
    # @param page [Integer] Use this to page through the results.
    #
    # @return [Langchain::Tool::Response] JSON response
    def get_top_headlines(
      country: nil,
      category: nil,
      sources: nil,
      q: nil,
      page_size: 5,
      page: nil
    )
      Langchain.logger.debug("#{self.class} - Retrieving top news headlines")

      params = {apiKey: @api_key}
      params[:country] = country if country
      params[:category] = category if category
      params[:sources] = sources if sources
      params[:q] = q if q
      params[:pageSize] = page_size if page_size
      params[:page] = page if page

      response = send_request(path: "top-headlines", params: params)
      tool_response(content: response)
    end

    # Retrieve news sources
    #
    # @param category [String] The category you want to get headlines for. Possible options: business, entertainment, general, health, science, sports, technology.
    # @param language [String] The 2-letter ISO-639-1 code of the language you want to get headlines for. Possible options: ar, de, en, es, fr, he, it, nl, no, pt, ru, se, ud, zh.
    # @param country [String] The 2-letter ISO 3166-1 code of the country you want to get headlines for. Possible options: ae, ar, at, au, be, bg, br, ca, ch, cn, co, cu, cz, de, eg, fr, gb, gr, hk, hu, id, ie, il, in, it, jp, kr, lt, lv, ma, mx, my, ng, nl, no, nz, ph, pl, pt, ro, rs, ru, sa, se, sg, si, sk, th, tr, tw, ua, us, ve, za.
    #
    # @return [Langchain::Tool::Response] JSON response
    def get_sources(
      category: nil,
      language: nil,
      country: nil
    )
      Langchain.logger.debug("#{self.class} - Retrieving news sources")

      params = {apiKey: @api_key}
      params[:country] = country if country
      params[:category] = category if category
      params[:language] = language if language

      response = send_request(path: "top-headlines/sources", params: params)
      tool_response(content: response)
    end

    private

    def send_request(path:, params:)
      uri = URI.parse("https://newsapi.org/v2/#{path}?#{URI.encode_www_form(params)}")
      http = Net::HTTP.new(uri.host, uri.port)
      http.use_ssl = true

      request = Net::HTTP::Get.new(uri.request_uri)
      request["Content-Type"] = "application/json"

      response = http.request(request)
      response
        .body
        # Remove non-UTF-8 characters
        .force_encoding(Encoding::UTF_8)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/wikipedia.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # Tool that adds the capability to search using the Wikipedia API
  #
  # Gem requirements:
  #     gem "wikipedia-client", "~> 1.17.0"
  #
  # Usage:
  #     wikipedia = Langchain::Tool::Wikipedia.new
  #     wikipedia.execute(input: "The Roman Empire")
  #
  class Wikipedia
    extend Langchain::ToolDefinition
    include Langchain::DependencyHelper

    define_function :execute, description: "Executes Wikipedia API search and returns the answer" do
      property :input, type: "string", description: "Search query", required: true
    end

    # Initializes the Wikipedia tool
    def initialize
      depends_on "wikipedia-client", req: "wikipedia"
    end

    # Executes Wikipedia API search and returns the answer
    #
    # @param input [String] search query
    # @return [Langchain::Tool::Response] Answer
    def execute(input:)
      Langchain.logger.debug("#{self.class} - Executing \"#{input}\"")

      page = ::Wikipedia.find(input)
      # It would be nice to figure out a way to provide page.content but the LLM token limit is an issue
      tool_response(content: page.summary)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/file_system.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A tool that wraps the Ruby file system classes.
  #
  # Usage:
  #    file_system = Langchain::Tool::FileSystem.new
  #
  class FileSystem
    extend Langchain::ToolDefinition

    define_function :list_directory, description: "File System Tool: Lists out the content of a specified directory" do
      property :directory_path, type: "string", description: "Directory path to list", required: true
    end

    define_function :read_file, description: "File System Tool: Reads the contents of a file" do
      property :file_path, type: "string", description: "Path to the file to read from", required: true
    end

    define_function :write_to_file, description: "File System Tool: Write content to a file" do
      property :file_path, type: "string", description: "Path to the file to write", required: true
      property :content, type: "string", description: "Content to write to the file", required: true
    end

    def list_directory(directory_path:)
      tool_response(content: Dir.entries(directory_path))
    rescue Errno::ENOENT
      tool_response(content: "No such directory: #{directory_path}")
    end

    def read_file(file_path:)
      tool_response(content: File.read(file_path))
    rescue Errno::ENOENT
      tool_response(content: "No such file: #{file_path}")
    end

    def write_to_file(file_path:, content:)
      File.write(file_path, content)
      tool_response(content: "File written successfully")
    rescue Errno::EACCES
      tool_response(content: "Permission denied: #{file_path}")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/calculator.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A calculator tool that falls back to the Google calculator widget
  #
  # Gem requirements:
  #     gem "eqn", "~> 1.6.5"
  #     gem "google_search_results", "~> 2.0.0"
  #
  # Usage:
  #     calculator = Langchain::Tool::Calculator.new
  #
  class Calculator
    extend Langchain::ToolDefinition
    include Langchain::DependencyHelper

    define_function :execute, description: "Evaluates a pure math expression or if equation contains non-math characters (e.g.: \"12F in Celsius\") then it uses the google search calculator to evaluate the expression" do
      property :input, type: "string", description: "Math expression", required: true
    end

    def initialize
      depends_on "eqn"
    end

    # Evaluates a pure math expression or if equation contains non-math characters (e.g.: "12F in Celsius") then it uses the google search calculator to evaluate the expression
    #
    # @param input [String] math expression
    # @return [Langchain::Tool::Response] Answer
    def execute(input:)
      Langchain.logger.debug("#{self.class} - Executing \"#{input}\"")

      result = Eqn::Calculator.calc(input)
      tool_response(content: result)
    rescue Eqn::ParseError, Eqn::NoVariableValueError
      tool_response(content: "\"#{input}\" is an invalid mathematical expression")
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/weather.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A weather tool that gets current weather data
  #
  # Current weather data is free for 1000 calls per day (https://home.openweathermap.org/api_keys)
  # Forecast and historical data require registration with credit card, so not supported yet.
  #
  # Usage:
  #     weather = Langchain::Tool::Weather.new(api_key: ENV["OPEN_WEATHER_API_KEY"])
  #     assistant = Langchain::Assistant.new(
  #       llm: llm,
  #       tools: [weather]
  #     )
  #
  class Weather
    extend Langchain::ToolDefinition

    define_function :get_current_weather, description: "Returns current weather for a city" do
      property :city,
        type: "string",
        description: "City name",
        required: true
      property :state_code,
        type: "string",
        description: "State code",
        required: true
      property :country_code,
        type: "string",
        description: "Country code",
        required: false
      property :units,
        type: "string",
        description: "Units for temperature (imperial or metric). Default: \"imperial\"",
        enum: ["imperial", "metric", "standard"],
        required: false
    end

    def initialize(api_key:)
      @api_key = api_key
    end

    def get_current_weather(city:, state_code:, country_code: nil, units: "imperial")
      validate_input(city: city, state_code: state_code, country_code: country_code, units: units)

      Langchain.logger.debug("#{self.class} - get_current_weather #{{city:, state_code:, country_code:, units:}}")

      fetch_current_weather(city: city, state_code: state_code, country_code: country_code, units: units)
    end

    private

    def fetch_current_weather(city:, state_code:, country_code:, units:)
      params = {appid: @api_key, q: [city, state_code, country_code].compact.join(","), units: units}

      location_response = send_request(path: "geo/1.0/direct", params: params.except(:units))
      return tool_response(content: location_response) if location_response.is_a?(String) # Error occurred

      location = location_response.first
      return tool_response(content: "Location not found") unless location

      params = params.merge(lat: location["lat"], lon: location["lon"]).except(:q)
      weather_data = send_request(path: "data/2.5/weather", params: params)

      tool_response(content: parse_weather_response(weather_data, units))
    end

    def send_request(path:, params:)
      uri = URI.parse("https://api.openweathermap.org/#{path}?#{URI.encode_www_form(params)}")
      http = Net::HTTP.new(uri.host, uri.port)
      http.use_ssl = true

      request = Net::HTTP::Get.new(uri.request_uri)
      request["Content-Type"] = "application/json"

      Langchain.logger.debug("#{self.class} - Sending request to OpenWeatherMap API #{{path: path, params: params.except(:appid)}}")
      response = http.request(request)
      Langchain.logger.debug("#{self.class} - Received response from OpenWeatherMap API #{{status: response.code}}")

      if response.code == "200"
        JSON.parse(response.body)
      else
        "API request failed: #{response.code} - #{response.message}"
      end
    end

    def validate_input(city:, state_code:, country_code:, units:)
      raise ArgumentError, "City name cannot be empty" if city.to_s.strip.empty?
      raise ArgumentError, "State code cannot be empty" if state_code.to_s.strip.empty?
      raise ArgumentError, "Invalid units. Use \"imperial\", \"standard\" or \"metric\"" unless ["imperial", "metric", "standard"].include?(units)
    end

    def parse_weather_response(response, units)
      temp_unit = case units
      when "standard" then "K"
      when "metric" then "¬∞C"
      when "imperial" then "¬∞F"
      end
      speed_unit = (units == "imperial") ? "mph" : "m/s"
      {
        temperature: "#{response["main"]["temp"]} #{temp_unit}",
        humidity: "#{response["main"]["humidity"]}%",
        description: response["weather"][0]["description"],
        wind_speed: "#{response["wind"]["speed"]} #{speed_unit}"
      }
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/google_search.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # Wrapper around SerpApi's Google Search API
  #
  # Gem requirements:
  #     gem "google_search_results", "~> 2.0.0"
  #
  # Usage:
  #     search = Langchain::Tool::GoogleSearch.new(api_key: "YOUR_API_KEY")
  #     search.execute(input: "What is the capital of France?")
  #
  class GoogleSearch
    extend Langchain::ToolDefinition
    include Langchain::DependencyHelper

    define_function :execute, description: "Executes Google Search and returns the result" do
      property :input, type: "string", description: "Search query", required: true
    end

    attr_reader :api_key

    #
    # Initializes the Google Search tool
    #
    # @param api_key [String] Search API key
    # @return [Langchain::Tool::GoogleSearch] Google search tool
    #
    def initialize(api_key:)
      depends_on "google_search_results"

      @api_key = api_key
    end

    # Executes Google Search and returns the result
    #
    # @param input [String] search query
    # @return [Langchain::Tool::Response] Answer
    def execute(input:)
      Langchain.logger.debug("#{self.class} - Executing \"#{input}\"")

      results = execute_search(input: input)

      answer_box = results[:answer_box_list] ? results[:answer_box_list].first : results[:answer_box]
      if answer_box
        return tool_response(content: answer_box[:result] ||
            answer_box[:answer] ||
            answer_box[:snippet] ||
            answer_box[:snippet_highlighted_words] ||
            answer_box.reject { |_k, v| v.is_a?(Hash) || v.is_a?(Array) || v.start_with?("http") })
      elsif (events_results = results[:events_results])
        return tool_response(content: events_results.take(10))
      elsif (sports_results = results[:sports_results])
        return tool_response(content: sports_results)
      elsif (top_stories = results[:top_stories])
        return tool_response(content: top_stories)
      elsif (news_results = results[:news_results])
        return tool_response(content: news_results)
      elsif (jobs_results = results.dig(:jobs_results, :jobs))
        return tool_response(content: jobs_results)
      elsif (shopping_results = results[:shopping_results]) && shopping_results.first.key?(:title)
        return tool_response(content: shopping_results.take(3))
      elsif (questions_and_answers = results[:questions_and_answers])
        return tool_response(content: questions_and_answers)
      elsif (popular_destinations = results.dig(:popular_destinations, :destinations))
        return tool_response(content: popular_destinations)
      elsif (top_sights = results.dig(:top_sights, :sights))
        return tool_response(content: top_sights)
      elsif (images_results = results[:images_results]) && images_results.first.key?(:thumbnail)
        return tool_response(content: images_results.map { |h| h[:thumbnail] }.take(10))
      end

      snippets = []
      if (knowledge_graph = results[:knowledge_graph])
        snippets << knowledge_graph[:description] if knowledge_graph[:description]

        title = knowledge_graph[:title] || ""
        knowledge_graph.each do |k, v|
          if v.is_a?(String) &&
              k != :title &&
              k != :description &&
              !k.to_s.end_with?("_stick") &&
              !k.to_s.end_with?("_link") &&
              !k.to_s.start_with?("http")
            snippets << "#{title} #{k}: #{v}"
          end
        end
      end

      if (first_organic_result = results.dig(:organic_results, 0))
        if (snippet = first_organic_result[:snippet])
          snippets << snippet
        elsif (snippet_highlighted_words = first_organic_result[:snippet_highlighted_words])
          snippets << snippet_highlighted_words
        elsif (rich_snippet = first_organic_result[:rich_snippet])
          snippets << rich_snippet
        elsif (rich_snippet_table = first_organic_result[:rich_snippet_table])
          snippets << rich_snippet_table
        elsif (link = first_organic_result[:link])
          snippets << link
        end
      end

      if (buying_guide = results[:buying_guide])
        snippets << buying_guide
      end

      if (local_results = results.dig(:local_results, :places))
        snippets << local_results
      end

      return tool_response(content: "No good search result found") if snippets.empty?
      tool_response(content: snippets)
    end

    #
    # Executes Google Search and returns hash_results JSON
    #
    # @param input [String] search query
    # @return [Hash] hash_results JSON
    #
    def execute_search(input:)
      ::GoogleSearch
        .new(q: input, serp_api_key: api_key)
        .get_hash
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/ruby_code_interpreter.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # A tool that execute Ruby code in a sandboxed environment.
  #
  # Gem requirements:
  #     gem "safe_ruby", "~> 1.0.4"
  #
  # Usage:
  #    interpreter = Langchain::Tool::RubyCodeInterpreter.new
  #
  class RubyCodeInterpreter
    extend Langchain::ToolDefinition
    include Langchain::DependencyHelper

    define_function :execute, description: "Executes Ruby code in a sandboxes environment" do
      property :input, type: "string", description: "Ruby code expression", required: true
    end

    def initialize(timeout: 30)
      depends_on "safe_ruby"

      @timeout = timeout
    end

    # Executes Ruby code in a sandboxes environment.
    #
    # @param input [String] ruby code expression
    # @return [Langchain::Tool::Response] Answer
    def execute(input:)
      Langchain.logger.debug("#{self.class} - Executing \"#{input}\"")

      tool_response(content: safe_eval(input))
    end

    def safe_eval(code)
      SafeRuby.eval(code, timeout: @timeout)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/database.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # Connects to a SQL database, executes SQL queries, and outputs DB schema for Agents to use
  #
  # Gem requirements:
  #     gem "sequel", "~> 5.87.0"
  #
  # Usage:
  #     database = Langchain::Tool::Database.new(connection_string: "postgres://user:password@localhost:5432/db_name")
  #
  class Database
    extend Langchain::ToolDefinition
    include Langchain::DependencyHelper

    define_function :list_tables, description: "Database Tool: Returns a list of tables in the database"

    define_function :describe_tables, description: "Database Tool: Returns the schema for a list of tables" do
      property :tables, type: "array", description: "The tables to describe", required: true do
        item type: "string"
      end
    end

    define_function :dump_schema, description: "Database Tool: Returns the database schema"

    define_function :execute, description: "Database Tool: Executes a SQL query and returns the results" do
      property :input, type: "string", description: "SQL query to be executed", required: true
    end

    attr_reader :db, :requested_tables, :excluded_tables

    # Establish a database connection
    #
    # @param connection_string [String] Database connection info, e.g. 'postgres://user:password@localhost:5432/db_name'
    # @param tables [Array<Symbol>] The tables to use. Will use all if empty.
    # @param except_tables [Array<Symbol>] The tables to exclude. Will exclude none if empty.
    # @return [Database] Database object
    def initialize(connection_string:, tables: [], exclude_tables: [])
      depends_on "sequel"

      raise StandardError, "connection_string parameter cannot be blank" if connection_string.empty?

      @db = Sequel.connect(connection_string)
      # TODO: This is a bug, these 2 parameters are completely ignored.
      @requested_tables = tables
      @excluded_tables = exclude_tables
    end

    # Database Tool: Returns a list of tables in the database
    #
    # @return [Langchain::Tool::Response] List of tables in the database
    def list_tables
      tool_response(content: db.tables)
    end

    # Database Tool: Returns the schema for a list of tables
    #
    # @param tables [Array<String>] The tables to describe.
    # @return [Langchain::Tool::Response] The schema for the tables
    def describe_tables(tables: [])
      return "No tables specified" if tables.empty?

      Langchain.logger.debug("#{self.class} - Describing tables: #{tables}")

      result = tables
        .map do |table|
          describe_table(table)
        end
        .join("\n")

      tool_response(content: result)
    end

    # Database Tool: Returns the database schema
    #
    # @return [Langchain::Tool::Response] Database schema
    def dump_schema
      Langchain.logger.debug("#{self.class} - Dumping schema tables and keys")

      schemas = db.tables.map do |table|
        describe_table(table)
      end

      tool_response(content: schemas.join("\n"))
    end

    # Database Tool: Executes a SQL query and returns the results
    #
    # @param input [String] SQL query to be executed
    # @return [Langchain::Tool::Response] Results from the SQL query
    def execute(input:)
      Langchain.logger.debug("#{self.class} - Executing \"#{input}\"")

      tool_response(content: db[input].to_a)
    rescue Sequel::DatabaseError => e
      Langchain.logger.error("#{self.class} - #{e.message}")
      tool_response(content: e.message)
    end

    private

    # Describes a table and its schema
    #
    # @param table [String] The table to describe
    # @return [Langchain::Tool::Response] The schema for the table
    def describe_table(table)
      # TODO: There's probably a clear way to do all of this below

      primary_key_columns = []
      primary_key_column_count = db.schema(table).count { |column| column[1][:primary_key] == true }

      schema = "CREATE TABLE #{table}(\n"
      db.schema(table).each do |column|
        schema << "#{column[0]} #{column[1][:type]}"
        if column[1][:primary_key] == true
          schema << " PRIMARY KEY" if primary_key_column_count == 1
        else
          primary_key_columns << column[0]
        end
        schema << " COMMENT '#{column[1][:comment]}'" if column[1][:comment]
        schema << ",\n" unless column == db.schema(table).last && primary_key_column_count == 1
      end
      if primary_key_column_count > 1
        schema << "PRIMARY KEY (#{primary_key_columns.join(",")})"
      end
      db.foreign_key_list(table).each do |fk|
        schema << ",\n" if fk == db.foreign_key_list(table).first
        schema << "FOREIGN KEY (#{fk[:columns]&.first}) REFERENCES #{fk[:table]}(#{fk[:key]&.first})"
        schema << ",\n" unless fk == db.foreign_key_list(table).last
      end
      schema << ");\n"

      tool_response(content: schema)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchain/tool/tavily.rb`:

```rb
# frozen_string_literal: true

module Langchain::Tool
  #
  # Tavily Search is a robust search API tailored specifically for LLM Agents.
  # It seamlessly integrates with diverse data sources to ensure a superior, relevant search experience.
  #
  # Usage:
  #    tavily = Langchain::Tool::Tavily.new(api_key: ENV["TAVILY_API_KEY"])
  #
  class Tavily
    extend Langchain::ToolDefinition

    define_function :search, description: "Tavily Tool: Robust search API" do
      property :query, type: "string", description: "The search query string", required: true
      property :search_depth, type: "string", description: "The depth of the search: basic for quick results and advanced for indepth high quality results but longer response time", enum: ["basic", "advanced"]
      property :include_images, type: "boolean", description: "Include a list of query related images in the response"
      property :include_answer, type: "boolean", description: "Include answers in the search results"
      property :include_raw_content, type: "boolean", description: "Include raw content in the search results"
      property :max_results, type: "integer", description: "The number of maximum search results to return"
      property :include_domains, type: "array", description: "A list of domains to specifically include in the search results" do
        item type: "string"
      end
      property :exclude_domains, type: "array", description: "A list of domains to specifically exclude from the search results" do
        item type: "string"
      end
    end

    def initialize(api_key:)
      @api_key = api_key
    end

    # Search for data based on a query.
    #
    # @param query [String] The search query string.
    # @param search_depth [String] The depth of the search. It can be basic or advanced. Default is basic for quick results and advanced for indepth high quality results but longer response time. Advanced calls equals 2 requests.
    # @param include_images [Boolean] Include a list of query related images in the response. Default is False.
    # @param include_answer [Boolean] Include answers in the search results. Default is False.
    # @param include_raw_content [Boolean] Include raw content in the search results. Default is False.
    # @param max_results [Integer] The number of maximum search results to return. Default is 5.
    # @param include_domains [Array<String>] A list of domains to specifically include in the search results. Default is None, which includes all domains.
    # @param exclude_domains [Array<String>] A list of domains to specifically exclude from the search results. Default is None, which doesn't exclude any domains.
    #
    # @return [Langchain::Tool::Response] The search results in JSON format.
    def search(
      query:,
      search_depth: "basic",
      include_images: false,
      include_answer: false,
      include_raw_content: false,
      max_results: 5,
      include_domains: [],
      exclude_domains: []
    )
      uri = URI("https://api.tavily.com/search")
      request = Net::HTTP::Post.new(uri)
      request.content_type = "application/json"
      request.body = {
        api_key: @api_key,
        query: query,
        search_depth: search_depth,
        include_images: include_images,
        include_answer: include_answer,
        include_raw_content: include_raw_content,
        max_results: max_results,
        include_domains: include_domains,
        exclude_domains: exclude_domains
      }.to_json

      response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: uri.scheme == "https") do |http|
        http.request(request)
      end
      tool_response(content: response.body)
    end
  end
end

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/lib/langchainrb.rb`:

```rb
require "langchain"

```

`/Users/estiens/.asdf/installs/ruby/3.4.1/lib/ruby/gems/3.4.0/gems/langchainrb-0.19.4/LICENSE.txt`:

```txt
The MIT License (MIT)

Copyright (c) 2023 Andrei Bondarev

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

```