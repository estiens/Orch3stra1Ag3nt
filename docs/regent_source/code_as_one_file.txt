Project Path: regent

Source Tree:

```
regent
├── CODE_OF_CONDUCT.md
├── LICENSE
├── bin
│   ├── setup
│   └── console
├── CHANGELOG.md
├── sig
│   └── regent.rbs
├── spec
│   ├── spec_helper.rb
│   ├── regent_spec.rb
│   ├── fixtures
│   │   └── cassettes
│   │       ├── LLM
│   │       │   ├── Google_Gemini
│   │       │   │   ├── success_response.yml
│   │       │   │   └── non_existent_model.yml
│   │       │   ├── Anthropic
│   │       │   │   ├── success_response.yml
│   │       │   │   └── non_existent_model.yml
│   │       │   ├── Ollama
│   │       │   │   ├── success_response.yml
│   │       │   │   └── non_existent_model.yml
│   │       │   └── OpenAI
│   │       │       ├── success_response.yml
│   │       │       ├── compatible_success_response.yml
│   │       │       └── non_existent_model.yml
│   │       └── Regent_Agent
│   │           ├── Google_Gemini
│   │           │   ├── answers_a_basic_question.yml
│   │           │   └── answers_a_question_with_a_tool.yml
│   │           ├── Anthropic
│   │           │   ├── answers_a_basic_question.yml
│   │           │   └── answers_a_question_with_a_tool.yml
│   │           ├── function_tools
│   │           │   └── answers_a_question_with_a_tool.yml
│   │           └── OpenAI
│   │               ├── answers_a_basic_question.yml
│   │               └── answers_a_question_with_a_tool.yml
│   └── regent
│       ├── agent_spec.rb
│       └── llm_spec.rb
├── README.md
├── Rakefile
├── lib
│   ├── regent.rb
│   └── regent
│       ├── tool.rb
│       ├── span.rb
│       ├── llm
│       │   ├── base.rb
│       │   ├── ollama.rb
│       │   ├── open_ai.rb
│       │   ├── anthropic.rb
│       │   ├── gemini.rb
│       │   └── open_router.rb
│       ├── agent.rb
│       ├── toolchain.rb
│       ├── llm.rb
│       ├── session.rb
│       ├── logger.rb
│       ├── version.rb
│       ├── engine
│       │   ├── base.rb
│       │   ├── react
│       │   │   └── prompt_template.rb
│       │   └── react.rb
│       └── concerns
│           ├── identifiable.rb
│           ├── dependable.rb
│           ├── durationable.rb
│           └── toolable.rb
├── Gemfile
├── Gemfile.lock
└── regent.gemspec

```

`/Users/estiens/code/ai/regent/CODE_OF_CONDUCT.md`:

```md
# Contributor Covenant Code of Conduct

## Our Pledge

We as members, contributors, and leaders pledge to make participation in our
community a harassment-free experience for everyone, regardless of age, body
size, visible or invisible disability, ethnicity, sex characteristics, gender
identity and expression, level of experience, education, socio-economic status,
nationality, personal appearance, race, caste, color, religion, or sexual
identity and orientation.

We pledge to act and interact in ways that contribute to an open, welcoming,
diverse, inclusive, and healthy community.

## Our Standards

Examples of behavior that contributes to a positive environment for our
community include:

* Demonstrating empathy and kindness toward other people
* Being respectful of differing opinions, viewpoints, and experiences
* Giving and gracefully accepting constructive feedback
* Accepting responsibility and apologizing to those affected by our mistakes,
  and learning from the experience
* Focusing on what is best not just for us as individuals, but for the overall
  community

Examples of unacceptable behavior include:

* The use of sexualized language or imagery, and sexual attention or advances of
  any kind
* Trolling, insulting or derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or email address,
  without their explicit permission
* Other conduct which could reasonably be considered inappropriate in a
  professional setting

## Enforcement Responsibilities

Community leaders are responsible for clarifying and enforcing our standards of
acceptable behavior and will take appropriate and fair corrective action in
response to any behavior that they deem inappropriate, threatening, offensive,
or harmful.

Community leaders have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, and will communicate reasons for moderation
decisions when appropriate.

## Scope

This Code of Conduct applies within all community spaces, and also applies when
an individual is officially representing the community in public spaces.
Examples of representing our community include using an official email address,
posting via an official social media account, or acting as an appointed
representative at an online or offline event.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported to the community leaders responsible for enforcement at
[INSERT CONTACT METHOD].
All complaints will be reviewed and investigated promptly and fairly.

All community leaders are obligated to respect the privacy and security of the
reporter of any incident.

## Enforcement Guidelines

Community leaders will follow these Community Impact Guidelines in determining
the consequences for any action they deem in violation of this Code of Conduct:

### 1. Correction

**Community Impact**: Use of inappropriate language or other behavior deemed
unprofessional or unwelcome in the community.

**Consequence**: A private, written warning from community leaders, providing
clarity around the nature of the violation and an explanation of why the
behavior was inappropriate. A public apology may be requested.

### 2. Warning

**Community Impact**: A violation through a single incident or series of
actions.

**Consequence**: A warning with consequences for continued behavior. No
interaction with the people involved, including unsolicited interaction with
those enforcing the Code of Conduct, for a specified period of time. This
includes avoiding interactions in community spaces as well as external channels
like social media. Violating these terms may lead to a temporary or permanent
ban.

### 3. Temporary Ban

**Community Impact**: A serious violation of community standards, including
sustained inappropriate behavior.

**Consequence**: A temporary ban from any sort of interaction or public
communication with the community for a specified period of time. No public or
private interaction with the people involved, including unsolicited interaction
with those enforcing the Code of Conduct, is allowed during this period.
Violating these terms may lead to a permanent ban.

### 4. Permanent Ban

**Community Impact**: Demonstrating a pattern of violation of community
standards, including sustained inappropriate behavior, harassment of an
individual, or aggression toward or disparagement of classes of individuals.

**Consequence**: A permanent ban from any sort of public interaction within the
community.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage],
version 2.1, available at
[https://www.contributor-covenant.org/version/2/1/code_of_conduct.html][v2.1].

Community Impact Guidelines were inspired by
[Mozilla's code of conduct enforcement ladder][Mozilla CoC].

For answers to common questions about this code of conduct, see the FAQ at
[https://www.contributor-covenant.org/faq][FAQ]. Translations are available at
[https://www.contributor-covenant.org/translations][translations].

[homepage]: https://www.contributor-covenant.org
[v2.1]: https://www.contributor-covenant.org/version/2/1/code_of_conduct.html
[Mozilla CoC]: https://github.com/mozilla/diversity
[FAQ]: https://www.contributor-covenant.org/faq
[translations]: https://www.contributor-covenant.org/translations

```

`/Users/estiens/code/ai/regent/LICENSE`:

```
MIT License

Copyright (c) 2024 Alex Chaplinsky

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`/Users/estiens/code/ai/regent/bin/setup`:

```
#!/usr/bin/env bash
set -euo pipefail
IFS=$'\n\t'
set -vx

bundle install

# Do any other automated setup that you need to do here

```

`/Users/estiens/code/ai/regent/bin/console`:

```
#!/usr/bin/env ruby
# frozen_string_literal: true

require "bundler/setup"
require "regent"

# You can add fixtures and/or initialization code here to make experimenting
# with your gem easier. You can also use a different console, if you like.

require "irb"
IRB.start(__FILE__)

```

`/Users/estiens/code/ai/regent/CHANGELOG.md`:

```md
## [Unreleased]

## [0.1.0] - 2024-11-18

- Initial release

```

`/Users/estiens/code/ai/regent/sig/regent.rbs`:

```rbs
module Regent
  VERSION: String
  # See the writing guide of rbs: https://github.com/ruby/rbs#guides
end

```

`/Users/estiens/code/ai/regent/spec/spec_helper.rb`:

```rb
# frozen_string_literal: true

require "regent"
require 'vcr'

VCR.configure do |config|
  config.cassette_library_dir = "spec/fixtures/cassettes"
  config.hook_into :webmock
  config.configure_rspec_metadata!

  # Filter out sensitive data like API keys
  config.filter_sensitive_data('<OPENAI_API_KEY>') { ENV['OPENAI_API_KEY'] }
  config.filter_sensitive_data('<ANTHROPIC_API_KEY>') { ENV['ANTHROPIC_API_KEY'] }
  config.filter_sensitive_data('<GEMINI_API_KEY>') { ENV['GEMINI_API_KEY'] }
end

RSpec.configure do |config|
  # Enable flags like --only-failures and --next-failure
  config.example_status_persistence_file_path = ".rspec_status"

  # Disable RSpec exposing methods globally on `Module` and `main`
  config.disable_monkey_patching!

  config.expect_with :rspec do |c|
    c.syntax = :expect
  end

  config.around(:each, :vcr) do |example|
    VCR.use_cassette(cassette, record: :new_episodes) { example.call }
  end
end

```

`/Users/estiens/code/ai/regent/spec/regent_spec.rb`:

```rb
# frozen_string_literal: true

RSpec.describe Regent do
  it "has a version number" do
    expect(Regent::VERSION).not_to be nil
  end
end

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Google_Gemini/success_response.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"What is the capital of
        Japan?"}]}],"generation_config":{"temperature":0.0,"stop_sequences":[]}}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Mon, 20 Jan 2025 11:08:06 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '239'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=805
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Tokyo\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.08858470618724823
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 8,
            "candidatesTokenCount": 2,
            "totalTokenCount": 10
          },
          "modelVersion": "gemini-1.5-flash"
        }
  recorded_at: Mon, 20 Jan 2025 11:08:06 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Google_Gemini/non_existent_model.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-3.5-flash:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"What is the capital of
        Japan?"}]}]}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 404
      message: ''
    headers:
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Type:
      - application/json; charset=UTF-8
      Content-Encoding:
      - gzip
      Date:
      - Wed, 01 Jan 2025 22:49:29 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '208'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=742
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "error": {
            "code": 404,
            "message": "models/gemini-3.5-flash is not found for API version v1, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.",
            "status": "NOT_FOUND"
          }
        }
  recorded_at: Wed, 01 Jan 2025 22:49:30 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Anthropic/success_response.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"model":"claude-3-5-sonnet-20240620","temperature":0.0,"stop_sequences":[],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Mon, 20 Jan 2025 11:10:50 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2025-01-20T11:10:50Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2025-01-20T11:10:50Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2025-01-20T11:10:50Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2025-01-20T11:10:50Z'
      Request-Id:
      - req_01F5dU3gDfQMydbcr4bUSqks
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 904e9ec19a4fee3c-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01ThdWE25qLykmyoS1NPAMMm","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"The
        capital of Japan is Tokyo. Tokyo has been the capital of Japan since 1868,
        when it replaced Kyoto as the seat of the Emperor and the national government.
        It is the most populous metropolitan area in the world and serves as Japan''s
        political, economic, and cultural center. Tokyo is known for its unique blend
        of modern technology and traditional culture, bustling urban areas, and efficient
        transportation systems."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":14,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":86}}'
  recorded_at: Mon, 20 Jan 2025 11:10:50 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Anthropic/non_existent_model.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"system":null,"model":"claude-4.1-haiku","stop_sequences":null,"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 400
      message: Bad Request
    headers:
      Date:
      - Wed, 01 Jan 2025 22:52:14 GMT
      Content-Type:
      - application/json
      Content-Length:
      - '106'
      Connection:
      - keep-alive
      X-Should-Retry:
      - 'false'
      Request-Id:
      - req_01BNZVpVFg1ZTuFWcPpErUjq
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8fb6142069413bc6-WAW
    body:
      encoding: UTF-8
      string: '{"type":"error","error":{"type":"invalid_request_error","message":"system:
        Input should be a valid list"}}'
  recorded_at: Wed, 01 Jan 2025 22:52:14 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Ollama/success_response.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: http://localhost:11434/api/chat
    body:
      encoding: UTF-8
      string: '{"model":"gemma","messages":[{"role":"user","content":"What is the
        capital of Japan?"}],"stream":false}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
  response:
    status:
      code: 200
      message: OK
    headers:
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Mon, 20 Jan 2025 10:53:23 GMT
      Content-Length:
      - '451'
    body:
      encoding: UTF-8
      string: '{"model":"gemma","created_at":"2025-01-20T10:53:23.717649Z","message":{"role":"assistant","content":"The
        capital city of Japan is Tokyo.\n\nIt is the political, economic, and cultural
        center of Japan and is known for its modern cityscape and traditional culture."},"done_reason":"stop","done":true,"total_duration":2949346250,"load_duration":613586833,"prompt_eval_count":29,"prompt_eval_duration":1952000000,"eval_count":33,"eval_duration":382000000}'
  recorded_at: Mon, 20 Jan 2025 10:53:23 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/Ollama/non_existent_model.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: http://localhost:11434/api/chat
    body:
      encoding: UTF-8
      string: '{"model":"llama3.1","messages":[{"role":"user","content":"What is the
        capital of Japan?"}],"stream":false}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
  response:
    status:
      code: 404
      message: Not Found
    headers:
      Content-Type:
      - application/json; charset=utf-8
      Date:
      - Mon, 20 Jan 2025 10:00:14 GMT
      Content-Length:
      - '62'
    body:
      encoding: UTF-8
      string: '{"error":"model \"llama3.1\" not found, try pulling it first"}'
  recorded_at: Mon, 20 Jan 2025 10:00:14 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/OpenAI/success_response.yml`:

```yml
---
http_interactions:
  - request:
      method: post
      uri: https://api.openai.com/v1/chat/completions
      body:
        encoding: UTF-8
        string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"model":"gpt-4o-mini","temperature":0.0,"stop":[]}'
      headers:
        Content-Type:
          - application/json
        Authorization:
          - Bearer <OPENAI_API_KEY>
        Accept-Encoding:
          - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
        Accept:
          - "*/*"
        User-Agent:
          - Ruby
    response:
      status:
        code: 200
        message: OK
      headers:
        Date:
          - Mon, 20 Jan 2025 11:06:05 GMT
        Content-Type:
          - application/json
        Transfer-Encoding:
          - chunked
        Connection:
          - keep-alive
        Access-Control-Expose-Headers:
          - X-Request-ID
        Openai-Organization:
          - favia-czx6t5
        Openai-Processing-Ms:
          - "440"
        Openai-Version:
          - "2020-10-01"
        X-Ratelimit-Limit-Requests:
          - "5000"
        X-Ratelimit-Limit-Tokens:
          - "4000000"
        X-Ratelimit-Remaining-Requests:
          - "4999"
        X-Ratelimit-Remaining-Tokens:
          - "3999975"
        X-Ratelimit-Reset-Requests:
          - 12ms
        X-Ratelimit-Reset-Tokens:
          - 0s
        X-Request-Id:
          - req_f0c36092826a884e2cf084aea1e27e80
        Strict-Transport-Security:
          - max-age=31536000; includeSubDomains; preload
        Cf-Cache-Status:
          - DYNAMIC
        Set-Cookie:
          - __cf_bm=25msmnb59eG3FG8h2RzlQXJ4vvDu0MVV76DjE3ys5WU-1737371165-1.0.1.1-6ODQly3nUgYaBAJ.rMxxjewmRv1bMetQuqH_.ppcm8NquyHcGpX3HSnYDbzomPZsrZYb0vx0AYh7nl_JJ6kyRg;
            path=/; expires=Mon, 20-Jan-25 11:36:05 GMT; domain=.api.openai.com; HttpOnly;
            Secure; SameSite=None
          - _cfuvid=ZHthzA1KgMF1j2LJNffn6Z3vW1k1epZR3MtNsiqxd9I-1737371165579-0.0.1.1-604800000;
            path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
        X-Content-Type-Options:
          - nosniff
        Server:
          - cloudflare
        Cf-Ray:
          - 904e97d0dc67bf1e-WAW
      body:
        encoding: ASCII-8BIT
        string: |
          {
            "id": "chatcmpl-ArjtVx2HaWnsxQWjGTWoUQ1gXtpUV",
            "object": "chat.completion",
            "created": 1737371165,
            "model": "gpt-4o-mini-2024-07-18",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "The capital of Japan is Tokyo.",
                  "refusal": null
                },
                "logprobs": null,
                "finish_reason": "stop"
              }
            ],
            "usage": {
              "prompt_tokens": 14,
              "completion_tokens": 7,
              "total_tokens": 21,
              "prompt_tokens_details": {
                "cached_tokens": 0,
                "audio_tokens": 0
              },
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "audio_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "service_tier": "default",
            "system_fingerprint": "fp_bd83329f63"
          }
    recorded_at: Mon, 20 Jan 2025 11:06:05 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/OpenAI/compatible_success_response.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.groq.com/openai/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"model":"qwen-qwq-32b","temperature":0.0,"stop":[]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Tue, 25 Mar 2025 09:16:54 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Cache-Control:
      - private, max-age=0, no-store, no-cache, must-revalidate
      Vary:
      - Origin
      X-Groq-Region:
      - gcp-europe-west3
      X-Ratelimit-Limit-Requests:
      - '1000'
      X-Ratelimit-Limit-Tokens:
      - '6000'
      X-Ratelimit-Remaining-Requests:
      - '998'
      X-Ratelimit-Remaining-Tokens:
      - '5988'
      X-Ratelimit-Reset-Requests:
      - 1m59.84s
      X-Ratelimit-Reset-Tokens:
      - 120ms
      X-Request-Id:
      - req_01jq69ea58fzjrr5amfemf2320
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=cps3k6oXesY06v9ixDB97dvH07LTeB22d08yiDXysuI-1742894214-1.0.1.1-G_NQieIxZvqWXBRzklmz5XnghKNc5IE_aWxAag4Sdlm7jH3jOR2evnsGH94SC1Y_3kINJYg5MwroDvkXvPQ4NwS837_Y9WLdT6Qa462IRjo;
        path=/; expires=Tue, 25-Mar-25 09:46:54 GMT; domain=.groq.com; HttpOnly; Secure;
        SameSite=None
      Server:
      - cloudflare
      Cf-Ray:
      - 925d4fe0cc77bb0e-ZRH
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: '{"id":"chatcmpl-79b84452-f7c9-4714-b559-6c4e2ec5759e","object":"chat.completion","created":1742894213,"model":"qwen-qwq-32b","choices":[{"index":0,"message":{"role":"assistant","content":"The capital of Japan is Tokyo."},"logprobs":null,"finish_reason":"stop"}],"usage":{"queue_time":0.10954180300000001,"prompt_tokens":17,"prompt_time":0.003309235,"completion_tokens":270,"completion_time":0.655944184,"total_tokens":287,"total_time":0.659253419},"system_fingerprint":"fp_27d5db8d87","x_groq":{"id":"req_01jq69ea58fzjrr5amfemf2325"}}

        '
  recorded_at: Tue, 25 Mar 2025 09:16:54 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/LLM/OpenAI/non_existent_model.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"model":"gpt-4.1o-mini","temperature":0.0,"stop":null}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 404
      message: Not Found
    headers:
      Date:
      - Wed, 01 Jan 2025 22:43:05 GMT
      Content-Type:
      - application/json; charset=utf-8
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Vary:
      - Origin
      X-Request-Id:
      - req_0771c41da640cfe3b0a128ba32606627
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=Mn7zpswiYcoQNyTawRH2PIlgNgX7SVp3K_lyCparwMA-1735771385-1.0.1.1-r6kttiCiP6Tv6ss.1DhxDIx345hQeInSOstsdFbBscadaG2iXJjmgm89a3X4dPMoe65JxhdFwxeQmoKotXV4Zw;
        path=/; expires=Wed, 01-Jan-25 23:13:05 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=ihUOSSEN7_GnteVTob0tDtBuaY0F.neYoIge4gtB6Ak-1735771385502-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8fb606b5cd533534-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
            "error": {
                "message": "The model `gpt-4.1o-mini` does not exist or you do not have access to it.",
                "type": "invalid_request_error",
                "param": null,
                "code": "model_not_found"
            }
        }
  recorded_at: Wed, 01 Jan 2025 22:43:05 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/Google_Gemini/answers_a_basic_question.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro-002:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"## Instructions\nConsider
        the following context: You are an AI agent that can answer basic questions
        from own knowledge\n\n\nYou are an AI agent reasoning step-by-step to solve
        complex problems.\nYour reasoning process happens in a loop of Thought, Action,
        Observation.\nThought - a description of your thoughts about the question.\nAction
        - pick a an action from available tools if required. If there are no tools
        that can help return an Answer saying you are not able to help.\nObservation
        - is the result of running a tool.\nPAUSE - is always present after an Action.\n\n##
        Available tools:\n\n\n## Example session\nQuestion: What is the weather in
        London today?\nThought: I need to get current weather in London\nAction: weather_tool
        | London\nPAUSE\n\nYou will have a response form a user with Observation:\nObservation:
        It is 32 degress and Sunny\n\n... (this Thought/Action/Observation can repeat
        N times)\n\nThought: I know the final answer\nAnswer: It is 32 degress and
        Sunny in London\n"}]},{"role":"user","parts":[{"text":"What is the capital
        of Japan?"}]}]}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Sun, 29 Dec 2024 10:45:31 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '278'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=1412
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Thought: I know the capital of Japan.\nAnswer: Tokyo\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.02762071362563542
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 229,
            "candidatesTokenCount": 14,
            "totalTokenCount": 243
          },
          "modelVersion": "gemini-1.5-pro-002"
        }
  recorded_at: Sun, 29 Dec 2024 10:45:31 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/Google_Gemini/answers_a_question_with_a_tool.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro-002:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"}]},{"role":"user","parts":[{"text":"What
        is the price of Bitcoin?"}]}],"generation_config":{"temperature":0.0,"stop_sequences":["PAUSE"]}}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Sat, 04 Jan 2025 17:59:10 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '313'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=1767
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Thought: I need to get the current price of Bitcoin.\nAction: {\"tool\": \"price_tool\", \"args\": [\"Bitcoin\"]}\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.00097042558093865712
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 246,
            "candidatesTokenCount": 30,
            "totalTokenCount": 276
          },
          "modelVersion": "gemini-1.5-pro-002"
        }
  recorded_at: Sat, 04 Jan 2025 17:59:10 GMT
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro-002:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"}]},{"role":"user","parts":[{"text":"What
        is the price of Bitcoin?"}]},{"role":"assistant","parts":[{"text":"Thought:
        I need to get the current price of Bitcoin.\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Bitcoin\"]}"}]},{"role":"user","parts":[{"text":"Observation:
        {''BTC'': ''$107,000'', ''ETH'': ''$6,000''}"}]}],"generation_config":{"temperature":0.0,"stop_sequences":["PAUSE"]}}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Sat, 04 Jan 2025 17:59:11 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '289'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=1113
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Thought: I have the price of Bitcoin.\nAnswer: The price of Bitcoin is $107,000.\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.00083029649599834722
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 301,
            "candidatesTokenCount": 27,
            "totalTokenCount": 328
          },
          "modelVersion": "gemini-1.5-pro-002"
        }
  recorded_at: Sat, 04 Jan 2025 17:59:11 GMT
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro-002:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"}]},{"role":"user","parts":[{"text":"What
        is the price of Ethereum?"}]}],"generation_config":{"temperature":0.0,"stop_sequences":["PAUSE"]}}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Sat, 04 Jan 2025 17:59:12 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '311'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=1193
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Thought: I need to get the current price of Ethereum.\nAction: {\"tool\": \"price_tool\", \"args\": [\"Ethereum\"]}\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.00477554053068161
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 246,
            "candidatesTokenCount": 30,
            "totalTokenCount": 276
          },
          "modelVersion": "gemini-1.5-pro-002"
        }
  recorded_at: Sat, 04 Jan 2025 17:59:12 GMT
- request:
    method: post
    uri: https://generativelanguage.googleapis.com/v1/models/gemini-1.5-pro-002:generateContent?key=<GEMINI_API_KEY>
    body:
      encoding: UTF-8
      string: '{"contents":[{"role":"user","parts":[{"text":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"}]},{"role":"user","parts":[{"text":"What
        is the price of Ethereum?"}]},{"role":"assistant","parts":[{"text":"Thought:
        I need to get the current price of Ethereum.\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Ethereum\"]}"}]},{"role":"user","parts":[{"text":"Observation:
        {''BTC'': ''$107,000'', ''ETH'': ''$6,000''}"}]}],"generation_config":{"temperature":0.0,"stop_sequences":["PAUSE"]}}'
    headers:
      User-Agent:
      - Faraday v2.12.2
      Content-Type:
      - application/json
      Expect:
      - ''
  response:
    status:
      code: 200
      message: ''
    headers:
      Content-Type:
      - application/json; charset=UTF-8
      Vary:
      - Origin
      - Referer
      - X-Origin
      Content-Encoding:
      - gzip
      Date:
      - Sat, 04 Jan 2025 17:59:13 GMT
      Server:
      - scaffolding on HTTPServer2
      Content-Length:
      - '288'
      X-Xss-Protection:
      - '0'
      X-Frame-Options:
      - SAMEORIGIN
      X-Content-Type-Options:
      - nosniff
      Server-Timing:
      - gfet4t7; dur=1044
      Alt-Svc:
      - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "candidates": [
            {
              "content": {
                "parts": [
                  {
                    "text": "Thought: I have the price of Ethereum.\nAnswer: The price of Ethereum is $6,000.\n"
                  }
                ],
                "role": "model"
              },
              "finishReason": "STOP",
              "avgLogprobs": -0.0011838728934526444
            }
          ],
          "usageMetadata": {
            "promptTokenCount": 301,
            "candidatesTokenCount": 25,
            "totalTokenCount": 326
          },
          "modelVersion": "gemini-1.5-pro-002"
        }
  recorded_at: Sat, 04 Jan 2025 17:59:13 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/Anthropic/answers_a_basic_question.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the capital of Japan?"}],"system":"##
        Instructions\nConsider the following context: You are an AI agent\n\n\nYou
        are an AI agent reasoning step-by-step to solve complex problems.\nYour reasoning
        process happens in a loop of Thought, Action, Observation.\nThought - a description
        of your thoughts about the question.\nAction - pick a an action from available
        tools. If there are no tools that can help return an Answer saying you are
        not able to help.\nObservation - is the result of running a tool.\nPAUSE -
        is always present after an Action.\n\n## Available tools:\n\n\n## Example
        session\nQuestion: What is the weather in London today?\nThought: I need to
        get current weather in London\nAction: weather_tool | London\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n","model":"claude-3-5-sonnet-20240620","stop_sequences":["PAUSE"],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sun, 29 Dec 2024 10:13:02 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2024-12-29T10:13:01Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2024-12-29T10:13:02Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2024-12-29T10:13:02Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2024-12-29T10:13:02Z'
      Request-Id:
      - req_01UmqcXnBB4X4bnrMVUm1wz8
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8f9903d76d7dbbdb-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01XTNFpQSJMAtniMYm1hB5vW","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"Thought:
        To answer this question, I need to recall basic geographical knowledge about
        Japan. This is a straightforward factual question that doesn''t require any
        special tools or complex reasoning.\n\nAnswer: The capital of Japan is Tokyo.\n\nTokyo
        has been the capital of Japan since 1868, when it replaced the former capital,
        Kyoto. It is not only the political center of Japan but also its economic
        and cultural hub, being one of the world''s largest and most populous metropolitan
        areas."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":237,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":104}}'
  recorded_at: Sun, 29 Dec 2024 10:13:02 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/Anthropic/answers_a_question_with_a_tool.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the price of Bitcoin?"}],"system":"##
        Instructions\nConsider the following context: You are an AI agent\n\n\nYou
        are an AI agent reasoning step-by-step to solve complex problems.\nYour reasoning
        process happens in a loop of Thought, Action, Observation.\nThought - a description
        of your thoughts about the question.\nAction - pick a an action from available
        tools if required. If there are no tools that can help return an Answer saying
        you are not able to help.\nObservation - is the result of running a tool.\nPAUSE
        - a stop sequence that will always be present after an Action.\n\n## Available
        tools:\nprice_tool - Get the price of cryptocurrencies\n\n## Example session\nQuestion:
        What is the weather in London today?\nThought: I need to get current weather
        in London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n","model":"claude-3-5-sonnet-20240620","stop_sequences":["PAUSE"],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:45:43 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2025-01-04T17:45:42Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2025-01-04T17:45:43Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2025-01-04T17:45:43Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2025-01-04T17:45:43Z'
      Request-Id:
      - req_01UxpGFsmMjAs9Lto7bLGrDA
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8fcd0b31ff66bfda-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01JEhxSJTgCqTRJHLptiVaXk","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"Thought:
        To answer this question, I need to get the current price of Bitcoin. I can
        use the price_tool to obtain this information.\n\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Bitcoin\"]}\n"}],"stop_reason":"stop_sequence","stop_sequence":"PAUSE","usage":{"input_tokens":266,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":51}}'
  recorded_at: Sat, 04 Jan 2025 17:45:43 GMT
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the price of Bitcoin?"},{"role":"assistant","content":"Thought:
        To answer this question, I need to get the current price of Bitcoin. I can
        use the price_tool to obtain this information.\n\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Bitcoin\"]}\n"},{"role":"user","content":"Observation: {''BTC'':
        ''$107,000'', ''ETH'': ''$6,000''}"}],"system":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n","model":"claude-3-5-sonnet-20240620","stop_sequences":["PAUSE"],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:45:45 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2025-01-04T17:45:44Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2025-01-04T17:45:45Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2025-01-04T17:45:45Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2025-01-04T17:45:45Z'
      Request-Id:
      - req_015X4c7CAfhiMwoyVSZTVfDG
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8fcd0b3e8ec2eebf-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01CtqW1XJ9q7DGaPwoSJmq3i","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"Thought:
        I have received the current price information for Bitcoin (BTC) and Ethereum
        (ETH). The question specifically asked for the price of Bitcoin, so I''ll
        focus on that information.\n\nAnswer: The current price of Bitcoin (BTC) is $107,000."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":342,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":58}}'
  recorded_at: Sat, 04 Jan 2025 17:45:45 GMT
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the price of Ethereum?"}],"system":"##
        Instructions\nConsider the following context: You are an AI agent\n\n\nYou
        are an AI agent reasoning step-by-step to solve complex problems.\nYour reasoning
        process happens in a loop of Thought, Action, Observation.\nThought - a description
        of your thoughts about the question.\nAction - pick a an action from available
        tools if required. If there are no tools that can help return an Answer saying
        you are not able to help.\nObservation - is the result of running a tool.\nPAUSE
        - a stop sequence that will always be present after an Action.\n\n## Available
        tools:\nprice_tool - Get the price of cryptocurrencies\n\n## Example session\nQuestion:
        What is the weather in London today?\nThought: I need to get current weather
        in London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n","model":"claude-3-5-sonnet-20240620","stop_sequences":["PAUSE"],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:45:47 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2025-01-04T17:45:46Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2025-01-04T17:45:47Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2025-01-04T17:45:47Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2025-01-04T17:45:47Z'
      Request-Id:
      - req_01NBs3DhuVT3UFKsFTRukSmV
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8fcd0b4ab960b197-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01PaPmoEzM8x2BCTAFjb4VKa","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"Thought:
        To answer this question, I need to get the current price of Ethereum. I can
        use the price_tool to obtain this information.\n\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Ethereum\"]}\n"}],"stop_reason":"stop_sequence","stop_sequence":"PAUSE","usage":{"input_tokens":267,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":53}}'
  recorded_at: Sat, 04 Jan 2025 17:45:47 GMT
- request:
    method: post
    uri: https://api.anthropic.com/v1/messages
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"user","content":"What is the price of Ethereum?"},{"role":"assistant","content":"Thought:
        To answer this question, I need to get the current price of Ethereum. I can
        use the price_tool to obtain this information.\n\nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Ethereum\"]}\n"},{"role":"user","content":"Observation: {''BTC'':
        ''$107,000'', ''ETH'': ''$6,000''}"}],"system":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n","model":"claude-3-5-sonnet-20240620","stop_sequences":["PAUSE"],"max_tokens":1000}'
    headers:
      X-Api-Key:
      - "<ANTHROPIC_API_KEY>"
      Anthropic-Version:
      - '2023-06-01'
      Content-Type:
      - application/json
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:45:49 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Anthropic-Ratelimit-Requests-Limit:
      - '50'
      Anthropic-Ratelimit-Requests-Remaining:
      - '49'
      Anthropic-Ratelimit-Requests-Reset:
      - '2025-01-04T17:45:48Z'
      Anthropic-Ratelimit-Input-Tokens-Limit:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Remaining:
      - '40000'
      Anthropic-Ratelimit-Input-Tokens-Reset:
      - '2025-01-04T17:45:48Z'
      Anthropic-Ratelimit-Output-Tokens-Limit:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Remaining:
      - '8000'
      Anthropic-Ratelimit-Output-Tokens-Reset:
      - '2025-01-04T17:45:48Z'
      Anthropic-Ratelimit-Tokens-Limit:
      - '48000'
      Anthropic-Ratelimit-Tokens-Remaining:
      - '48000'
      Anthropic-Ratelimit-Tokens-Reset:
      - '2025-01-04T17:45:48Z'
      Request-Id:
      - req_01RyJ4HCtrWL3FzRtqk5GqcD
      Via:
      - 1.1 google
      Cf-Cache-Status:
      - DYNAMIC
      X-Robots-Tag:
      - none
      Server:
      - cloudflare
      Cf-Ray:
      - 8fcd0b56a9ac0278-WAW
    body:
      encoding: ASCII-8BIT
      string: '{"id":"msg_01K1sxhhMLjTWucTxKH9AvJX","type":"message","role":"assistant","model":"claude-3-5-sonnet-20240620","content":[{"type":"text","text":"Thought:
        I have received the price information for Ethereum (ETH) along with Bitcoin
        (BTC). The price tool provided the current prices for both cryptocurrencies.
        I can now answer the question about Ethereum''s price.\n\nAnswer: The current
        price of Ethereum (ETH) is $6,000."}],"stop_reason":"end_turn","stop_sequence":null,"usage":{"input_tokens":345,"cache_creation_input_tokens":0,"cache_read_input_tokens":0,"output_tokens":70}}'
  recorded_at: Sat, 04 Jan 2025 17:45:48 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/function_tools/answers_a_question_with_a_tool.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are a weather tool\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nget_weather
        - Get the weather for a given location\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the weather in San Francisco?"}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Fri, 10 Jan 2025 12:45:49 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '1607'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999717'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_22ec0e2cdb3037c62376fe06c9b12871
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=C6izTDHLm4cqyx8KivpL.xjL1WMiZfCG3ZIle3SxZIs-1736513149-1.0.1.1-nrJLOkKZWX2rM24or_SJeHqepfUs24G.uL.6zz6SQOmvJ8JxCLPp_pgCe3YJnRAJIn4uZLgLjJLRnh.JwWUX4Q;
        path=/; expires=Fri, 10-Jan-25 13:15:49 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=olo1Opy.SczAbKnihldKz.OXJG9_lOgBoD4dycXxxKw-1736513149631-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8ffcc425cca3bf6f-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Ao8gWanDKNfK9QEsNSIIK4shQtz1t",
          "object": "chat.completion",
          "created": 1736513148,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I need to get the current weather in San Francisco. \nAction: {\"tool\": \"get_weather\", \"args\": [\"San Francisco\"]}\n",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 242,
            "completion_tokens": 30,
            "total_tokens": 272,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "service_tier": "default",
          "system_fingerprint": "fp_01aeff40ea"
        }
  recorded_at: Fri, 10 Jan 2025 12:45:49 GMT
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are a weather tool\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nget_weather
        - Get the weather for a given location\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the weather in San Francisco?"},{"role":"assistant","content":"Thought:
        I need to get the current weather in San Francisco. \nAction: {\"tool\": \"get_weather\",
        \"args\": [\"San Francisco\"]}\n"},{"role":"user","content":"Observation:
        The weather in San Francisco is 70 degrees and sunny."}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Fri, 10 Jan 2025 12:45:50 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '1034'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999668'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_e2be32cf0060bbc96fd23a2ff30bd17e
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=D_CejBkM6iKZmuh05RB4hYwcd854kFyBncal0oQieds-1736513150-1.0.1.1-x.UaRjIzoBHlZJorrb4PosSuAYOp40r7u3ryez_3LYIH2sldh4gk.CG7tKGKCj..e2DA_nuu7nEOYzU6oAz6rg;
        path=/; expires=Fri, 10-Jan-25 13:15:50 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=MRktabUsQceLzBqbFdQivzlZagjdTROc0rp1sm9IUxE-1736513150937-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8ffcc43188fdb218-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Ao8gXZlg2ntYSzCbkX5E80NGCRCpO",
          "object": "chat.completion",
          "created": 1736513149,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I have the current weather information for San Francisco. \nAnswer: It is 70 degrees and sunny in San Francisco.",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 294,
            "completion_tokens": 26,
            "total_tokens": 320,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "service_tier": "default",
          "system_fingerprint": "fp_01aeff40ea"
        }
  recorded_at: Fri, 10 Jan 2025 12:45:51 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/OpenAI/answers_a_basic_question.yml`:

```yml
---
http_interactions:
  - request:
      method: post
      uri: https://api.openai.com/v1/chat/completions
      body:
        encoding: UTF-8
        string:
          '{"messages":[{"role":"system","content":"You are assisstant reasoning
          step-by-step to solve complex problems.\nYour reasoning process happens in
          a loop of Though, Action, Observation.\nThought - a description of your thoughts
          about the question.\nAction - pick a an action from available tools.\nObservation
          - is the result of running a tool.\n\n## Available tools:\n\n\n## Example
          session\nQuestion: What is the weather in London today?\nThought: I need to
          get the wether in London\nAction: weather_tool | \"London\"\nPAUSE\n\nYou
          will have a response with Observation:\nObservation: It is 32 degress and
          Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
          I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
          is the capital of Japan"}],"model":"gpt-4o","temperature":0.0,"n":1}'
      headers:
        Content-Type:
          - application/json
        Authorization:
          - Bearer <OPENAI_API_KEY>
        Accept-Encoding:
          - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
        Accept:
          - "*/*"
        User-Agent:
          - Ruby
    response:
      status:
        code: 200
        message: OK
      headers:
        Date:
          - Sun, 22 Dec 2024 10:14:15 GMT
        Content-Type:
          - application/json
        Transfer-Encoding:
          - chunked
        Connection:
          - keep-alive
        Access-Control-Expose-Headers:
          - X-Request-ID
        Openai-Organization:
          - favia-czx6t5
        Openai-Processing-Ms:
          - "1035"
        Openai-Version:
          - "2020-10-01"
        X-Ratelimit-Limit-Requests:
          - "5000"
        X-Ratelimit-Limit-Tokens:
          - "4000000"
        X-Ratelimit-Remaining-Requests:
          - "4999"
        X-Ratelimit-Remaining-Tokens:
          - "3999802"
        X-Ratelimit-Reset-Requests:
          - 12ms
        X-Ratelimit-Reset-Tokens:
          - 2ms
        X-Request-Id:
          - req_aeed211c5997c595d70d67e67a1d99c9
        Strict-Transport-Security:
          - max-age=31536000; includeSubDomains; preload
        Cf-Cache-Status:
          - DYNAMIC
        Set-Cookie:
          - __cf_bm=s1KVbM9HLzy2IZghijPPsaUk3I5LTqGmtqpHRYtpAdw-1734862455-1.0.1.1-6X8Mr2IdxffMT2iKN2WT7diHKqaJ1zZOVD6.hUXFGMmC7m03eZGcuQSVY2sGp_FfYOYxXbd3zrhQwrjLNtJmSg;
            path=/; expires=Sun, 22-Dec-24 10:44:15 GMT; domain=.api.openai.com; HttpOnly;
            Secure; SameSite=None
          - _cfuvid=TNxM4eBmEkoIe0q8UuBFR9xcBzzeZPmpfl4jkLjbV6I-1734862455129-0.0.1.1-604800000;
            path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
        X-Content-Type-Options:
          - nosniff
        Server:
          - cloudflare
        Cf-Ray:
          - 8f5f57fe5fd377b9-KBP
        Alt-Svc:
          - h3=":443"; ma=86400
      body:
        encoding: ASCII-8BIT
        string: |
          {
            "id": "chatcmpl-AhDGQtYpcLAYPJpe7aapf6XdJX0vD",
            "object": "chat.completion",
            "created": 1734862454,
            "model": "gpt-4o-mini-2024-07-18",
            "choices": [
              {
                "index": 0,
                "message": {
                  "role": "assistant",
                  "content": "Thought: I need to find out what the capital of Japan is. \nAction: I will recall my knowledge about countries and their capitals. \nObservation: The capital of Japan is Tokyo. \n\nThought: I have the answer now.\nAnswer: The capital of Japan is Tokyo.",
                  "refusal": null
                },
                "logprobs": null,
                "finish_reason": "stop"
              }
            ],
            "usage": {
              "prompt_tokens": 170,
              "completion_tokens": 57,
              "total_tokens": 227,
              "prompt_tokens_details": {
                "cached_tokens": 0,
                "audio_tokens": 0
              },
              "completion_tokens_details": {
                "reasoning_tokens": 0,
                "audio_tokens": 0,
                "accepted_prediction_tokens": 0,
                "rejected_prediction_tokens": 0
              }
            },
            "system_fingerprint": "fp_0aa8d3e20b"
          }
    recorded_at: Sun, 22 Dec 2024 10:14:14 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/fixtures/cassettes/Regent_Agent/OpenAI/answers_a_question_with_a_tool.yml`:

```yml
---
http_interactions:
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the price of Bitcoin?"}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:36:21 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '750'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999720'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_ab8a4bdb9654e3a7e6c6377d13ba9875
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=crYAqWxR4XiNUHh29OYM3hKPag5CZG1pZEPPtCdiLKQ-1736012181-1.0.1.1-TQ8AbrtImXecw07tgkhcCVMuRn2XnoWFuaMTBN7Yk4nVoAhce0C9e_q_ORpZzPRO9f.0RlovnCF74zZMhhfNMQ;
        path=/; expires=Sat, 04-Jan-25 18:06:21 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=qAqdkG7mZWDJ7DywhTKdbKv85DEE5_I.LTIKOvwIeD0-1736012181374-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8fccfd7f8a190296-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Am2MOBHi8u8EZ7UZEfrqIUnMkPjLB",
          "object": "chat.completion",
          "created": 1736012180,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I need to get the current price of Bitcoin. \nAction: {\"tool\": \"price_tool\", \"args\": [\"Bitcoin\"]}\n",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 239,
            "completion_tokens": 28,
            "total_tokens": 267,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "system_fingerprint": "fp_0aa8d3e20b"
        }
  recorded_at: Sat, 04 Jan 2025 17:36:21 GMT
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the price of Bitcoin?"},{"role":"assistant","content":"Thought: I need
        to get the current price of Bitcoin. \nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Bitcoin\"]}\n"},{"role":"user","content":"Observation: {''BTC'':
        ''$107,000'', ''ETH'': ''$6,000''}"}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:36:22 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '630'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999679'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_5858301dea5a42b4aa1433a7661bbe9b
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=acP2UcYDrSEXUquKcq0EDJXxMvXecrVSM3w7LhihjJU-1736012182-1.0.1.1-jqZqI9BqGUd6F_qnLxxXFSzMv1CP9unzRwuUH5.g03vjmVnBdvHlv2BkR4NjlzxPlwdNY2k5CKNVRdD6qaxwmQ;
        path=/; expires=Sat, 04-Jan-25 18:06:22 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=UEaCv.au.QZSIPwOtQDOk_rKpNZQ1ktdXJ0invvssZc-1736012182264-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8fccfd85dc21ecc3-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Am2MPn63w327jiHv2xmLiUX0lPv3L",
          "object": "chat.completion",
          "created": 1736012181,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I have the current price of Bitcoin, which is $107,000. \nAnswer: The price of Bitcoin is $107,000.",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 293,
            "completion_tokens": 30,
            "total_tokens": 323,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "system_fingerprint": "fp_0aa8d3e20b"
        }
  recorded_at: Sat, 04 Jan 2025 17:36:22 GMT
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the price of Ethereum?"}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:36:23 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '708'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999720'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_4453494f74186ab2e3e95ff32120ed15
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=qnlz_6z9apJdFmPfnZqCOI.mgN1EnTK4zPUqCr6XW1g-1736012183-1.0.1.1-MSxUdrTv39Y_eaX8sELsQwD8WE1nIelit2.5KmaDXgYLnqyJbsCb9L2VYD26iwZJp3k2ZYpUNbaxpgZ0KeYDvg;
        path=/; expires=Sat, 04-Jan-25 18:06:23 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=f8tunOMs7hbgljiSBJ_qyrSJH4nGgl_bDWCgpaXZGLc-1736012183290-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8fccfd8bbf9b025c-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Am2MQbJc5tsO4S68wyk8rJLYMFBwf",
          "object": "chat.completion",
          "created": 1736012182,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I need to get the current price of Ethereum. \nAction: {\"tool\": \"price_tool\", \"args\": [\"Ethereum\"]}\n",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 239,
            "completion_tokens": 28,
            "total_tokens": 267,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "system_fingerprint": "fp_0aa8d3e20b"
        }
  recorded_at: Sat, 04 Jan 2025 17:36:23 GMT
- request:
    method: post
    uri: https://api.openai.com/v1/chat/completions
    body:
      encoding: UTF-8
      string: '{"messages":[{"role":"system","content":"## Instructions\nConsider
        the following context: You are an AI agent\n\n\nYou are an AI agent reasoning
        step-by-step to solve complex problems.\nYour reasoning process happens in
        a loop of Thought, Action, Observation.\nThought - a description of your thoughts
        about the question.\nAction - pick a an action from available tools if required.
        If there are no tools that can help return an Answer saying you are not able
        to help.\nObservation - is the result of running a tool.\nPAUSE - a stop sequence
        that will always be present after an Action.\n\n## Available tools:\nprice_tool
        - Get the price of cryptocurrencies\n\n## Example session\nQuestion: What
        is the weather in London today?\nThought: I need to get current weather in
        London\nAction: {\"tool\": \"weather_tool\", \"args\": [\"London\"]}\nPAUSE\n\nYou
        will have a response form a user with Observation:\nObservation: It is 32
        degress and Sunny\n\n... (this Thought/Action/Observation can repeat N times)\n\nThought:
        I know the final answer\nAnswer: It is 32 degress and Sunny in London\n"},{"role":"user","content":"What
        is the price of Ethereum?"},{"role":"assistant","content":"Thought: I need
        to get the current price of Ethereum. \nAction: {\"tool\": \"price_tool\",
        \"args\": [\"Ethereum\"]}\n"},{"role":"user","content":"Observation: {''BTC'':
        ''$107,000'', ''ETH'': ''$6,000''}"}],"model":"gpt-4o-mini","temperature":0.0,"stop":["PAUSE"]}'
    headers:
      Content-Type:
      - application/json
      Authorization:
      - Bearer <OPENAI_API_KEY>
      Accept-Encoding:
      - gzip;q=1.0,deflate;q=0.6,identity;q=0.3
      Accept:
      - "*/*"
      User-Agent:
      - Ruby
  response:
    status:
      code: 200
      message: OK
    headers:
      Date:
      - Sat, 04 Jan 2025 17:36:24 GMT
      Content-Type:
      - application/json
      Transfer-Encoding:
      - chunked
      Connection:
      - keep-alive
      Access-Control-Expose-Headers:
      - X-Request-ID
      Openai-Organization:
      - favia-czx6t5
      Openai-Processing-Ms:
      - '658'
      Openai-Version:
      - '2020-10-01'
      X-Ratelimit-Limit-Requests:
      - '5000'
      X-Ratelimit-Limit-Tokens:
      - '4000000'
      X-Ratelimit-Remaining-Requests:
      - '4999'
      X-Ratelimit-Remaining-Tokens:
      - '3999679'
      X-Ratelimit-Reset-Requests:
      - 12ms
      X-Ratelimit-Reset-Tokens:
      - 4ms
      X-Request-Id:
      - req_db58ee8d3d71cd987d6366108461894c
      Strict-Transport-Security:
      - max-age=31536000; includeSubDomains; preload
      Cf-Cache-Status:
      - DYNAMIC
      Set-Cookie:
      - __cf_bm=9BVr_qw.XZBjGMHUNApTobewR6NjY2qIZMvIJHDqHHY-1736012184-1.0.1.1-gE0OhI1b9LHkdM8fAgPRnYyu3wvrhS5e6_JPyYoS4zVzVKdHhIM2f_KjR96RKnoH4d0pWXgtCR0zEJ0DgHvMVg;
        path=/; expires=Sat, 04-Jan-25 18:06:24 GMT; domain=.api.openai.com; HttpOnly;
        Secure; SameSite=None
      - _cfuvid=griYPpALI4YhrilUNHa28uC75KuakpXWvoi1s2SE6D4-1736012184245-0.0.1.1-604800000;
        path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None
      X-Content-Type-Options:
      - nosniff
      Server:
      - cloudflare
      Cf-Ray:
      - 8fccfd920cf5b185-WAW
      Alt-Svc:
      - h3=":443"; ma=86400
    body:
      encoding: ASCII-8BIT
      string: |
        {
          "id": "chatcmpl-Am2MR6plWO7t9QYhK8vmY9OjvLbQc",
          "object": "chat.completion",
          "created": 1736012183,
          "model": "gpt-4o-mini-2024-07-18",
          "choices": [
            {
              "index": 0,
              "message": {
                "role": "assistant",
                "content": "Thought: I have the current price of Ethereum, which is $6,000. \nAnswer: The price of Ethereum is $6,000.",
                "refusal": null
              },
              "logprobs": null,
              "finish_reason": "stop"
            }
          ],
          "usage": {
            "prompt_tokens": 293,
            "completion_tokens": 30,
            "total_tokens": 323,
            "prompt_tokens_details": {
              "cached_tokens": 0,
              "audio_tokens": 0
            },
            "completion_tokens_details": {
              "reasoning_tokens": 0,
              "audio_tokens": 0,
              "accepted_prediction_tokens": 0,
              "rejected_prediction_tokens": 0
            }
          },
          "system_fingerprint": "fp_0aa8d3e20b"
        }
  recorded_at: Sat, 04 Jan 2025 17:36:24 GMT
recorded_with: VCR 6.3.1

```

`/Users/estiens/code/ai/regent/spec/regent/agent_spec.rb`:

```rb
# frozen_string_literal: true

RSpec.describe Regent::Agent, :vcr do
  let(:llm) { Regent::LLM.new(model) }
  let(:agent) { Regent::Agent.new("You are an AI agent", model: llm) }
  let(:tool) { PriceTool.new(name: 'price_tool', description: 'Get the price of cryptocurrencies') }
  let(:spinner) { double("spinner", auto_spin: nil, update: nil, success: nil, error: nil) }
  class PriceTool < Regent::Tool
    def call(query)
      "{'BTC': '$107,000', 'ETH': '$6,000'}"
    end
  end

  context "OpenAI" do
    let(:model) { "gpt-4o-mini" }

    context "without a tool" do
      let(:cassette) { "Regent_Agent/OpenAI/answers_a_basic_question" }

      it "answers a basic question" do
        expect(agent.run("What is the capital of Japan?")).to eq("The capital of Japan is Tokyo.")
      end

      it "stores messages within a session" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "") },
          { role: :user, content: "What is the capital of Japan?" },
          { role: :assistant, content: "Thought: I need to find out what the capital of Japan is. \nAction: I will recall my knowledge about countries and their capitals. \nObservation: The capital of Japan is Tokyo. \n\nThought: I have the answer now.\nAnswer: The capital of Japan is Tokyo." }
        ])
      end

      it "stores session history" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.spans.count).to eq(3)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans.first.output).to eq("What is the capital of Japan?")
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[1].output).to eq("Thought: I need to find out what the capital of Japan is. \nAction: I will recall my knowledge about countries and their capitals. \nObservation: The capital of Japan is Tokyo. \n\nThought: I have the answer now.\nAnswer: The capital of Japan is Tokyo.")

        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
        expect(agent.session.spans.last.output).to eq("The capital of Japan is Tokyo.")
      end

      context "logging" do
        before do
          allow(TTY::Spinner).to receive(:new).and_return(spinner)
        end

        it "logs steps in the console" do
          agent.run("What is the capital of Japan?")

          # Input
          expect(spinner).to have_received(:update).with(
            title: /\[.*?INPUT.*?\].*?What is the capital of Japan\?/
          ).exactly(2).times

          # LLM Call
          expect(spinner).to have_received(:update).with(
            title: /\[.*?LLM.*?❯.*?gpt-4o-mini.*?\].*?What is the capital of Japan\?/
          ).exactly(2).times

          # Answer
          expect(spinner).to have_received(:update).with(
            title: /\[.*?ANSWER.*?\].*?\[.*?0\.\d*s.*?\].*?The capital of Japan is Tokyo\./
          ).exactly(2).times
        end
      end
    end

    context "with a tool" do
      let(:cassette) { "Regent_Agent/OpenAI/answers_a_question_with_a_tool" }
      let(:agent) { Regent::Agent.new("You are an AI agent", model: llm, tools: [tool]) }

      it "answers a question with a tool" do
        expect(agent.run("What is the price of Bitcoin?")).to eq("The price of Bitcoin is $107,000.")
        expect(agent.run("What is the price of Ethereum?")).to eq("The price of Ethereum is $6,000.")
      end

      it "stores messages within a session" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "price_tool - Get the price of cryptocurrencies") },
          { role: :user, content: "What is the price of Bitcoin?" },
          { role: :assistant, content: "Thought: I need to get the current price of Bitcoin. \nAction: {\"tool\": \"price_tool\", \"args\": [\"Bitcoin\"]}\n" },
          { role: :user, content: "Observation: {'BTC': '$107,000', 'ETH': '$6,000'}" },
          { role: :assistant, content: "Thought: I have the current price of Bitcoin, which is $107,000. \nAnswer: The price of Bitcoin is $107,000." }
        ])
      end

      it "stores session history" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.spans.count).to eq(5)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[2].type).to eq(Regent::Span::Type::TOOL_EXECUTION)
        expect(agent.session.spans[3].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
      end

      context "logging" do
        before do
          allow(TTY::Spinner).to receive(:new).and_return(spinner)
        end

        it "logs steps in the console" do
          agent.run("What is the price of Bitcoin?")

          # Input
          expect(spinner).to have_received(:update).with(
            title: /\[.*?INPUT.*?\].*?What is the price of Bitcoin\?/
          ).exactly(2).times

          # LLM Call
          expect(spinner).to have_received(:update).with(
            title: /\[.*?LLM.*?❯.*?gpt-4o-mini.*?\].*?What is the price of Bitcoin\?/
          ).exactly(2).times

          # Tool Execution - Initial call
          expect(spinner).to have_received(:update).with(
            title: /\[.*?TOOL.*?❯.*?price_tool.*?\].*?:.*?\["Bitcoin"\](?:\e\[0m)?$/
          ).once

          # Tool Execution - Result
          expect(spinner).to have_received(:update).with(
            title: /\[.*?TOOL.*?❯.*?price_tool.*?\[.*?0\.\d*s.*?\].*?:.*?\["Bitcoin"\] → {'BTC': '\$107,000', 'ETH': '\$6,000'}(?:\e\[0m)?/
          ).once

          expect(spinner).to have_received(:update).with(
            title: /\[.*?LLM.*?❯.*?gpt-4o-mini.*?\].*?Observation:/
          ).exactly(2).times

          # Answer
          expect(spinner).to have_received(:update).with(
            title: /\[.*?ANSWER.*?❯.*?success.*?\].*?The price of Bitcoin is \$107,000\./
          ).exactly(2).times
        end
      end
    end
  end

  context "Anthropic" do
    let(:model) { "claude-3-5-sonnet-20240620" }

    context "without a tool" do
      let(:cassette) { "Regent_Agent/Anthropic/answers_a_basic_question" }

      it "answers a basic question" do
        expect(agent.run("What is the capital of Japan?")).to eq("The capital of Japan is Tokyo.\n\nTokyo has been the capital of Japan since 1868, when it replaced the former capital, Kyoto. It is not only the political center of Japan but also its economic and cultural hub, being one of the world's largest and most populous metropolitan areas.")
      end

      it "stores messages within a session" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "") },
          { role: :user, content: "What is the capital of Japan?" },
          { role: :assistant, content: "Thought: To answer this question, I need to recall basic geographical knowledge about Japan. This is a straightforward factual question that doesn't require any special tools or complex reasoning.\n\nAnswer: The capital of Japan is Tokyo.\n\nTokyo has been the capital of Japan since 1868, when it replaced the former capital, Kyoto. It is not only the political center of Japan but also its economic and cultural hub, being one of the world's largest and most populous metropolitan areas." }
        ])
      end

      it "stores session history" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.spans.count).to eq(3)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans.first.output).to eq("What is the capital of Japan?")
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[1].output).to eq("Thought: To answer this question, I need to recall basic geographical knowledge about Japan. This is a straightforward factual question that doesn't require any special tools or complex reasoning.\n\nAnswer: The capital of Japan is Tokyo.\n\nTokyo has been the capital of Japan since 1868, when it replaced the former capital, Kyoto. It is not only the political center of Japan but also its economic and cultural hub, being one of the world's largest and most populous metropolitan areas.")

        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
        expect(agent.session.spans.last.output).to eq("The capital of Japan is Tokyo.\n\nTokyo has been the capital of Japan since 1868, when it replaced the former capital, Kyoto. It is not only the political center of Japan but also its economic and cultural hub, being one of the world's largest and most populous metropolitan areas.")
      end
    end

    context "with a tool" do
      let(:cassette) { "Regent_Agent/Anthropic/answers_a_question_with_a_tool" }
      let(:agent) { Regent::Agent.new("You are an AI agent", model: llm, tools: [tool]) }

      it "answers a question with a tool" do
        expect(agent.run("What is the price of Bitcoin?")).to eq("The current price of Bitcoin (BTC) is $107,000.")
        expect(agent.run("What is the price of Ethereum?")).to eq("The current price of Ethereum (ETH) is $6,000.")
      end

      it "stores messages within a session" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "price_tool - Get the price of cryptocurrencies") },
          { role: :user, content: "What is the price of Bitcoin?" },
          { role: :assistant, content: "Thought: To answer this question, I need to get the current price of Bitcoin. I can use the price_tool to obtain this information.\n\nAction: {\"tool\": \"price_tool\", \"args\": [\"Bitcoin\"]}\n" },
          { role: :user, content: "Observation: {'BTC': '$107,000', 'ETH': '$6,000'}" },
          { role: :assistant, content: "Thought: I have received the current price information for Bitcoin (BTC) and Ethereum (ETH). The question specifically asked for the price of Bitcoin, so I'll focus on that information.\n\nAnswer: The current price of Bitcoin (BTC) is $107,000." }
        ])
      end

      it "stores session history" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.spans.count).to eq(5)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[2].type).to eq(Regent::Span::Type::TOOL_EXECUTION)
        expect(agent.session.spans[3].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
      end
    end
  end

  context "Google Gemini" do
    let(:model) { "gemini-1.5-pro-002" }

    context "without a tool" do
      let(:cassette) { "Regent_Agent/Google_Gemini/answers_a_basic_question" }

      it "answers a basic question" do
        expect(agent.run("What is the capital of Japan?")).to eq("Tokyo")
      end

      it "stores messages within a session" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "") },
          { role: :user, content: "What is the capital of Japan?" },
          { role: :assistant, content: "Thought: I know the capital of Japan.\nAnswer: Tokyo" }
        ])
      end

      it "stores session history" do
        agent.run("What is the capital of Japan?")

        expect(agent.session.spans.count).to eq(3)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans.first.output).to eq("What is the capital of Japan?")
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[1].output).to eq("Thought: I know the capital of Japan.\nAnswer: Tokyo")

        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
        expect(agent.session.spans.last.output).to eq("Tokyo")
      end
    end

    context "with a tool" do
      let(:cassette) { "Regent_Agent/Google_Gemini/answers_a_question_with_a_tool" }
      let(:agent) { Regent::Agent.new("You are an AI agent", model: llm, tools: [tool]) }

      it "answers a question with a tool" do
        expect(agent.run("What is the price of Bitcoin?")).to eq("The price of Bitcoin is $107,000.")
        expect(agent.run("What is the price of Ethereum?")).to eq("The price of Ethereum is $6,000.")
      end

      it "stores messages within a session" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.messages).to eq([
          { role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt("You are an AI agent", "price_tool - Get the price of cryptocurrencies") },
          { role: :user, content: "What is the price of Bitcoin?" },
          { role: :assistant, content: "Thought: I need to get the current price of Bitcoin.\nAction: {\"tool\": \"price_tool\", \"args\": [\"Bitcoin\"]}" },
          { role: :user, content: "Observation: {'BTC': '$107,000', 'ETH': '$6,000'}" },
          { role: :assistant, content: "Thought: I have the price of Bitcoin.\nAnswer: The price of Bitcoin is $107,000." }
        ])
      end

      it "stores session history" do
        agent.run("What is the price of Bitcoin?")

        expect(agent.session.spans.count).to eq(5)
        expect(agent.session.spans.first.type).to eq(Regent::Span::Type::INPUT)
        expect(agent.session.spans[1].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans[2].type).to eq(Regent::Span::Type::TOOL_EXECUTION)
        expect(agent.session.spans[3].type).to eq(Regent::Span::Type::LLM_CALL)
        expect(agent.session.spans.last.type).to eq(Regent::Span::Type::ANSWER)
      end
    end
  end

  context "allows tools to be defined in the agent class" do
    let(:model) { "gpt-4o-mini" }
    let(:cassette) { "Regent_Agent/function_tools/answers_a_question_with_a_tool" }

    context "with tool definition missing" do
      subject { InvalidWeatherAgent.new("You are a weather tool", model: Regent::LLM.new(model))}
      class InvalidWeatherAgent < Regent::Agent
        tool(:get_weather, "Get the weather for a given location")
      end

      it "raises an error" do
        expect{ subject }.to raise_error("A tool method 'get_weather' is missing in the InvalidWeatherAgent")
      end
    end

    context "properly defined tool" do
      let(:agent) { ValidWeatherAgent.new("You are a weather tool", model: Regent::LLM.new(model)) }

      class ValidWeatherAgent < Regent::Agent
        tool(:get_weather, "Get the weather for a given location")

        def get_weather(location)
          "The weather in #{location} is 70 degrees and sunny."
        end
      end

      it "allows tools to be defined in the agent class" do
        expect(agent.run("What is the weather in San Francisco?")).to eq("It is 70 degrees and sunny in San Francisco.")
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/spec/regent/llm_spec.rb`:

```rb
# frozen_string_literal: true

RSpec.describe Regent::LLM do
  let(:strict_mode) { true }
  let(:messages) { [{ role: :user, content: "What is the capital of Japan?" }] }

  subject { Regent::LLM.new(model, strict_mode: strict_mode) }

  context "OpenAI", vcr: true do
    let(:model) { "gpt-4o-mini" }
    let(:cassette) { "LLM/OpenAI/success_response" }

    it "returns a model response" do
      result = subject.invoke(messages)
      expect(result).to be_a(Regent::LLM::Result)
      expect(result.content).to eq("The capital of Japan is Tokyo.")
    end
  end

  context "OpenAICompatible", vcr: true do
    let(:model) { Regent::LLM::OpenAI.new(model: "qwen-qwq-32b", api_key: 'api_key', uri_base: 'https://api.groq.com/openai') }
    let(:cassette) { "LLM/OpenAI/compatible_success_response" }

    it "returns a model response" do
      result = subject.invoke(messages)
      expect(result).to be_a(Regent::LLM::Result)
      expect(result.content).to eq("The capital of Japan is Tokyo.")
    end
  end

  context "Gemini", vcr: true do
    let(:model) { "gemini-1.5-flash" }
    let(:cassette) { "LLM/Google_Gemini/success_response" }

    it "returns a model response" do
      result = subject.invoke(messages)
      expect(result).to be_a(Regent::LLM::Result)
      expect(result.content).to eq("Tokyo")
    end
  end

  context "Anthropic", vcr: true do
    let(:model) { "claude-3-5-sonnet-20240620" }
    let(:cassette) { "LLM/Anthropic/success_response" }

    it "returns a model response" do
      result = subject.invoke(messages)
      expect(result).to be_a(Regent::LLM::Result)
      expect(result.content).to eq("The capital of Japan is Tokyo. Tokyo has been the capital of Japan since 1868, when it replaced Kyoto as the seat of the Emperor and the national government. It is the most populous metropolitan area in the world and serves as Japan's political, economic, and cultural center. Tokyo is known for its unique blend of modern technology and traditional culture, bustling urban areas, and efficient transportation systems.")
    end
  end

  context "Ollama", vcr: true do
    let(:model) { Regent::LLM::Ollama.new(model: "gemma") }
    let(:cassette) { "LLM/Ollama/success_response" }

    it "returns a model response" do
      result = subject.invoke(messages)
      expect(result).to be_a(Regent::LLM::Result)
      expect(result.content).to eq("The capital city of Japan is Tokyo.\n\nIt is the political, economic, and cultural center of Japan and is known for its modern cityscape and traditional culture.")
    end
  end

  context "Unsupported model" do
    let(:model) { "llama-3.1-8b" }

    it "raises an error if the model is not supported" do
      expect { subject }.to raise_error(Regent::LLM::ProviderNotFoundError)
    end
  end

  context "API key not set in environment" do
    let(:model) { "gpt-4o-mini" }

    it "raises an error if the API key is not set" do
      original_api_key = ENV["OPENAI_API_KEY"]
      ENV["OPENAI_API_KEY"] = nil
      expect { subject }.to raise_error(Regent::LLM::APIKeyNotFoundError)
    ensure
      ENV["OPENAI_API_KEY"] = original_api_key
    end
  end

  context "API error", vcr: true do
    context "OpenAI" do
      let(:model) { "gpt-4.1o-mini" }
      let(:cassette) { "LLM/OpenAI/non_existent_model" }

      context "strict mode" do
        it "raises an API error" do
          expect { subject.invoke(messages) }.to raise_error(
            Regent::LLM::ApiError,
            "The model `gpt-4.1o-mini` does not exist or you do not have access to it."
          )
        end
      end

      context "non strict mode" do
        let(:strict_mode) { false }

        it "returns a result with error message" do
          result = subject.invoke(messages)
          expect(result).to be_a(Regent::LLM::Result)
          expect(result.content).to eq("The model `gpt-4.1o-mini` does not exist or you do not have access to it.")
        end
      end
    end

    context "Gemini" do
      let(:model) { "gemini-3.5-flash" }
      let(:cassette) { "LLM/Google_Gemini/non_existent_model" }

      context "strict mode" do
        it "raises an API error" do
          expect { subject.invoke(messages) }.to raise_error(
            Regent::LLM::ApiError,
            "models/gemini-3.5-flash is not found for API version v1, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods."
          )
        end
      end

      context "non strict mode" do
        let(:strict_mode) { false }

        it "returns a result with error message" do
          result = subject.invoke(messages)
          expect(result).to be_a(Regent::LLM::Result)
          expect(result.content).to eq("models/gemini-3.5-flash is not found for API version v1, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.")
        end
      end
    end

    context "Anthropic" do
      let(:model) { "claude-4.1-haiku" }
      let(:cassette) { "LLM/Anthropic/non_existent_model" }

      context "strict mode" do
        it "raises an API error" do
          expect { subject.invoke(messages) }.to raise_error(
            Regent::LLM::ApiError,
            "system: Input should be a valid list"
          )
        end
      end

      context "non strict mode" do
        let(:strict_mode) { false }

        it "returns a result with error message" do
          result = subject.invoke(messages)
          expect(result).to be_a(Regent::LLM::Result)
          expect(result.content).to eq("system: Input should be a valid list")
        end
      end
    end

    context "Ollama" do
      let(:model) { Regent::LLM::Ollama.new(model: "llama3.1")   }
      let(:cassette) { "LLM/Ollama/non_existent_model" }

      it "raises an API error" do
        expect { subject.invoke(messages) }.to raise_error(Regent::LLM::ApiError)
      end

      context "non strict mode" do
        let(:strict_mode) { false }

        it "returns a result with error message" do
          result = subject.invoke(messages)
          expect(result).to be_a(Regent::LLM::Result)
          expect(result.content).to eq("model \"llama3.1\" not found, try pulling it first")
        end
      end
    end
  end

  context "Missing model dependency" do
    let(:model) { "claude-3-5-sonnet-20240620" }

    before do
      allow(Regent::Logger).to receive(:warn_and_exit).and_return(true)
      allow_any_instance_of(Regent::LLM::Anthropic).to receive(:gem).with("anthropic").and_raise(Gem::LoadError)
    end

    it "warns and exists if the dependency is not installed" do
      subject

      expect(Regent::Logger).to have_received(:warn_and_exit).with(
         /\n.*In order to use .*claude-3-5-sonnet-20240620.* model you need to install .*anthropic.* gem. Please add .*gem "anthropic".* to your Gemfile.*/
      )
    end
  end
end

```

`/Users/estiens/code/ai/regent/README.md`:

```md
![regent_light](https://github.com/user-attachments/assets/62564dac-b8d7-4dc0-9b63-64c6841b5872)

<div align="center">

# Regent

[![Gem Version](https://badge.fury.io/rb/regent.svg)](https://badge.fury.io/rb/regent)
[![Build](https://github.com/alchaplinsky/regent/actions/workflows/main.yml/badge.svg)](https://github.com/alchaplinsky/regent/actions/workflows/main.yml)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

</div>

**Regent** is a small and elegant Ruby framework for building AI agents that can think, reason, and take actions through tools. It provides a clean, intuitive interface for creating agents that can solve complex problems by breaking them down into logical steps.

> [!NOTE]
> Regent is currently an experiment intended to explore patterns for building easily traceable and debuggable AI agents of different architectures. It is not yet intended to be used in production and is currently in development.
> 
> Read more about Regent in a Medium article: [Building AI Agent from scratch with Ruby](https://medium.com/towards-artificial-intelligence/building-ai-agent-from-scratch-with-ruby-c6260dad45b7)

## Key Features

- **ReAct Pattern Implementation**: Agents follow the Reasoning-Action pattern, making decisions through a clear thought process before taking actions
- **Multi-LLM Support**: Seamlessly works with:
  - OpenAI (GPT models)
  - Anthropic (Claude models)
  - Google (Gemini models)
- **Extensible Tool System**: Create custom tools that agents can use to interact with external services, APIs, or perform specific tasks
- **Built-in Tracing**: Every agent interaction is traced and can be replayed, making debugging and monitoring straightforward
- **Clean Ruby Interface**: Designed to feel natural to Ruby developers while maintaining powerful capabilities

## Showcase

A basic Regnt Agent extended with a `price_tool` that allows for retrieving cryptocurrency prices from coingecko.com.

![Screen_gif](https://github.com/user-attachments/assets/63c8c923-0c1e-48db-99f6-33758411623f)

## Quick Start

```bash
gem install regent
```

or add regent to the Gemfile:

```ruby
gem 'regent'
```

and run

```bash
bundle install
```

## Usage

### Quick Example

Create your first weather agent:

```ruby
# Define agent class
class WeatherAgent < Regent::Agent
  tool(:weather_tool, "Get current weather for a location")

  def weather_tool(location)
    "Currently 72°F and sunny in #{location}"
  end
end

# Instantiate an agent
agent = WeatherAgent.new("You are a helpful weather assistant", model: "gpt-4o")

# Execute a query
agent.run("What's the weather like in Tokyo?") # => "It is currently 72°F and sunny in Tokyo."
```

### LLMs
Regent provides an interface for invoking an LLM through an instance of `Regent::LLM` class. Even though Agent initializer allows you to pass a modal name as a string, sometimes it is useful to create a model instance if you want to tune model params before passing it to the agent. Or if you need to invoke a model directly without passing it to an Agent you can do that by creating an instance of LLM class:

```ruby
model = Regent::LLM.new("gemini-1.5-flash")
# or with options
model = Regent::LLM.new("gemini-1.5-flash", temperature: 0.5) # supports options that are supported by the model
```

#### API keys
By default, **Regent** will try to fetch API keys for corresponding models from environment variables. Make sure that the following ENV variables are set depending on your model choice:

| Model series | ENV variable name   |
|--------------|---------------------|
| `gpt-`       | `OPENAI_API_KEY`    |
| `gemini-`    | `GEMINI_API_KEY`    |
| `claude-`    | `ANTHROPIC_API_KEY` |

But you can also pass an `api_key` option to the` Regent::LLM` constructor should you need to override this behavior:

```ruby
model = Regent::LLM.new("gemini-1.5-flash", api_key: "AIza...")
```

> [!NOTE]
> Currently **Regent** supports only `gpt-`, `gemini-` and `claude-` models series and local **ollama** models. But you can build, your custom model classes that conform to the Regent's interface and pass those instances to the Agent.

#### Calling LLM
Once your model is instantiated you can call the `invoke` method:

```ruby
model.invoke("Hello!") 
```

Alternatively, you can pass message history to the `invoke` method. Messages need to follow OpenAI's message format (eg. `{role: "user", content: "..."}`)

```ruby
model.invoke([
  {role: "system", content: "You are a helpful assistant"},
  {role: "user", content: "Hello!"}
])
```

This method returns an instance of the `Regent::LLM::Result` class, giving access to the content or error and token usage stats.

```ruby
result = model.invoke("Hello!")

result.content # => Hello there! How can I help you today?
result.input_tokens # => 2
result.output_tokens # => 11
result.error # => nil
```

### Tools

There are multiple ways how you can give agents tools for performing actions and retrieving additional information. First of all you can define a **function tool** directly on the agent class:

```ruby
class MyAgent < Regent::Agent
  # define the tool by giving a unique name and description
  tool :search_web, "Search for information on the web" 

  def search_web(query)
    # Implement tool logic within the method with the same name
  end
end
```

For more complex tools we can define a dedicated class with a `call` method that will get called. And then pass an instance of this tool to an agent:

```ruby
class SearchTool < Regent::Tool
  def call(query)
    # Implement tool logic
  end
end

agent = Regent::Agent.new("Find information and answer any question", {
  model: "gpt-4o",
  tools: [SearchTool.new]
})

```

### Agent

**Agent** class is the core of the library. To crate an agent, you can use `Regent::Agent` class directly if you don't need to add any business logic. Or you can create your own class inheriting from `Regent::Agent`. To instantiate an agent you need to pass a **purpose** of an agent and a model it should use.

```ruby
agent = Regent::Agent.new("You are a helpful assistant", model: "gpt-4o-mini")
```

Additionally, you can pass a list of Tools to extend the agent's capabilities. Those should be instances of classes that inherit from `Regent::Tool` class:

```ruby
class SearchTool < Regent::Tool
  def call
    # make a call to search API
  end
end

class CalculatorTool < Regent::Tool
  def call
    # perform calculations
  end
end

tools = [SearchTool.new, CalculatorTool.new]

agent = Regent::Agent.new("You are a helpful assistant", model: "gpt-4o-mini", tools: tools)
```

Each agent run creates a **session** that contains every operation that is performed by the agent while working on a task. Sessions can be replayed and drilled down into while debugging.
```ruby
agent.sessions # => Returns all sessions performed by the agent
agent.session # => Returns last session performed by the agent
agent.session.result # => Returns result of latest agent run
```

While running agent logs all session spans (all operations) to the console with all sorts of useful information, that helps to understand what the agent was doing and why it took a certain path.
```ruby
weather_agent.run("What is the weather in San Francisco?")
```

Outputs:
```console
[✔] [INPUT][0.0s]: What is the weather in San Francisco?
 ├──[✔] [LLM ❯ gpt-4o-mini][242 → 30 tokens][0.02s]: What is the weather in San Francisco?
 ├──[✔] [TOOL ❯ get_weather][0.0s]: ["San Francisco"] → The weather in San Francisco is 70 degrees and sunny.
 ├──[✔] [LLM ❯ gpt-4o-mini][294 → 26 tokens][0.01s]: Observation: The weather in San Francisco is 70 degrees and sunny.
[✔] [ANSWER ❯ success][0.03s]: It is 70 degrees and sunny in San Francisco.
```

### Engine
By default, Regent uses ReAct agent architecture. You can see the [details of its implementation](https://github.com/alchaplinsky/regent/blob/main/lib/regent/engine/react.rb). However, Agent constructor accepts an `engine` option that allows you to swap agent engine when instantiating an Agent. This way you can implement your own agent architecture that can be plugged in and user within Regent framework.

```ruby
agent = CustomAgent.new("You are a self-correcting assistant", model: "gpt-4o", engine: CustomEngine)
```

In order to implement your own engine you need to define a class that inherits from `Regent::Engine::Base` class and implements `reason` method:

```ruby
class CustomEngine < Regent::Engine::Base
  def reason(task)
    # Your implementation of an Agent lifecycle
  end
end
```

Note that Base class already handles `max_iteration` check, so you won't end up in an infinite loop. Also, it allows you to use `llm_call_response` and `tool_call_response` methods for agent reasoning as well as `success_answer` and `error_answer` for the final result.

For any other operation that happens in your agent architecture that you want to track separately call it within the `session.exec` block. See examples in `Regent::Engine::Base` class.


---
## Why Regent?

- **Transparent Decision Making**: Watch your agent's thought process as it reasons through problems
- **Flexible Architecture**: Easy to extend with custom tools and adapt to different use cases
- **Ruby-First Design**: Takes advantage of Ruby's elegant syntax and conventions
- **Transparent Execution**: Built with tracing, error handling, and clean abstractions


## Development

After checking out the repo, run `bin/setup` to install dependencies. Then, run `rake spec` to run the tests. You can also run `bin/console` for an interactive prompt that will allow you to experiment.

To install this gem onto your local machine, run `bundle exec rake install`. To release a new version, update the version number in `version.rb`, and then run `bundle exec rake release`, which will create a git tag for the version, push git commits and the created tag, and push the `.gem` file to [rubygems.org](https://rubygems.org).

## Contributing

Bug reports and pull requests are welcome on GitHub at https://github.com/alchaplinsky/regent. This project is intended to be a safe, welcoming space for collaboration, and contributors are expected to adhere to the [code of conduct](https://github.com/alchaplinsky/regent/blob/main/CODE_OF_CONDUCT.md).

## Code of Conduct

Everyone interacting in the Regent project's codebases, issue trackers, chat rooms and mailing lists is expected to follow the [code of conduct](https://github.com/alchaplinsky/regent/blob/main/CODE_OF_CONDUCT.md).

```

`/Users/estiens/code/ai/regent/Rakefile`:

```
# frozen_string_literal: true

require "bundler/gem_tasks"
require "rspec/core/rake_task"

RSpec::Core::RakeTask.new(:spec)

task default: :spec

```

`/Users/estiens/code/ai/regent/lib/regent.rb`:

```rb
# frozen_string_literal: true

require 'securerandom'
require 'json'
require 'faraday'
require 'pastel'
require 'tty-spinner'
require 'zeitwerk'

module Regent
  class Error < StandardError; end
  # Your code goes here...

  loader = Zeitwerk::Loader.for_gem
  loader.inflector.inflect("llm" => "LLM")
  loader.inflector.inflect("open_ai" => "OpenAI")
  loader.setup
end

```

`/Users/estiens/code/ai/regent/lib/regent/tool.rb`:

```rb
# frozen_string_literal: true

module Regent
  class ToolError < StandardError; end

  class Tool
    def initialize(name:, description:)
      @name = name
      @description = description
    end

    attr_reader :name, :description

    def call(argument)
      raise NotImplementedError, "Tool #{name} has not implemented the execute method"
    end

    def execute(*arguments)
      call(*arguments)
    rescue NotImplementedError, StandardError => e
      raise ToolError, e.message
    end

    def to_s
      "#{name} - #{description}"
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/span.rb`:

```rb
# frozen_string_literal: true

module Regent
  class Span
    include Concerns::Identifiable
    include Concerns::Durationable

    module Type
      INPUT = 'INPUT'.freeze
      LLM_CALL = 'LLM'.freeze
      TOOL_EXECUTION = 'TOOL'.freeze
      MEMORY_ACCESS = 'MEMO'.freeze
      ANSWER = 'ANSWER'.freeze

      def self.all
        constants.map { |c| const_get(c) }
      end

      def self.valid?(type)
        all.include?(type)
      end
    end

    # @param type [String] The type of span (must be one of Type.all)
    # @param arguments [Hash] Arguments for the span
    # @param logger [Logger] Logger instance
    def initialize(type:, arguments:, logger: Logger.new)
      super()

      validate_type!(type)

      @logger = logger
      @type = type
      @arguments = arguments
      @meta = nil
      @output = nil
    end

    attr_reader :name, :arguments, :output, :type, :start_time, :end_time

    # @raise [ArgumentError] if block is not given
    # @return [String] The output of the span
    def run
      @output = log_operation do
        yield

      rescue StandardError, ToolError => e
        logger.error(label: type, message: e.message, **arguments)
        raise
      end
    end

    # @return [String] The output of the span
    def replay
      log_operation(live: false) { @output }
    end

    # @return [Boolean] Whether the span is currently running
    def running?
      @start_time && @end_time.nil?
    end

    # @return [Boolean] Whether the span is completed
    def completed?
      @start_time && @end_time
    end

    # @param value [String] The meta value to set
    def set_meta(value)
      @meta = value.freeze
    end

    private

    attr_reader :logger, :meta

    def validate_type!(type)
      raise InvalidSpanType, "Invalid span type: #{type}" unless Type.valid?(type)
    end

    def log_operation(live: true, &block)
      @start_time = live ? Time.now.freeze : @start_time
      logger.start(label: type, **arguments)

      result = yield

      @end_time = live ? Time.now.freeze : @end_time
      update_message_with_result(result) if type == Type::TOOL_EXECUTION
      logger.success(label: type, **({ duration: duration.round(2), meta: meta }.merge(arguments)))
      result
    end

    def update_message_with_result(message)
      arguments[:message] = "#{arguments[:message]} → #{message}"
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/base.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    Result = Struct.new(:model, :content, :input_tokens, :output_tokens, keyword_init: true)

    class Base
      include Concerns::Dependable

      def initialize(model:, api_key: nil, **options)
        @model = model
        @api_key = api_key || api_key_from_env
        @options = options

        super()
      end

      def parse_error(error)
        error.response.dig(:body, "error", "message")
      end

      private

      attr_reader :model, :api_key, :options

      def result(model:, content:, input_tokens:, output_tokens:)
        Result.new(
          model: model,
          content: content,
          input_tokens: input_tokens,
          output_tokens: output_tokens
        )
      end

      def api_key_from_env
        ENV.fetch(self.class::ENV_KEY) do
          raise APIKeyNotFoundError, "API key not found. Make sure to set #{self.class::ENV_KEY} environment variable."
        end
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/ollama.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    class Ollama < Base
      # Default host for Ollama API.
      DEFAULT_HOST = "http://localhost:11434"

      def initialize(model:, host: nil, **options)
        @model = model
        @host = host || DEFAULT_HOST
        @options = options
      end

      attr_reader :model

      def invoke(messages, **args)
        response = client.post("/api/chat", {
          model: model,
          messages: messages,
          stream: false
        })

        if response.status == 200
          result(
            model: response.body.dig("model"),
            content: response.body.dig("message", "content").strip,
            input_tokens: nil,
            output_tokens: nil
          )
        else
          raise ApiError, response.body.dig("error")
        end
      end

      def parse_error(error)
        error.message
      end

      private

      attr_reader :host

      def client
        @client ||= Faraday.new(host) do |f|
          f.request :json
          f.response :json
          f.adapter :net_http
        end
      end

      def api_key_from_env
        nil
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/open_ai.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    class OpenAI < Base
      ENV_KEY = "OPENAI_API_KEY"

      depends_on "ruby-openai"

      attr_reader :model

      def invoke(messages, **args)
        response = client.chat(parameters: {
          messages: messages,
          model: model,
          temperature: args[:temperature] || 0.0,
          stop: args[:stop] || []
        })

        result(
          model: model,
          content: response.dig("choices", 0, "message", "content"),
          input_tokens: response.dig("usage", "prompt_tokens"),
          output_tokens: response.dig("usage", "completion_tokens")
        )
      end

      private

      def client
        client_options = { access_token: api_key }
        client_options[:uri_base] = options[:uri_base] if options[:uri_base]

        @client ||= ::OpenAI::Client.new(**client_options)
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/anthropic.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    class Anthropic < Base
      MAX_TOKENS = 1000
      ENV_KEY = "ANTHROPIC_API_KEY"

      depends_on "anthropic"

      def invoke(messages, **args)
        parameters = {
          messages: format_messages(messages),
          model: model,
          temperature: args[:temperature] || 0.0,
          stop_sequences: args[:stop] || [],
          max_tokens: MAX_TOKENS
        }
        if system_instruction = system_instruction(messages)
          parameters[:system] = system_instruction
        end

        response = client.messages(parameters:)

        result(
          model: model,
          content: response.dig("content", 0, "text"),
          input_tokens: response.dig("usage", "input_tokens"),
          output_tokens: response.dig("usage", "output_tokens")
        )
      end

      private

      def client
        @client ||= ::Anthropic::Client.new(access_token: api_key)
      end

      def system_instruction(messages)
        messages.find { |message| message[:role].to_s == "system" }&.dig(:content)
      end

      def format_messages(messages)
        messages.reject { |message| message[:role].to_s == "system" }
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/gemini.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    class Gemini < Base
      ENV_KEY = "GEMINI_API_KEY"
      SERVICE = "generative-language-api"

      depends_on "gemini-ai"

      def invoke(messages, **args)
        response = client.generate_content({
          contents: format_messages(messages),
          generation_config: {
            temperature: args[:temperature] || 0.0,
            stop_sequences: args[:stop] || []
          }
        })

        result(
          model: model,
          content: response.dig("candidates", 0, "content", "parts", 0, "text").strip,
          input_tokens: response.dig("usageMetadata", "promptTokenCount"),
          output_tokens: response.dig("usageMetadata", "candidatesTokenCount")
        )
      end

      def parse_error(error)
        JSON.parse(error.response.dig(:body)).dig("error", "message")
      end

      private

      def client
        @client ||= ::Gemini.new(
          credentials: { service: SERVICE, api_key: api_key },
          options: { model: model }
        )
      end

      def format_messages(messages)
        messages.map do |message|
          { role: message[:role].to_s == "system" ? "user" : message[:role], parts: [{ text: message[:content] }] }
        end
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm/open_router.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    class OpenRouter < Base
      ENV_KEY = "OPEN_ROUTER_API_KEY"

      depends_on "open_router"

      def invoke(messages, **args)
        response = client.complete(
          messages,
          model: model,
          extras: {
            temperature: args[:temperature] || 0.0,
            stop: args[:stop] || [],
            **args
          }
        )
        result(
          model: model,
          content: response.dig("choices", 0, "message", "content"),
          input_tokens: response.dig("usage", "prompt_tokens"),
          output_tokens: response.dig("usage", "completion_tokens")
        )
      end

      private

      def client
        @client ||= ::OpenRouter::Client.new access_token: api_key
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/agent.rb`:

```rb
# frozen_string_literal: true

module Regent
  class Agent
    include Concerns::Identifiable
    include Concerns::Toolable

    DEFAULT_MAX_ITERATIONS = 10

    def initialize(context, model:, tools: [], engine: Regent::Engine::React, **options)
      super()

      @context = context
      @model = model.is_a?(String) ? Regent::LLM.new(model) : model
      @engine = engine
      @sessions = []
      @tools = build_toolchain(tools)
      @max_iterations = options[:max_iterations] || DEFAULT_MAX_ITERATIONS
    end

    attr_reader :context, :sessions, :model, :tools, :inline_tools

    def run(task)
      raise ArgumentError, "Task cannot be empty" if task.to_s.strip.empty?

      start_session
      reason(task)
    ensure
      complete_session
    end

    def running?
      session&.active? || false
    end

    def session
      @sessions.last
    end

    private

    def reason(task)
      engine.reason(task)
    end

    def start_session
      complete_session
      @sessions << Session.new
      session.start
    end

    def complete_session
      session&.complete if running?
    end

    def build_toolchain(tools)
      context = self

      toolchain = Toolchain.new(Array(tools))

      self.class.function_tools.each do |entry|
        toolchain.add(entry, context)
      end

      toolchain
    end

    def engine
      @engine.new(context, model, tools, session, @max_iterations)
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/toolchain.rb`:

```rb
# frozen_string_literal: true

module Regent
  class Toolchain
    def initialize(tools)
      @tools = tools
    end

    attr_reader :tools

    def find(name)
      tools.find { |tool| tool.name.downcase == name.downcase }
    end

    def add(tool, context)
      @tools << Regent::Tool.new(name: tool[:name].to_s, description: tool[:description]).instance_eval do
        raise "A tool method '#{tool[:name]}' is missing in the #{context.class.name}" unless context.respond_to?(tool[:name])

        define_singleton_method(:call){ |*args| context.send(tool[:name], *args) }
        self
      end
    end

    def to_s
      tools.map(&:to_s).join("\n")
    end

    private

    def tool_missing_error(tool_name, context_name)
      "A tool method '#{tool_name}' is missing in the #{context_name}"
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/llm.rb`:

```rb
# frozen_string_literal: true

module Regent
  class LLM
    DEFAULT_RETRY_COUNT = 3
    PROVIDER_PATTERNS = {
      OpenAI: /^gpt-/,
      Gemini: /^gemini-/,
      Anthropic: /^claude-/
    }.freeze

    class ProviderNotFoundError < StandardError; end
    class APIKeyNotFoundError < StandardError; end
    class ApiError < StandardError; end

    def initialize(model, strict_mode: true, **options)
      @strict_mode = strict_mode
      @options = options
      if model.class.ancestors.include?(Regent::LLM::Base)
        @model = model.model
        @provider = model
      else
        @model = model
        @provider = instantiate_provider
      end
    end

    attr_reader :model, :options

    def invoke(messages, **args)
      retries = 0

      messages = [{ role: "user", content: messages }] if messages.is_a?(String)

      provider.invoke(messages, **args)

    rescue Faraday::Error, ApiError => error
      if error.respond_to?(:retryable?) && error.retryable? && retries < DEFAULT_RETRY_COUNT
        sleep(exponential_backoff(retries))
        retry
      end
      handle_error(error)
    end

    private

    attr_reader :provider, :strict_mode

    def instantiate_provider
      provider_class = find_provider_class
      raise ProviderNotFoundError, "Provider for #{model} is not found" if provider_class.nil?

      create_provider(provider_class)
    end

    def find_provider_class
      PROVIDER_PATTERNS.find { |key, pattern| model.match?(pattern) }&.first
    end

    def create_provider(provider_class)
      Regent::LLM.const_get(provider_class).new(**options.merge(model: model))
    end

    def handle_error(error)
      message = provider.parse_error(error) || error.message
      raise ApiError, message if strict_mode
      Result.new(model: model, content: message, input_tokens: nil, output_tokens: nil)
    end

    def exponential_backoff(retry_count)
      # Exponential backoff with jitter: 2^n * 100ms + random jitter
      (2**retry_count * 0.1) + rand(0.1)
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/session.rb`:

```rb
# frozen_string_literal: true

module Regent
  class Session
    include Concerns::Identifiable
    include Concerns::Durationable

    class SessionError < StandardError; end
    class InactiveSessionError < SessionError; end
    class AlreadyStartedError < SessionError; end

    def initialize
      super()

      @spans = []
      @messages = []
      @start_time = nil
      @end_time = nil
    end

    attr_reader :id, :spans, :messages, :start_time, :end_time

    # Starts the session
    # @raise [AlreadyStartedError] if session is already started
    # @return [void]
    def start
      raise AlreadyStartedError, "Session already started" if @start_time

      @start_time = Time.now.freeze
    end

    # Executes a new span in the session
    # @param type [Symbol, String] The type of span
    # @param options [Hash] Options for the span
    # @raise [InactiveSessionError] if session is not active
    # @return [String] The output of the span
    def exec(type, options = {}, &block)
      raise InactiveSessionError, "Cannot execute span in inactive session" unless active?

      @spans << Span.new(type: type, arguments: options)
      current_span.run(&block)
    end

    # Replays the session
    # @return [String] The result of the session
    def replay
      spans.each { |span| span.replay }
      result
    end

    # Completes the session and returns the result
    # @return [Object] The result of the last span
    # @raise [InactiveSessionError] if session is not active
    # @return [String] The result of the last span
    def complete
      raise InactiveSessionError, "Cannot complete inactive session" unless active?

      @end_time = Time.now.freeze
      result
    end

    # @return [Span, nil] The current span or nil if no spans exist
    def current_span
      @spans.last
    end

    # @return [String, nil] The output of the current span or nil if no spans exist
    def result
      current_span&.output
    end

    # @return [Boolean] Whether the session is currently active
    def active?
      start_time && end_time.nil?
    end

    # Adds a message to the session
    # @param message [String] The message to add
    # @raise [ArgumentError] if message is nil or empty
    def add_message(message)
      raise ArgumentError, "Message cannot be nil or empty" if message.nil? || message.empty?

      @messages << message
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/logger.rb`:

```rb
# frozen_string_literal: true

module Regent
  class Logger
    COLORS = %i[dim white green yellow red blue cyan clear].freeze

    class << self
      def warn_and_exit(message)
        warn message
        exit 1
      end
    end

    def initialize(output: $stdout)
      @pastel = Pastel.new
      @spinner = build_spinner(spinner_symbol, output)
      @nested_spinner = build_spinner("#{dim(" ├──")}#{spinner_symbol}", output)
    end

    attr_reader :spinner, :nested_spinner

    def info(label:, message:, duration: nil, type: nil, meta: nil, top_level: false)
      current_spinner = top_level ? spinner : nested_spinner
      current_spinner.update(title: format_message(label, message, duration, type, meta))
      current_spinner
    end

    def start(**args)
      info(**args).auto_spin
    end

    def success(**args)
      info(**args).success
    end

    def error(**args)
      info(**args).error
    end

    private

    def format_message(label, message, duration, type, meta)
      parts = []
      parts << "#{dim("[")}#{cyan(label)}"
      parts << "#{dim(" ❯")} #{yellow(type)}" if type
      parts << dim("]")
      parts << dim("[#{meta}]") if meta
      parts << dim("[#{duration.round(2)}s]") if duration
      parts << dim(":")
      parts << clear(" #{message}")

      parts.join
    end

    def spinner_symbol
      "#{dim("[")}#{white(":spinner")}#{dim("]")}"
    end

    def build_spinner(spinner_format, output)
      TTY::Spinner.new("#{spinner_format} :title", format: :dots, output: output)
    end

    COLORS.each do |color|
      define_method(color) do |message|
        @pastel.send(color, message)
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/version.rb`:

```rb
# frozen_string_literal: true

module Regent
  VERSION = "0.3.4"
end

```

`/Users/estiens/code/ai/regent/lib/regent/engine/base.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Engine
    class Base
      def initialize(context, llm, toolchain, session, max_iterations)
        @context = context
        @llm = llm
        @toolchain = toolchain
        @session = session
        @max_iterations = max_iterations
      end

      attr_reader :context, :llm, :toolchain, :session, :max_iterations

      private

      # Run reasoning block within this method to ensure that it
      # will not run more than max_iterations times.
      def with_max_iterations
        max_iterations.times do
          yield
        end

        error_answer("Max iterations reached without finding an answer.")
      end

      # Make a call to LLM and return the response.
      def llm_call_response(args)
        session.exec(Span::Type::LLM_CALL, type: llm.model, message: session.messages.last[:content]) do
          result = llm.invoke(session.messages, **args)

          session.current_span.set_meta("#{result.input_tokens} → #{result.output_tokens} tokens")
          result.content
        end
      end

      # Make a call to a tool and return the response.
      def tool_call_response(tool, arguments)
        session.exec(Span::Type::TOOL_EXECUTION, { type: tool.name, message: arguments }) do
          tool.execute(*arguments)
        end
      end

      # Find a tool in the toolchain by name and return it.
      def find_tool(tool_name)
        tool = toolchain.find(tool_name)
        return tool if tool

        session.exec(Span::Type::ANSWER, type: :failure, message: "No matching tool found for: #{tool_name}")
      end

      # Complete a session with a success answer
      def success_answer(content)
        session.exec(Span::Type::ANSWER, top_level: true, type: :success, message: content, duration: session.duration.round(2)) { content }
      end

      # Complete a session with an error answer
      def error_answer(content)
        session.exec(Span::Type::ANSWER, top_level: true, type: :failure, message: content, duration: session.duration.round(2)) { content }
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/engine/react/prompt_template.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Engine
    class React
      module PromptTemplate
        def self.system_prompt(context = "", tool_list = "")
          <<~PROMPT
            ## Instructions
            #{context ? "Consider the following context: #{context}\n\n" : ""}
            You are an AI agent reasoning step-by-step to solve complex problems.
            Your reasoning process happens in a loop of Thought, Action, Observation.
            Thought - a description of your thoughts about the question.
            Action - pick a an action from available tools if required. If there are no tools that can help return an Answer saying you are not able to help.
            Observation - is the result of running a tool.
            PAUSE - a stop sequence that will always be present after an Action.

            ## Available tools:
            #{tool_list}

            ## Example session
            Question: What is the weather in London today?
            Thought: I need to get current weather in London
            Action: {"tool": "weather_tool", "args": ["London"]}
            PAUSE

            You will have a response form a user with Observation:
            Observation: It is 32 degress and Sunny

            ... (this Thought/Action/Observation can repeat N times)

            Thought: I know the final answer
            Answer: It is 32 degress and Sunny in London
          PROMPT
        end
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/engine/react.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Engine
    class React < Base
      SEQUENCES = {
        answer: "Answer:",
        action: "Action:",
        observation: "Observation:",
        stop: "PAUSE"
      }.freeze

      def reason(task)
        session.exec(Span::Type::INPUT, top_level: true, message: task) { task }
        session.add_message({role: :system, content: Regent::Engine::React::PromptTemplate.system_prompt(context, toolchain.to_s)})
        session.add_message({role: :user, content: task})

        with_max_iterations do
          content = llm_call_response(stop: [SEQUENCES[:stop]])
          session.add_message({role: :assistant, content: content })

          return extract_answer(content) if answer_present?(content)

          if action_present?(content)
            tool_name, arguments = parse_tool_signature(content)
            tool = find_tool(tool_name)
            return unless tool
            result = tool_call_response(tool, arguments)
            session.add_message({ role: :user, content: "#{SEQUENCES[:observation]} #{result}" })
          end
        end
      end

      private

      def answer_present?(content)
        content.include?(SEQUENCES[:answer])
      end

      def action_present?(content)
        content.include?(SEQUENCES[:action])
      end

      def extract_answer(content)
        success_answer content.split(SEQUENCES[:answer])[1]&.strip
      end

      def parse_tool_signature(content)
        return [nil, nil] unless match = content.match(/Action:.*?\{.*"tool".*\}/m)

        # Extract just the JSON part using a second regex
        json = JSON.parse(match[0].match(/\{.*\}/m)[0])
        [json["tool"], json["args"] || []]
      rescue JSON::ParserError
        [nil, nil]
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/concerns/identifiable.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Concerns
    module Identifiable
      def self.included(base)
        base.class_eval do
          attr_reader :id
        end
      end

      private

      def generate_id
        @id = SecureRandom.uuid
      end

      def initialize(*)
        generate_id
        super
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/concerns/dependable.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Concerns
    module Dependable
      class VersionError < StandardError; end

      def self.included(base)
        base.class_eval do
          class << self
            def depends_on(gem_name)
              @dependency = gem_name
            end

            def dependency
              @dependency
            end
          end
        end
      end

      def initialize(**options)
        @dependency = self.class.dependency
        require_dynamic(dependency) if dependency

        super()
      rescue Gem::LoadError
        Regent::Logger.warn_and_exit dependency_warning(dependency, model)
      end

      def require_dynamic(*names)
        names.each { |name| load_dependency(name) }
      end

      private

      attr_reader :dependency

      def load_dependency(name)
        gem(name)
        gem_spec = Gem::Specification.find_by_name(name)

        if defined?(Bundler)
          gem_requirement = dependencies.find { |gem| gem.name == gem_spec.name }.requirement

          unless gem_requirement.satisfied_by?(gem_spec.version)
            raise VersionError, version_error(gem_spec, gem_requirement)
          end
        end

        require_gem(gem_spec)
      end

      def version_error(gem_spec, gem_requirement)
        "'#{gem_spec.name}' gem version is #{gem_spec.version}, but your Gemfile specified #{gem_requirement}."
      end

      def require_gem(gem_spec)
        gem_spec.full_require_paths.each do |path|
          Dir.glob("#{path}/*.rb").each { |file| require file }
        end
      end

      def dependencies
        Bundler.load.dependencies
      end

      def dependency_warning(dependency, model)
        "\n\e[33mIn order to use \e[33;1m#{model}\e[0m\e[33m model you need to install \e[33;1m#{dependency}\e[0m\e[33m gem. Please add \e[33;1mgem \"#{dependency}\"\e[0m\e[33m to your Gemfile.\e[0m"
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/concerns/durationable.rb`:

```rb
# frozen_string_literal: true
module Regent
  module Concerns
    module Durationable
      def duration
        return 0 unless @start_time

        (@end_time || Time.now) - @start_time
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/lib/regent/concerns/toolable.rb`:

```rb
# frozen_string_literal: true

module Regent
  module Concerns
    module Toolable
      def self.included(base)
        base.class_eval do
          class << self
            def tool(name, description)
              @function_tools ||= []
              @function_tools << { name: name, description: description }
            end

            def function_tools
              @function_tools || []
            end
          end
        end
      end
    end
  end
end

```

`/Users/estiens/code/ai/regent/Gemfile`:

```
# frozen_string_literal: true

source "https://rubygems.org"

# Specify your gem's dependencies in regent.gemspec
gemspec

gem "rake", "~> 13.0"

gem "rspec", "~> 3.0"
gem "ruby-openai", "~> 7.3.1"
gem "tty-spinner", "~> 0.9.3"
gem "pastel", "~> 0.8.0"
gem "http", "~> 5.1"

gem "anthropic"
gem "gemini-ai"
gem "open_router"

group :test do
  gem "vcr", "~> 6.2"
  gem "webmock", "~> 3.19"
end

```

`/Users/estiens/code/ai/regent/Gemfile.lock`:

```lock
PATH
  remote: .
  specs:
    regent (0.3.4)
      faraday (~> 2.12)
      pastel (~> 0.8.0)
      tty-spinner (~> 0.9.3)
      zeitwerk (~> 2.7)

GEM
  remote: https://rubygems.org/
  specs:
    activesupport (8.0.1)
      base64
      benchmark (>= 0.3)
      bigdecimal
      concurrent-ruby (~> 1.0, >= 1.3.1)
      connection_pool (>= 2.2.5)
      drb
      i18n (>= 1.6, < 2)
      logger (>= 1.4.2)
      minitest (>= 5.1)
      securerandom (>= 0.3)
      tzinfo (~> 2.0, >= 2.0.5)
      uri (>= 0.13.1)
    addressable (2.8.7)
      public_suffix (>= 2.0.2, < 7.0)
    anthropic (0.3.2)
      event_stream_parser (>= 0.3.0, < 2.0.0)
      faraday (>= 1)
      faraday-multipart (>= 1)
    base64 (0.2.0)
    benchmark (0.4.0)
    bigdecimal (3.1.8)
    concurrent-ruby (1.3.5)
    connection_pool (2.5.0)
    crack (1.0.0)
      bigdecimal
      rexml
    diff-lcs (1.5.1)
    domain_name (0.6.20240107)
    dotenv (3.1.7)
    drb (2.2.1)
    ethon (0.16.0)
      ffi (>= 1.15.0)
    event_stream_parser (1.0.0)
    faraday (2.12.2)
      faraday-net_http (>= 2.0, < 3.5)
      json
      logger
    faraday-multipart (1.1.0)
      multipart-post (~> 2.0)
    faraday-net_http (3.4.0)
      net-http (>= 0.5.0)
    faraday-typhoeus (1.1.0)
      faraday (~> 2.0)
      typhoeus (~> 1.4)
    ffi (1.17.0)
    ffi (1.17.0-arm64-darwin)
    ffi-compiler (1.3.2)
      ffi (>= 1.15.5)
      rake
    gemini-ai (4.2.0)
      event_stream_parser (~> 1.0)
      faraday (~> 2.10)
      faraday-typhoeus (~> 1.1)
      googleauth (~> 1.8)
      typhoeus (~> 1.4, >= 1.4.1)
    google-cloud-env (2.2.1)
      faraday (>= 1.0, < 3.a)
    google-logging-utils (0.1.0)
    googleauth (1.12.2)
      faraday (>= 1.0, < 3.a)
      google-cloud-env (~> 2.2)
      google-logging-utils (~> 0.1)
      jwt (>= 1.4, < 3.0)
      multi_json (~> 1.11)
      os (>= 0.9, < 2.0)
      signet (>= 0.16, < 2.a)
    hashdiff (1.1.2)
    http (5.2.0)
      addressable (~> 2.8)
      base64 (~> 0.1)
      http-cookie (~> 1.0)
      http-form_data (~> 2.2)
      llhttp-ffi (~> 0.5.0)
    http-cookie (1.0.8)
      domain_name (~> 0.5)
    http-form_data (2.3.0)
    i18n (1.14.7)
      concurrent-ruby (~> 1.0)
    json (2.9.0)
    jwt (2.10.0)
      base64
    llhttp-ffi (0.5.0)
      ffi-compiler (~> 1.0)
      rake (~> 13.0)
    logger (1.6.3)
    minitest (5.25.4)
    multi_json (1.15.0)
    multipart-post (2.4.1)
    net-http (0.6.0)
      uri
    open_router (0.3.3)
      activesupport (>= 6.0)
      dotenv (>= 2)
      faraday (>= 1)
      faraday-multipart (>= 1)
    os (1.1.4)
    pastel (0.8.0)
      tty-color (~> 0.5)
    public_suffix (6.0.1)
    rake (13.2.1)
    rexml (3.4.0)
    rspec (3.12.0)
      rspec-core (~> 3.12.0)
      rspec-expectations (~> 3.12.0)
      rspec-mocks (~> 3.12.0)
    rspec-core (3.12.2)
      rspec-support (~> 3.12.0)
    rspec-expectations (3.12.3)
      diff-lcs (>= 1.2.0, < 2.0)
      rspec-support (~> 3.12.0)
    rspec-mocks (3.12.6)
      diff-lcs (>= 1.2.0, < 2.0)
      rspec-support (~> 3.12.0)
    rspec-support (3.12.1)
    ruby-openai (7.3.1)
      event_stream_parser (>= 0.3.0, < 2.0.0)
      faraday (>= 1)
      faraday-multipart (>= 1)
    securerandom (0.4.1)
    signet (0.19.0)
      addressable (~> 2.8)
      faraday (>= 0.17.5, < 3.a)
      jwt (>= 1.5, < 3.0)
      multi_json (~> 1.10)
    tty-color (0.6.0)
    tty-cursor (0.7.1)
    tty-spinner (0.9.3)
      tty-cursor (~> 0.7)
    typhoeus (1.4.1)
      ethon (>= 0.9.0)
    tzinfo (2.0.6)
      concurrent-ruby (~> 1.0)
    uri (1.0.2)
    vcr (6.3.1)
      base64
    webmock (3.24.0)
      addressable (>= 2.8.0)
      crack (>= 0.3.2)
      hashdiff (>= 0.4.0, < 2.0.0)
    zeitwerk (2.7.1)

PLATFORMS
  arm64-darwin-22
  ruby

DEPENDENCIES
  anthropic
  gemini-ai
  http (~> 5.1)
  open_router
  pastel (~> 0.8.0)
  rake (~> 13.0)
  regent!
  rspec (~> 3.0)
  ruby-openai (~> 7.3.1)
  tty-spinner (~> 0.9.3)
  vcr (~> 6.2)
  webmock (~> 3.19)

BUNDLED WITH
   2.5.9

```

`/Users/estiens/code/ai/regent/regent.gemspec`:

```gemspec
# frozen_string_literal: true

require_relative "lib/regent/version"

Gem::Specification.new do |spec|
  spec.name = "regent"
  spec.version = Regent::VERSION
  spec.authors = ["Alex Chaplinsky"]
  spec.email = ["alchaplinsky@gmail.com"]

  spec.summary = "Library for building AI Agents in Ruby"
  spec.description = "Regent is a library for building AI Agents that utilize tools to accomplish tasks. Current implementation is based on the ReAct Agent architecture."
  spec.homepage = "https://github.com/alchaplinsky/regent"
  spec.license = "MIT"
  spec.required_ruby_version = ">= 3.0.0"

  spec.metadata["homepage_uri"] = spec.homepage
  spec.metadata["source_code_uri"] = "#{spec.homepage}/tree/main"
  spec.metadata["changelog_uri"] = "#{spec.homepage}/blob/main/CHANGELOG.md"

  # Specify which files should be added to the gem when it is released.
  # The `git ls-files -z` loads the files in the RubyGem that have been added into git.
  gemspec = File.basename(__FILE__)
  spec.files = IO.popen(%w[git ls-files -z], chdir: __dir__, err: IO::NULL) do |ls|
    ls.readlines("\x0", chomp: true).reject do |f|
      (f == gemspec) ||
        f.start_with?(*%w[bin/ test/ spec/ features/ .git .github appveyor Gemfile])
    end
  end
  spec.bindir = "exe"
  spec.executables = spec.files.grep(%r{\Aexe/}) { |f| File.basename(f) }
  spec.require_paths = ["lib"]

  # Uncomment to register a new dependency of your gem
  # spec.add_dependency "example-gem", "~> 1.0"
  spec.add_dependency "faraday", "~> 2.12"
  spec.add_dependency "zeitwerk", "~> 2.7"
  spec.add_dependency "tty-spinner", "~> 0.9.3"
  spec.add_dependency "pastel", "~> 0.8.0"

  # For more information and examples about making a new gem, check out our
  # guide at: https://bundler.io/guides/creating_gem.html
end

```